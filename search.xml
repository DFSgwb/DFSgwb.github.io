<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>ACM格式输入（一）</title>
      <link href="/2023/04/13/ACM%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%85%A5/"/>
      <url>/2023/04/13/ACM%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%85%A5/</url>
      
        <content type="html"><![CDATA[<h1 id="c-常用的输入输出方法"><a href="#c-常用的输入输出方法" class="headerlink" title="c++常用的输入输出方法"></a>c++常用的输入输出方法</h1><h2 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h2><p>1.cin</p><blockquote><p>注意1：cin可以连续从键盘读入数据<br>注意2：cin以空格、tab、换行符作为分隔符<br>注意3：cin从第一个非空格字符开始读取，直到遇到分隔符结束读取</p></blockquote><pre><code class="cpp">// 用法1，读入单数据int num;cin &gt;&gt; num;cout &lt;&lt; num &lt;&lt; endl;  // 输出读入的整数num// 用法2，批量读入多个数据vector&lt;int&gt; nums(5);for(int i = 0; i &lt; nums.size(); i++) &#123;    cin &gt;&gt; nums[i];&#125;// 输出读入的数组for(int i = 0; i &lt; nums.size(); i++) &#123;    cout &lt;&lt; nums[i] &lt;&lt; &quot; &quot;;&#125;</code></pre><p>2.getline()</p><blockquote><p>当读取的字符串中间存在空格时，cin就不可用了，便可以使用getline()</p></blockquote><pre><code class="cpp">string s;getline(cin, s);// 输出读入的字符串cout &lt;&lt; s &lt;&lt; endl;</code></pre><p>3.getchar</p><pre><code class="cpp">char ch;ch = getchar();// 输出读入的字符cout &lt;&lt; ch &lt;&lt; endl;</code></pre><p>4.scanf()</p><blockquote><p>使用最多的输入方式</p></blockquote><pre><code class="cpp">//1.输入十进制的数 int a;scanf(&quot;%d&quot;,&amp;a);scanf(&quot;%i&quot;,&amp;a);scanf(&quot;%u&quot;,&amp;a);//这三种写法都是可以的 //2.输入八进制和十六进制数 int b;scanf(&quot;%o&quot;,&amp;b); //八进制 scanf(&quot;%x&quot;,&amp;b); //十六进制 //3.输入实数int c;scanf(&quot;%f&quot;,&amp;c);scanf(&quot;%e&quot;,&amp;c);//这两种写法可以互换 //4.输入字符和字符串 char d;string dd;scanf(&quot;%c&quot;,&amp;d); //单个字符 scanf(&quot;%s&quot;,&amp;dd); //字符串 //5.跳过一次输入 int e;scanf(&quot;%*&quot;,&amp;e);//6.输入长整型数 int f;scanf(&quot;%ld&quot;,&amp;f);scanf(&quot;%lo&quot;,&amp;f);scanf(&quot;%lx&quot;,&amp;f);scanf(&quot;%l&quot;,&amp;f);//四种写法都可以用 //7.输入短整型数 int g;scanf(&quot;%hd&quot;,&amp;g);scanf(&quot;%ho&quot;,&amp;g);scanf(&quot;%hx&quot;,&amp;g);scanf(&quot;%h&quot;,&amp;g);//四种写法都可以用 //8.输入double型数（小数 double h;scanf(&quot;%lf&quot;,&amp;h);scanf(&quot;%lf&quot;,&amp;h);scanf(&quot;%l&quot;,&amp;h);//三种写法都可以用 //9.域宽的使用 int i;scanf(&quot;%5d&quot;,&amp;i);//10.特殊占位符 int j,k;scanf(&quot;%d,%d&quot;,&amp;j,&amp;k);int j,k;scanf(&quot;%d&quot;,&amp;j);printf(&quot;,&quot;); //cout&lt;&lt;&quot;,&quot;;scanf(&quot;%d&quot;,&amp;k);</code></pre><h2 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h2><p>cout，printf随意搭配，就不讲了</p><p>下面将一些输入格式</p><pre><code class="cpp">#include&lt;iostream&gt;#include&lt;sstream&gt;#include&lt;string&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;#include&lt;limits.h&gt;  //INT_MIN 和 INT_MAX的头文件  using namespace std;struct stu &#123;    string name;    int num;&#125;;// 1. 直接输入一个数int main() &#123;    int n = 0;    while (cin &gt;&gt; n) &#123;         cout &lt;&lt; n &lt;&lt; endl;    &#125;    return -1;&#125;// 2. 直接输入一个字符串int main() &#123;    string str;    while (cin &gt;&gt; str) &#123;        cout &lt;&lt; str &lt;&lt; endl;    &#125;    return -1;&#125;// 3. 只读取一个字符 int main() &#123;    char ch;    //方式1    while (cin &gt;&gt; ch) &#123;        cout &lt;&lt; ch &lt;&lt; endl;    &#125;    //方式2： cin.get(ch) 或者 ch = cin.get() 或者 cin.get()    while (cin.get(ch)) &#123;           cout &lt;&lt; ch &lt;&lt; endl;    &#125;    //方式3 ：ch = getchar()      while (getchar()) &#123;        cout &lt;&lt; ch &lt;&lt; endl;    &#125;    return -1;&#125;// 3.1给定一个数，表示有多少组数（可能是数字和字符串的组合），然后读取int main() &#123;    int n = 0;     while (cin &gt;&gt; n) &#123;   //每次读取1 + n 个数，即一个样例有n+1个数         vector&lt;int&gt; nums(n);        for (int i = 0; i &lt; n; i++) &#123;            cin &gt;&gt; nums[i];        &#125;        //处理这组数/字符串        for (int i = 0; i &lt; n; i++) &#123;            cout &lt;&lt; nums[i] &lt;&lt; endl;        &#125;    &#125;    return -1;&#125;//3.2 首先给一个数字，表示需读取n个字符，然后顺序读取n个字符int main() &#123;    int n = 0;    while (cin &gt;&gt; n) &#123;  //输入数量        vector&lt;string&gt; strs;        for (int i = 0; i &lt; n; i++) &#123;            string temp;            cin &gt;&gt; temp;            strs.push_back(temp);        &#125;        //处理这组字符串        sort(strs.begin(), strs.end());        for (auto&amp; str : strs) &#123;            cout &lt;&lt; str &lt;&lt; &#39; &#39;;        &#125;    &#125;    return 0;&#125;//4.未给定数据个数，但是每一行代表一组数据，每个数据之间用空格隔开//4.1使用getchar() 或者 cin.get() 读取判断是否是换行符，若是的话，则表示该组数（样例）结束了，需进行处理int main() &#123;    int ele;    while (cin &gt;&gt; ele) &#123;        int sum = ele;        // getchar()   //读取单个字符        /*while (cin.get() != &#39;\n&#39;) &#123;*/   //判断换行符号        while (getchar() != &#39;\n&#39;) &#123;  //如果不是换行符号的话，读到的是数字后面的空格或者table            int num;            cin &gt;&gt; num;            sum += num;        &#125;        cout &lt;&lt; sum &lt;&lt; endl;    &#125;    return 0;&#125;//4.2 给定一行字符串，每个字符串用空格间隔，一个样例为一行int main() &#123;    string str;    vector&lt;string&gt; strs;    while (cin &gt;&gt; str) &#123;        strs.push_back(str);        if (getchar() == &#39;\n&#39;) &#123;  //控制测试样例            sort(strs.begin(), strs.end());            for (auto&amp; str : strs) &#123;                cout &lt;&lt; str &lt;&lt; &quot; &quot;;            &#125;            cout &lt;&lt; endl;            strs.clear();        &#125;    &#125;    return 0;&#125;//4.3 使用getline 读取一整行数字到字符串input中，然后使用字符串流stringstream，读取单个数字或者字符。int main() &#123;    string input;    while (getline(cin, input)) &#123;  //读取一行        stringstream data(input);  //使用字符串流        int num = 0, sum = 0;        while (data &gt;&gt; num) &#123;            sum += num;        &#125;        cout &lt;&lt; sum &lt;&lt; endl;    &#125;    return 0;&#125;//4.4 使用getline 读取一整行字符串到字符串input中，然后使用字符串流stringstream，读取单个数字或者字符。int main() &#123;    string words;    while (getline(cin, words)) &#123;        stringstream data(words);        vector&lt;string&gt; strs;        string str;        while (data &gt;&gt; str) &#123;            strs.push_back(str);        &#125;        sort(strs.begin(), strs.end());        for (auto&amp; str : strs) &#123;            cout &lt;&lt; str &lt;&lt; &quot; &quot;;        &#125;    &#125;&#125;//4.5 使用getline 读取一整行字符串到字符串input中，然后使用字符串流stringstream，读取单个数字或者字符。每个字符中间用&#39;,&#39;间隔int main() &#123;    string line;        //while (cin &gt;&gt; line) &#123;  //因为加了“，”所以可以看出一个字符串读取    while(getline(cin, line))&#123;        vector&lt;string&gt; strs;        stringstream ss(line);        string str;        while (getline(ss, str, &#39;,&#39;)) &#123;            strs.push_back(str);        &#125;        //        sort(strs.begin(), strs.end());        for (auto&amp; str : strs) &#123;            cout &lt;&lt; str &lt;&lt; &quot; &quot;;        &#125;        cout &lt;&lt; endl;    &#125;    return 0;&#125;int main() &#123;    string str;        //C语言读取字符、数字    int a;    char c;    string s;    scanf_s(&quot;%d&quot;, &amp;a);    scanf(&quot;%c&quot;, &amp;c);    scanf(&quot;%s&quot;, &amp;s);    printf(&quot;%d&quot;, a);    //读取字符    char ch;    cin &gt;&gt; ch;    ch = getchar();    while (cin.get(ch)) &#123; //获得单个字符        ;    &#125;        //读取字符串    cin &gt;&gt; str;  //遇到空白停止    getline(cin, str);  //读入一行字符串&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> ACM </tag>
            
            <tag> 刷题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ACM格式输入（二）</title>
      <link href="/2023/04/13/ACM%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%85%A52/"/>
      <url>/2023/04/13/ACM%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%85%A52/</url>
      
        <content type="html"><![CDATA[<h1 id="c-常用的输入输出方法"><a href="#c-常用的输入输出方法" class="headerlink" title="c++常用的输入输出方法"></a>c++常用的输入输出方法</h1><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><p>一维数组：</p><blockquote><p>输入包含一个整数n代表数组长度。<br>接下来包含n个整数，代表数组中的元素<br>3<br>1 2 3</p></blockquote><pre><code class="cpp">int n;scanf(&quot;%d&quot;,&amp;n); // 读入3，说明数组的大小是3vector&lt;int&gt; nums(n); // 创建大小为3的vector&lt;int&gt;for(int i = 0; i &lt; n; i++) &#123;    cin &gt;&gt; nums[i];&#125;// 验证是否读入成功for(int i = 0; i &lt; nums.size(); i++) &#123;    cout &lt;&lt; nums[i] &lt;&lt; &quot; &quot;;&#125;cout &lt;&lt; endl;</code></pre><p>若是不限定输入数据的大小</p><pre><code class="cpp">vector&lt;int&gt; nums;int num;while(cin &gt;&gt; num) &#123;    nums.push_back(num);    // 读到换行符，终止循环    if(getchar() == &#39;\n&#39;) &#123;        break;    &#125;&#125;// 验证是否读入成功for(int i = 0; i &lt; nums.size(); i++) &#123;    cout &lt;&lt; nums[i] &lt;&lt; &quot; &quot;;&#125;cout &lt;&lt; endl;</code></pre><h3 id="二维数组"><a href="#二维数组" class="headerlink" title="二维数组"></a>二维数组</h3><p>例如</p><blockquote><p>输出N行，每行M个空格分隔的整数。每个整数表示该位置距离最近的水域的距离。<br>4 4<br>0110<br>1111<br>1111<br>0110</p></blockquote><pre><code class="cpp">int n,m;int res[n][m];//vector&lt;vector&lt;int&gt;&gt;res(n,vector&lt;int&gt;(n));scanf(&quot;%d%d&quot;,&amp;n,&amp;m);for(int i=0;i&lt;n;i++)&#123;    for(int j=0;j&lt;m;j++)&#123;        scanf(&quot;%d&quot;,&amp;res[i][j]);    &#125;&#125;// 验证是否读入成功for(int i = 0; i &lt; m; i++) &#123;    for(int j = 0; j &lt; n; j++) &#123;        cout &lt;&lt; matrix[i][j] &lt;&lt; &quot; &quot;;    &#125;    cout &lt;&lt; endl;</code></pre><p>再附加每行数据用特殊字符给隔开的限制</p><pre><code class="cpp">int m; // 接收行数int n; // 接收列数cin &gt;&gt; m &gt;&gt; n;vector&lt;vector&lt;int&gt;&gt; matrix(m);for(int i = 0; i &lt; m; i++) &#123;    // 读入字符串    string s;    getline(cin, s);        // 将读入的字符串按照逗号分隔为vector&lt;int&gt;    vector&lt;int&gt; vec;    int p = 0;    for(int q = 0; q &lt; s.size(); q++) &#123;        p = q;        while(s[p] != &#39;,&#39; &amp;&amp; p &lt; s.size()) &#123;            p++;        &#125;        string tmp = s.substr(q, p - q);        vec.push_back(stoi(tmp));        q = p;    &#125;        //写入matrix    matrix[i] = vec;    vec.clear();&#125;// 验证是否读入成功for(int i = 0; i &lt; matrix.size(); i++) &#123;    for(int j = 0; j &lt; matrix[i].size(); j++) &#123;        cout &lt;&lt; matrix[i][j] &lt;&lt; &quot; &quot;;    &#125;    cout &lt;&lt; endl;&#125;</code></pre><p>结构体输入：</p><blockquote><p>第 1 行：正整数 n<br>第 2 行：第 1 个学生的姓名 学号 成绩<br>第 3 行：第 2 个学生的姓名 学号 成绩<br> … … …<br>第 n+1 行：第 n 个学生的姓名 学号 成绩</p></blockquote><blockquote><p>3<br>Joe Math990112 89<br>Mike CS991301 100<br>Mary EE990830 95</p></blockquote><pre><code class="cpp">struct Student&#123;  char name[11];  char subject[11];  int score;&#125;;int main()&#123;    int n;    scanf(&quot;%d&quot;,&amp;n);    Student* s = new Student[n];    for(int i=0;i&lt;n;i++)&#123;         cin&gt;&gt;s[i].name&gt;&gt;s[i].subject&gt;&gt;s[i].score;    &#125;    return 0;&#125;</code></pre>]]></content>
      
      
      <categories>
          
          <category> c++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
            <tag> ACM </tag>
            
            <tag> 刷题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Disentangled High Quality Salient Object Detection</title>
      <link href="/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/"/>
      <url>/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/</url>
      
        <content type="html"><![CDATA[<h1 id="Disentangled-High-Quality-Salient-Object-Detection"><a href="#Disentangled-High-Quality-Salient-Object-Detection" class="headerlink" title="Disentangled High Quality Salient Object Detection"></a><center>Disentangled High Quality Salient Object Detection</center></h1><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>将高分辨率SOD任务分解为低分辨率显著性分类网络(LRSCN)和高分辨细化网络(HRRN),作为一项逐像素分类任务，LRSCN旨在以低分辨率来捕获足够的语义，以识别明确的显著，背景和不确定区域。HRRN是一项回归任务，旨在准确提炼不明确区域中的像素的显著性值。</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>一种好的高分辨率显著目标检测方法不仅要准确地检测出整个显著目标，还要预测显著目标的精确边界。基于低分辨率设计的SOD模型无法直接套用到高分辨率图像中，最主要的原因就是，低分辨的方法往往是将识别和定位两个过程使用一个过程实现，而对于高分辨率图像最为重要的是对于边界的精确分割。对于显著区域的定位我们可以通过扩大感受野来获取足够的语义，但是由于高分辨的特性，这将使得内存的使用大大的增加，此时往往采用下采样操作，但是下采样操作不可避免地会使得结构信息丢失。这种解决问题的思路也就是导致低分辨的SOD模型直接迁移至高分辨率图像中会出现边界模糊的原因。如下图所示</p><h3 id="模型结果对比"><a href="#模型结果对比" class="headerlink" title="模型结果对比"></a><center>模型结果对比</center></h3><div align="center"><img src="/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/低分辨率SOD模型.PNG"></div>从上图可以发现，显著像素点可以分为以下三类：<blockquote><ul><li>(1)大多数显著对象内部的像素具有最高的显著值，我们称为确定的显著像素</li><li>(2) 背景区域中的大多数像素具有最低的显著值，我们称为确定的背景显著像素</li><li>(3) 模糊物体边界像素的显著值在0到1之间波动，称之为不明确像素</li></ul></blockquote><p>理想的 SOD 方法应有效识别图像中明确的显着区域和背景区域，并准确计算不确定区域中像素的显着性值，以保持清晰的目标边界。也就是一个回归任务加一个分类任务。<br>将高分辨率显着对象检测分离为低分辨率显着性分类网络(LRSCN)和高分辨率细化网络(HRRN).LRSCN旨在以低分辨率捕获足够的语义并将像素分类为三个不同的集合以供以后处理.HRRN旨在准确提炼不确定区域中像素的显着性值,以在GPU内存有限的情况下以高分辨率保持清晰的对象边界.如上所述,HRRN 需要高分辨率图像中的结构细节。然而，广泛使用的低分辨率显着性数据集通常在注释质量方面存在一些问题,几乎不可能从这些有缺陷的数据集中直接获得足够的对象边界细节来训练高分辨率网络。在最近的工作中，Zeng 等人。提出通过使用具有准确注释的高分辨率图像来训练他们的 SOD 网络。然而，如此高质量的图像标注需要大量的人工成本。在我们的论文中,我们认为没有必要在网络训练中使用这种精确注释的高分辨率图像.通过在训练过程中引入不确定性，我们的 HRRN 可以仅使用标注不佳的低分辨率训练数据集很好地解决高分辨率细化任务。</p><h3 id="模型方法"><a href="#模型方法" class="headerlink" title="模型方法"></a>模型方法</h3><h3 id="HRRN-High-Resolution-Network-Framework"><a href="#HRRN-High-Resolution-Network-Framework" class="headerlink" title="HRRN High Resolution Network Framework"></a><center>HRRN High Resolution Network Framework</center></h3><div align="center"><img src="/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/HRRN高分辨率网络框架.PNG"></div>LRSCN的目的是在低分辨率下获取足够的语义并将像素分为三个不同的集合，同时节省内存的使用，HRRN计算回归像素的显著性值，并在高分辨率下保持清晰的对象边界<p>LRSCN使用一个简单的U-Net编码器解码器架构，VGG-16作为主干网络，因此将从Conv1-2，Conv2-2，Conv3-3，Conv4-3，Conv5-3，Conv6-3获得六个特征，但是由于前两个特征的感受野太小，则不使用。在编码器和解码器之间增加一个多尺度特征提取和跨级特征融合模块(MECF)，以提高特征表示的可辨别性。解码器自上而下的方式融合MECF的输出特征和上一阶段的上采样特征，每个解码器的输出定义为$D_{i},i&#x3D;1,2,3…n$,最后SGA模块建立在D3之上用来生成准确的显著预测图T，为了回归清晰的目标边界值，HRRN的输入是在LRSCN提供的trimap引导下的高分辨率图像。HRRN具有基本的编码器-解码器架构，在不确定性损失的帮助下，网络可以对噪声数据更加鲁棒，并预测具有清晰边界的高分辨率显着图。</p><h4 id="LRSCN"><a href="#LRSCN" class="headerlink" title="LRSCN"></a>LRSCN</h4><p>###<center>LRSCN架构图</center></p><div align="center"><img src="/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/LRSCN架构图.PNG"></div>开发了一种基于全局卷积网络GCN的多尺度特征提取模块ME，以扩大特征感受野并获得多尺度信息。为了实现第二个目标，我们利用跨级别特征融合模块CF来利用不同级别的特征优势。此外，在设计网络架构时，受的启发，我们使用拆分变换策略进一步放大 特征感受野，从而产生更具辨别力的特征表示。具体来说，我们将输入F按通道维度均匀地分成两部分${ F_{1}, F_2}$，然后将$F_1$送入多尺度特征提取路径，将$F_2$送入跨级特征融合路径。这两个路径的输出连接在一起作为最终输出。我们称这个桥接模块为MECF模块，如上图所示。<h3 id="SGA模块："><a href="#SGA模块：" class="headerlink" title="SGA模块："></a>SGA模块：</h3><p>每个解码器融合来自MECF模块和前一解码器级的特征，然后使用$3×3$卷积层进行最终预测。为了保持trimap和显著图的一致性，确保trimap的不确定区域能够准确覆盖显著图的边界，我们在D3上设计了一个显著引导注意模块（SGA）。具体来说，我们首先使用$3×3$卷积和sigmoid函数来计算显著性映射。然后，将显著性图视为空间权重图，有助于细化特征并生成精确的trimap。最后，输出trimap T是3通道分类logits。整个SGA模块保证trimap和saliecny地图的对齐。 </p><h3 id="HRRN模块"><a href="#HRRN模块" class="headerlink" title="HRRN模块"></a>HRRN模块</h3><p>HRRN遵循解纠缠原则，在LRSCN提供的trimap的指导下，精确细化不确定区域中像素的显著性值，以保持高分辨率下清晰的目标边界。HRRN的架构如图2所示。HRRN有一个简单的类似U-NET的体系结构。为了在高分辨率下进行更好的预测，我们进行了一些非平凡的修改。首先，底层特征包含丰富的空间和细节信息，这些信息在恢复清晰的对象边界方面起着至关重要的作用，因此解码器在每个上采样块之前而不是在每个上采样块之后组合编码器特征。此外，我们使用一个两层的快捷块来对齐编码器特征通道，以进行特征融合。其次，为了让网络更加关注细节信息，我们通过一个快捷块将原始输入直接反馈到最后一个卷积层，以产生更好的结果。最后，从图像生成任务中学习，我们对每个卷积层使用谱归一化，以对网络的$Lipschitz$常数添加约束并稳定训练。 </p><p>为了监督LRSCN，我们应该生成trimap的GT表示为$T^{gt}$，它可以表示确定的显着、确定的背景和不确定的区域。如上所述，不确定区域主要存在于对象的边界处。因此，我们使用随机像素数（5、7、9、11、13）在对象边界处擦除和扩展二进制真实图，以生成GT不确定区域。 剩余的前景和背景区域代表明确的显着和背景区域。$T^{gt}$ 定义为：<br>$$<br>\Large T_{gt}(x,y) &#x3D;\begin{cases}<br>2 &amp; T_{gt}(x,y)\in definite salient \<br>0 &amp; T_{gt}(x,y)\in definite background \<br>1 &amp; T_{gt}(x,y)\in uncertain region \<br>\end{cases}<br>$$<br>其中$(x,y)$表示图像上的每个像素位置。如下图所示</p><h3 id="结果可视化"><a href="#结果可视化" class="headerlink" title="结果可视化"></a><center>结果可视化</center></h3><div align="center"><img src="/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/HRRN结果可视化.PNG"></div><p>对于trimap的监督我们使用交叉熵损失<br>$$\Large L_{trimap}&#x3D;\dfrac{1}{N}\sum_{i}-log(\dfrac{e^{T_{i} } }{\sum_{j} e^{T_{j} } })$$<br>为保障trimap的准确率，我们在trimap监督的基础上增加了额外显著性监督$L_{saliency}$，总损失是<br>$$\Large L_{LRSCN}&#x3D;L_{saliency}+L_{trimap}$$<br>不使用不确定性损失，因为LRSCN的主要目标是获取足够的语义，而不是精确的边界。</p><p>对于输入的高分辨率图像$I$，让$G^H$表示其背景真值，预测显著性图为$S^H$。我们利用$L_1$损失来比较预测显著性图和背景真值在明确的显著性和背景区域上的绝对差异：<br>$$\Large L_1 &#x3D; \dfrac{1}{E}\sum_{i\in E}|S_{i}^H-G_{i}^H|$$<br>其中$E$表示在$trimap$中被标记为明确显着或背景的像素数，$S_{H}^i$和$G_{H}^i$表示位置$i$处的预测值和$groundtruth$值。<br>由于数据集本身在注释质量方面存在一些问题，因此引入一个不确定损失来解决数据集本身带来的缺陷。使用高斯似然的方式建模不确定性<br>$$\Large p(y|f(x))&#x3D;N(f(x),\delta^2)$$<br>其中$\delta$表示测量的不确定性，$y$是输出，在最大似然推断中，我们将模型的对数似然最大化，表示为:<br>$$\Large logp(y|f(x))\propto-\dfrac{||y-f(x)||}{2\delta^2}-\dfrac{1}{2}log{\delta^2}$$<br>则不确定损失定义为：<br>$$\Large L_{uncertainty}&#x3D;\dfrac{||y-f(x)||^2}{2\delta^2}+\dfrac{1}{2}log\delta^2$$<br>将其转化为像素的表达形式：<br>$$\Large L_{uncertainty}&#x3D;\dfrac{1}{U}\dfrac{||S_{i}^H-G_{i}^H||}{2\delta_{i}^2}+\dfrac{1}{2}log\delta_{i}^2$$</p><h3 id="HRRN损失可视化"><a href="#HRRN损失可视化" class="headerlink" title="HRRN损失可视化"></a><center>HRRN损失可视化</center></h3><div align="center"><img src="/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/HRRN损失可视化.PNG"></div>]]></content>
      
      
      <categories>
          
          <category> SOD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SOD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Looking for the Detail and Context Devils:High-Resolution Salient Object Detection</title>
      <link href="/2023/04/11/Looking%20for%20the%20Detail%20and%20Context%20Devils/"/>
      <url>/2023/04/11/Looking%20for%20the%20Detail%20and%20Context%20Devils/</url>
      
        <content type="html"><![CDATA[<h1 id="Looking-for-the-Detail-and-Context-Devils-High-Resolution-Salient-Object-Detection"><a href="#Looking-for-the-Detail-and-Context-Devils-High-Resolution-Salient-Object-Detection" class="headerlink" title="Looking for the Detail and Context Devils:High-Resolution Salient Object Detection "></a><center>Looking for the Detail and Context Devils:High-Resolution Salient Object Detection </center></h1><p>缺乏显著对象的边界细节和语义上下文是低分辨率SOD数据集的一大弊端，本文设计了一个端到端的学习框架，称为DRFNet,使用一个共享特征提取器和两个有效的细化头构成。通过解耦细节和上下文信息，一个细化头采用全局感知和特征金字塔，在不增加太多计算负担的情况下，提升空间细节信息，从而缩小高级语义和低级细节之间的差距，另一个细化头采用混合扩张卷积块和分组上采样，这在提取上下文信息方面非常有效，基于双重细化，使得实现扩大感受野并从高分辨率图像中获取更多的判别特征。</p><p>高分辨率图像具有一个突出的特点包含更多可以覆盖范围和形状的结构对象和更多的细节信息。一方面高级上下文特征更适合检测大而混乱的对象，而小对象则受益于低级精细特征。不同层次特征的结合将为语义定位和细节增强提供更丰富的信息。</p><p>现有的基于FCN结构的方法一个缺点就是特征通常是以粗到精细的方式集成，它缺乏获取足够的局部和全局上下文信息或远程依赖关系的能力。导致不显眼的对象和混淆区域的准确性较差。大量的使用卷积操作使得对于算力和内存的要求变得极高，但如果将输入图像限制为相对较低的分辨率，又阻碍了细节感知和高分辨的实际需求。</p><p>现有的高分辨率图像像素级标记方法大致分为三大类，</p><blockquote><ul><li>1首先将高分辨率图像裁剪为低分辨图像，然后预测低分辨率结果并将其结果插值为原始图像大小。这种操作虽然简单但是图像空间细节的丢失是不可避免，导致出现对物体边界的错误预测</li><li>2设计轻型编码器-解码器网络,通过特征融合层次特征，之间提高空间分辨率并恢复一些缺少的细节，但是这种由于连续的下采样操作会带来空间信息的丢失且缺乏足够的对象的感受野</li><li>3 引入具有多个分支的不对称网络,每个分支以不同分辨率运行，即低分辨率图像中提取全局信息，高分辨率图像中提取精细细节，但是如何在不同分支上整合全局和局部信息还没一个很好的方法，由于高级语义和低级细节之间的差距，不好的融合方式可能使得它们在预测中出现奇怪的预测区域</li></ul></blockquote><h3 id="常见的HRSOD网络架构"><a href="#常见的HRSOD网络架构" class="headerlink" title="常见的HRSOD网络架构"></a><center>常见的HRSOD网络架构</center></h3><div align="center"><img src="/2023/04/11/Looking%20for%20the%20Detail%20and%20Context%20Devils/三种常用的HR网络架构.PNG"></div><h3 id="本文网络结构"><a href="#本文网络结构" class="headerlink" title="本文网络结构"></a><center>本文网络结构</center></h3><div align="center"><img src="/2023/04/11/Looking%20for%20the%20Detail%20and%20Context%20Devils/DRFNet.PNG"></div><h3 id="共享特征提取器"><a href="#共享特征提取器" class="headerlink" title="共享特征提取器"></a>共享特征提取器</h3><p>采用修改后的VGG-16和ResNet-18作为共享特征提取器</p><h3 id="Detail-Refinement-Head"><a href="#Detail-Refinement-Head" class="headerlink" title="Detail Refinement Head"></a>Detail Refinement Head</h3><p>DRH包括三个关键块：</p><blockquote><ul><li>1卷积特征缩减块(CFRB):该块旨在缩小多尺度深度特征的维数，本质上就是一个$1\times1$的卷积块，后面是批归一化和Relu激活函数，为减少高分辨率图像的计算和内存需求，卷积滤波器的数量设为为32</li><li>2深度特征上采样块(DFUB):采用C组的反卷积进行上采样，通过适当的上采样率，可以放大较深层的输出特征以匹配较浅层产生的特征，且进一步减少计算量</li><li>3全局感知特征交互块(GFIB)：由于接受域有限，无法获取足够的全局信息，为表达增强表现能力，首先对CFRB和DFUB的特征进行级联全局平均池化。然后将其转发到全连接层以生成全局权重向量,整个过程可以表示为<br>$$\Large \alpha_G &#x3D; \sigma(W_1<em>GAP([F_C,F_D])+b)$$<br>$$\Large F_R&#x3D;g(W_2</em>[F_C,F_D]+b)$$<br>$$\Large F_G &#x3D; \alpha_G\odot F_R$$</li></ul></blockquote><h3 id="Context-Refinement-Head"><a href="#Context-Refinement-Head" class="headerlink" title="Context Refinement Head"></a>Context Refinement Head</h3><p>在直接堆叠或使用金字塔结构扩大感受野的策略中具有两个非常明显的缺点。1：计算量大，占用内存，不适合高分辨率图像。2：缺乏捕获足够多尺度局部上下文信息的能力，导致对于不显眼的对象的准确性较差本文提出的CRH使用混合膨胀卷积和分组上采样组成。具体的来说就是使用一个混合扩张卷积块和一个分组上采样组成</p>]]></content>
      
      
      <categories>
          
          <category> SOD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SOD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pyramidal Feature Shrinking for Salient Object Detection</title>
      <link href="/2023/04/11/PFSNet/"/>
      <url>/2023/04/11/PFSNet/</url>
      
        <content type="html"><![CDATA[<h1 id="Pyramidal-Feature-Shrinking-for-Salient-Object-Detection"><a href="#Pyramidal-Feature-Shrinking-for-Salient-Object-Detection" class="headerlink" title="Pyramidal Feature Shrinking for Salient Object Detection"></a><center>Pyramidal Feature Shrinking for Salient Object Detection</center></h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>提出了一种金字塔型特征收缩网络(PFSNet) ，其目的是将相邻的特征节点按照层层收缩的方式进行聚合，从而使聚合后的特征融合有效的细节和语义，丢弃干扰信息。特别地，提出了一种金字塔收缩译码器(PSD) ，以渐近的方式分层聚合相邻特征。此外，我们提出了一个相邻融合模块(AFM)来进行相邻特征之间的空间增强，以动态加权特征和自适应融合适当的信息。此外，利用基于主干提取特征的尺度感知富集模块(SEM)获取丰富的尺度信息，生成具有扩张卷积的不同初始特征。</p><p>在SOD任务中，高低级特征在功能之间有着很大的差距，SOD算法的关键在于如何充分利用语义和细节信息，在最后两个特征的合并中，具有丰富细节和噪声的低级特征和高级特征相融合，但是当两种特征完全不同时，差异较大的特征直接融合会产生噪声，导致性能的下降.<br><img src="/2023/04/11/PFSNet/PSD.PNG"><br>本文提出了一种金字塔收缩解码器，将相邻特征定义为相似特征，将不相邻的特征定义为孤立特征，PSD仅收缩每层中类似的特征，经过几层收缩后，最时候当前输入的特征得以保留，然后配合AFM融合模块实现增强当前样本的特征并抑制不适合的特征，最后为了充分利用好多尺度信息，配合使用SEM。其对应的架构图如下所示：<br><img src="/2023/04/11/PFSNet/PFSNet架构.PNG"></p><h3 id="AFM模块"><a href="#AFM模块" class="headerlink" title="AFM模块"></a>AFM模块</h3><p>将待合并的特征视为父特征，合并后的特征视为子特征，AFM要实现的两个功能：</p><blockquote><p>（1）子特征应该继承适当当前输入样本的特征，并丢弃不适合的特征</p><p>（2）子特征要和父特征保持相同的尺寸</p></blockquote><p>首先通过逐元素乘法从父特征中提取共享特征，然后通过逐元素加法将共享特征加到父特征中从而增强它们，通过级联运算将两个处理后的特征合并，然后让它们依次通过全局平均化，$1\times1$卷积和softmax函数来生成权重向量，最后对权重向量和特征进行相应的乘法，得到加权后的特征，在特征加权后再使用$3\times3$卷积来压缩与副特征一致的子特征的通道，由于不同的特征具有不同的权重，因此在卷积计算受，具有较小权重的元素很少被子特征继承，通过这种方式，达到子特征继承重要的特征并丢弃更多的噪声的目的。</p><h3 id="PSD模块"><a href="#PSD模块" class="headerlink" title="PSD模块"></a>PSD模块</h3><p>本文首次提出将相邻特征扩展到层次融合。这样，我们就可以利用相邻特征融合的优势，实现多层次的特征融合，避免跳跃式融合操作。此外，从最后一个特征融合的位置来看，它可以直接集成基于 FPN 的框架中包含噪声的低层特征，而 PFSNet 则消除了大量的噪声。<br>PSD的核心目的是为了实现多特征集成，同时尽可能避免跳跃式特征融合的操作，PSD是由AFM组成的结构。合并特征的过程在相邻节点对中进行，首先使用backone提出五层特征$f_1,f_2,f_3,f_4,f_5$,然后使用AFM模块将相邻的特征$f_i,f_{i+1}$得到$f^{‘}_i$依次类推得到最后的特征$f$</p>]]></content>
      
      
      <categories>
          
          <category> SOD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SOD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Salient Object Detection via Dynamic Scale Routing</title>
      <link href="/2023/04/11/Salient%20Object%20Detection%20via%20Dynamic%20Scale%20Routing/"/>
      <url>/2023/04/11/Salient%20Object%20Detection%20via%20Dynamic%20Scale%20Routing/</url>
      
        <content type="html"><![CDATA[<h2 id="Salient-Object-Detection-via-Dynamic-Scale-Routing"><a href="#Salient-Object-Detection-via-Dynamic-Scale-Routing" class="headerlink" title="Salient Object Detection via Dynamic Scale Routing "></a><center>Salient Object Detection via Dynamic Scale Routing </center></h2><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>现有的SOD模型的编码器可以通过提取多尺度特征，并通过各种微妙的解码器组合特征，但是这个特征通常是固定的，实际上，在不同场景中配合使用不同的内核大小是更可取的，因此本文提出了一种动态的金字塔卷积模型，动态选择最适合的内核大小，其次提出了一种自适应双向解码器以最好适应基于DPConv的编码器。最重要的的亮点是它能够在特征尺度及其动态集合之间进行路由，使推理过程具有尺度感知能力</p>]]></content>
      
      
      <categories>
          
          <category> SOD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SOD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>像素级光场显著性检测</title>
      <link href="/2023/04/11/%E5%83%8F%E7%B4%A0%E7%BA%A7%E5%85%89%E5%9C%BA%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B/"/>
      <url>/2023/04/11/%E5%83%8F%E7%B4%A0%E7%BA%A7%E5%85%89%E5%9C%BA%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在一个统一的框架中识别干净的标签且有效的整合光场线索之间的关系。将学习描述为光场内特征融合流和场景间相关流的联合优化，以生成预测，首先引入一个像素遗忘引导融合模块，以相互增强光场特征，并利用迭代过程中的像素一致性来识别噪声像素，再引入跨场景噪声惩罚损失，以更好地反映训练数据的潜在结构，并使学习对噪声保持不变。</p><blockquote><p>光场图像：</p><ul><li>光场：是一个四维的参数化表示，是空间中同时包含位置和方向信息的四维光辐射场，简单地说，涵盖了光线在传播中的所有信息，在空间内任意角度，任意的位置都可以获得整个空间环境的真实信息，用光场获得的图像信息更加全面，品质更好。<br><img src="/./image/%E5%85%89%E5%9C%BA.PNG"></li><li>光场成像的原理：传统相机成像是光线穿过镜头，而后传播到成像平面，光场成像则是在传感器平面添加了一个微透镜矩阵，在于将穿过主镜头的光线再次穿过每个微透镜，从而收获到光场的方向与位置信息，使成像结果在后期更加可调节，达到先拍照后聚焦的效果。<br><img src="/./image/%E5%85%89%E5%9C%BA%E6%88%90%E5%83%8F.PNG"></li></ul></blockquote><p>直接在像素级别噪声标签上训练显著性检测网络可能会引导网络过度适应损坏的标签。且当前现有的方式都缺乏全局视角来探索整个数据集之间的关系模式  </p><blockquote><h3 id="光场显著性："><a href="#光场显著性：" class="headerlink" title="光场显著性："></a>光场显著性：</h3></blockquote>]]></content>
      
      
      <categories>
          
          <category> SOD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SOD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation</title>
      <link href="/2023/04/11/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87Transformer/"/>
      <url>/2023/04/11/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87Transformer/</url>
      
        <content type="html"><![CDATA[<h1 id="Multi-Scale-High-Resolution-Vision-Transformer-for-Semantic-Segmentation"><a href="#Multi-Scale-High-Resolution-Vision-Transformer-for-Semantic-Segmentation" class="headerlink" title="Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation"></a><center>Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation</center></h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>VITs主要是为生成低分辨率表示的图像分类任务而设计的，这使得VITs的语义分割等密集预测任务具有挑战性，本文提出的HRVIT，通过高分辨率多分枝架构与ViT集成来增强ViT以学习语义丰富和空间精确的多尺度表示，通过各种分支块协同优化技术平衡HRVIT的模型行恩那个和效率</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>VIT的单尺度和低分辨率表示对于需要高敏感性和细粒度图像细节的语义分割不友好。已有的多尺度VIT网络大多遵循类似于分类的网络拓扑，具有顺序或串联架构，基于复杂性考虑，都是逐渐对特征图进行下采样，以提取更高级别的低分辨表示，并将每个阶段的输出直接馈送到下游分割头，这样的顺序结构缺乏足够的跨尺度交互，因此没法产生高质量的高分辨率表示</p><p>HRVIT并行提取多分辨率特征并反复融合它们以生成具有丰富语义信息的高质量HR表示。简单的将HRNET中所有的卷积残差块替换为Transformer将遇到严重的可扩展性问题，如果没有良好的架构块协同优化，从多尺度继承的高表示能力可能会被硬件上令人望而却步的延迟和能源成本所击倒。因此本文使用以下方式进行优化</p><ul><li>1）HRViT的多分支HR架构在跨分辨率融合的同时提取多尺度特征</li><li>2）使用增强局部注意力消除率冗余键和值以提高效率，并通过额外的并行卷积路径，额外的非线性单元和用于特征多样性增强的辅助快捷方式来增强模型的表达能力。</li><li>3）HRViT采用混合尺度卷积前馈网络加强多尺度特征提取</li><li>4）HRVIT的HR卷积结构和高效的补丁嵌入层在降低硬件成本的情况下保持率更多的低级细粒度特征</li></ul><h3 id="HRViT网络结构图"><a href="#HRViT网络结构图" class="headerlink" title="HRViT网络结构图"></a><center>HRViT网络结构图</center></h3><div align="center"><img src="/2023/04/11/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87Transformer/HRViT.PNG"></div>由图可知，HRViT第一部分由卷积干组成，同时提取低层特征，在卷积stem后，HRViT部署了四个渐进式Transformer阶段，其中第n阶段包含n个并行的多尺度Transformer分支，每个阶段可以有一个或多个模块。每个模块从一个轻量级密集融合层开始，以实现跨分辨率交互和一个用于局部特征提取的有效补丁嵌入块，然后是重复的增强局部自注意力块和混合尺度卷积前馈网络，与逐步降低空间维度以生成金字塔特征的顺序ViT主干不同，我们在整个网络中维护HR特征，以通过跨分辨率融合增强HR表示的质量。<p>多分支HRNet和self-attention运算所带来的高度复杂性会迅速导致内存占用，参数大小急剧上升，计算成本爆炸性增长，简单地在每个模块上分配相同局部注意力窗口大小的块将导致巨大的计算成本，根据对于复杂性分析，</p><div align="center"><img src="/2023/04/11/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87Transformer/复杂度分析.PNG"></div>最后决定使用狭窄的关注窗口代销，并在两条HR路径上使用最小数量的块。在中等分辨率的第三个分支，使用具有大窗口的深度分支，以提供大的感野和提取良好的高级特征。低分辨率的分支包含大多数参数们对于提供具有全局感受野的高级特征以及生成粗分割图非常有用，但是较低的空间尺度会导致图像细节丢失过多，因此旨在低分辨率的分支上部署几个大窗口块，已在参数预算下提高高级特征质量。十字形self-attention算子<div align="center"><img src="/2023/04/11/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87Transformer/HRVITAttn.PNG"></div><h4 id="cross-shaped-self-attention"><a href="#cross-shaped-self-attention" class="headerlink" title="cross-shaped self-attention"></a>cross-shaped self-attention</h4><ul><li>细粒度注意力</li><li>近似全局视图：通过两个平行正交的局部注意力，能够收集全局信息</li><li>可伸缩复杂性：窗口是一个维度固定的，避免了图像大小的二次复杂性</li></ul><p>遵循CSWin中的十字形窗口划分方法，将输入$x\in R^{H\times W\times C}$分成两部分${x_H,x_V \in R^{H\times W\times C&#x2F;2}}$,$x_H$被分割成不相交的水平窗口，而另外一半$x_V$被分割成垂直窗口。将窗口设置为$s\times W$或者$H\times s$，在每个窗口中，将补丁分块为$K$个$d_k$维头部，然后应用局部self-attention。将零填充应用于输入$x_H$或$x_V$，以允许完整的第k个窗口，然后将注意力图中的填充区域屏蔽为0，以避免不连贯的语义关联<br>原始的QKV线性层在计算和参数方面非常昂贵，因此共享键和值张量的线性投影，以节省计算和参数，此外，引入一个辅助路径，该路径具有并行深度方向卷积，以注入归纳偏置以促进训练，与CSWin中的局部位置编码不同，我们的并行路径是非线性的，并且在没有窗口划分的情况下应用于整个4—D特征映射$W^Vx$而没有窗口分区，这条路径可以被视为一个反向残差模块，它与self-attention中的线性投影层共享逐点卷积。这种共享路径可以有效注入归纳偏差，并以边际硬件开销的情况下增强局部特征聚合，作为对上述键值共享的性能补偿，引入一个额外的Hardswish函数来改善非线性，附加一个初始化为恒等投影的BatchNorm层以稳定分布以获得更好的可训练性，此外还添加了一个通道式投影作为多样性增强快捷方式，与传统增强的快捷方式不同，此快捷方式具有更高的非线性，不依赖于对硬件不友好的傅里叶变换。</p><h3 id="混合尺度卷积前馈网络"><a href="#混合尺度卷积前馈网络" class="headerlink" title="混合尺度卷积前馈网络"></a>混合尺度卷积前馈网络</h3><div align="center"><img src="/2023/04/11/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87Transformer/MiXFCN.PNG"></div><p>受到MIT的MIxFFN和HR-NAS中多分支倒置残差块的启发，我们通过在两个线性层之间插入多个尺度深度卷积路径来设计混合尺度卷积，在MiXCFN中，在LayerForm之后，我们将信道按r的比例展开，然后将其分成两个分支，$3\times 3$和$5 \times 5$深度方向卷积用于增加HRViT的多尺度局部信息提取，出于效率的考虑，我们利用信道冗余，将MiXCFN扩展比r从4降到3或者2，</p><h3 id="下采样部分"><a href="#下采样部分" class="headerlink" title="下采样部分"></a>下采样部分</h3><p>self-attention的复杂度与图像大小成二次方，为解决大图像是的可伸缩性问题，在输入端对图像进行4倍的下采样，不在stem中使用注意操作，因为早期卷积比self-attention更能有效的提取低级特征，作为早期的卷积，遵循HRNet中的设计，并使用两个步长为2的CONV-BNReLU块作为更强的下采样stem，以提取C通道特征，并保留更多信息，这与之前使用步长为4的卷积ViTs不同.</p><p>在每个模块中的Transformer块之前，我们在分支上添加一个补丁嵌入块，用于匹配通道并通过增强的补丁之间通信提取补丁信息，但是n阶段的每个模块将会有n个嵌入块所带来的巨大算力代价，我们将补丁嵌入简化为逐点CONV，然后是深度CONV。</p><p>交叉分辨率融合，在每个模块的开头插入重复的交叉分辨率融合层。为了帮助LR特征保持更多的图像细节和精准的位置信息，我们将它们与下采样的HR特征合并，不使用基于渐进卷积的下采样路径来匹配张量形状，而是采用直接下采样路径来最小化算力开销，在第i个输入和第j个输出之间的下采样路径中，使用步长为$2j-i$的深度可分离卷积来缩小空间维度并匹配输出通道。</p><p>多尺度ViT分层架构来逐步下采样的金字塔特征。PVT将金字塔结构集成到ViT中以进行多尺度特征提取，Twins交织局部和全局注意力以学习多尺度表示，SegFormer提出了一种有效的分层编码器来提取粗略和精细的特征，CSWin通过多尺度十字形局部注意力进一步提高性能。</p><p>用于语义分割的多尺度表示学习：原有的分割框架是逐步对特征图进行下采样以计算LR表示，并通过上采样恢复HR特征，例如SegNet，UNet，Hourglass，HRNet通过跨分辨率融合在整个网络中维护HR表示，Lite-HRNet提出了条件通道加权块来跨分辨率交换信息，HR-NAS搜索反转残差块和辅助Transformer分支的通道</p>]]></content>
      
      
      <categories>
          
          <category> 默认分类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> High-Resolution Vision </tag>
            
            <tag> Transformer </tag>
            
            <tag> Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于深度质量的特征操作实现高效的RGBD显著目标检测</title>
      <link href="/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
      <url>/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a><center>摘要</center></h2><p>基于RGBD显著目标检测模型在减少模型参数时，模型精度通常会下降，且受到深度质量的影响。本文设计了一种基于深度质量的特征操作(DQFM)：利用低级RGB和深度特征的对齐，以及深度流的整体关注来明确控制和增强跨模态融合。这是一个轻量化模型。  </p><p>高质量的深度图通常具有一些与相应RGB图像对齐的边界</p><h3 id="Efficient-RGBD-SOD-Method"><a href="#Efficient-RGBD-SOD-Method" class="headerlink" title="Efficient RGBD SOD Method"></a>Efficient RGBD SOD Method</h3><p>将知识蒸馏就是用于深度蒸馏器，将从深度流获取到的深度知识转移到RGB流，从而实现无深度推理框架，后Chen设计了一个定制的深度主干，以提取互补特征</p><h3 id="网络结构图"><a href="#网络结构图" class="headerlink" title="网络结构图"></a><center>网络结构图</center></h3><div align="center"><img src="/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/DFM-Net.PNG"></div><p>由编码器和解码器构成，其中RGB分支同时负责RGB特征提取和RGB与深度特征之间的交叉模式融合，另一方面解码器部分负责进行简单的两阶段融合以生成最终的显著性图，具体的说就是：编码器包括一个基于MobileNet-v2的RGB相关分支，一个深度相关分支，以及DQFM。在某个层次提取的深度特征通过DQFM后，再经过简单的元素加法融合到RGB分支中，然后发送到下一个层次。为了捕获多尺度语义信息，在RGB分支的末尾添加了PPM(金字塔池模块),在实际操作中，DQFM包含两个连续操作，深度质量启发加权和深度整体注意。  </p><h3 id="DQW结构"><a href="#DQW结构" class="headerlink" title="DQW结构"></a><center>DQW结构</center></h3><div align="center"><img src="/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/DQW%E7%BB%93%E6%9E%84.PNG"></div><p>首先将低层特征$f_{r}^{1}$ 和 $f_{d}^{1}$ 转化为特征$ f_{rt^{‘} }$和$ f_{dt^{‘} }$,<br>$$ \large f_{rt^{‘} }&#x3D;BConv_{1\times1}(f_{r}^{1}),f_{dt^{‘} }&#x3D;BConv_{1\times1}(f_{d}^{1})$$</p><p>其中$BConv$表示$1\times1$卷积和$ReLU$激活函数，为了评估低级特征对齐，对这两个特征进行对齐编码<br>$$ \large V_{BA}&#x3D;\dfrac{GAP(f_{rt^{‘} }\otimes f_{dt^{‘} })}{GAP(f_{rt^{‘} } + f_{dt^{‘} })}$$</p><p>其中$GAP(\cdot)$表示全局平均池化操作，$\otimes$表示按元素乘法。增强向量的计算方式：<br>$$ \large V_{BA}^{ms}&#x3D;[V_{BA},V_{BA}^{1},V_{BA}^{2}]$$<br>其中[$\cdot$]表示通道串联。然后使用两个完全连接的层使得$\alpha\in\mathbb{R}^{5}$转化到$V_{BA}^{ms}$计算方式为：<br>$$\large\alpha&#x3D;MLP(V_{BA}^{ms})$$<br>$MLP(\cdot)$表示末端为$Sigmoid$函数的感知器。</p><h3 id="DHA结构"><a href="#DHA结构" class="headerlink" title="DHA结构"></a><center>DHA结构</center></h3><div align="center"><img src="/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/DHA%E7%BB%93%E6%9E%84.PNG"></div><p>首先利用最高级特征$f_{d}^{5}$从深度流定位粗糙的突出区域，使用压缩和上采样方式使得$f_{d}^{5}$转化为$f_{dht}$计算方式为<br>$$\large f_{dht}&#x3D;F_{UP}^{8}(BConv_{1\times1}(F_{DN}^2(f_{d}^5))$$</p><p>其中$F_{UP}^{8}$表示8层双线性上采样，然后结合低层RGB和深度特征进行重新校准。为了更好地模拟低水平和高水平特征之间的长期依赖性，同时保持DHA的效率，我们采用最大池运算和扩大卷积来快速增加感受野。重新校准过程定义为：<br>$$ \large F_{rec}(f_{dht})&#x3D;F_{UP}^{2}(DConv_{3\times3}(F_{DN}^{2}(f_{dht}+f_{ec})))$$</p><p>$F_{rec}(\cdot)$表示重新校准过程。$DConv_{3\times3}(\cdot)$表示$3\times3$扩张卷积，步长为1，扩张率为2.然后是$BatchNorm$和$ReLU$激活函数，$F_{UP}^{2}(\cdot)&#x2F;F_{DN}^{2}(\cdot)$表示双线性上采样\下采样操作。为提高性能，再进行两次重新校准。<br>$$\large f_{dht}^{‘}&#x3D;F_{rec}(f_{dht}),f^{‘’}<em>{dht}&#x3D;F</em>{rec}(f^{‘}_{dht})$$</p><p>最终实现整体注意力地图：<br>$$\large \beta&#x3D;BConv_{3 \times 3}(f_{ec}+f_{dht}^{‘’})$$</p><p>最后获得五张深度整体注意图$\large{ {\beta_{1},\beta_{2},\beta_{3},\beta_{4},\beta_{5} }}$如下图所示：  </p><div align="center"><img src="/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/DHA%E5%8F%AF%E8%A7%86%E5%8C%96.PNG"></div><p>通常情况下，深度学习不如RGB图像，为实现效率和准确性的平衡，本文选择定制深度主干(TDB)，具体来说就是基于$MobliceNetV2$中的反向剩余瓶颈块(IRB)并构建一个新的更小的主干，减少信道数量和堆叠块。结构如下：</p><div align="center"><img src="/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/%E5%AE%9A%E5%88%B6TDB%E6%A8%A1%E5%9D%97.PNG"></div><h2 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h2><p>简化版的两级解码器，包括预融合和完全融合，预融合是通过信道压缩和层次分组来减少特征信道和层次，完全融合则是进一步聚合低层和高层特征，生成最终的显著图。</p><h3 id="预融合阶段"><a href="#预融合阶段" class="headerlink" title="预融合阶段"></a>预融合阶段</h3><p>首先使用具有$BatchNorm$和$ReLU$激活的$3\times3$深度可分离卷积，表示为$DSConv_{3\times3}$,将压缩编码器特征$f_{c}^{i},(i&#x3D;1,2,…6)$到统一信道16，然后使用通道注意算子$F_{CA}$通过加权不同信道来增强特征。这个过程可以表示为：<br>$$\large cf_{c}^{i}&#x3D;F_{CA}(DSConv_{3\times3}(f_{c}^{i}))$$<br>其中$cf_{c}^{i}$表示压缩和增强功能。为了减少特征层次，作者将6个层次分为两个层次(低级层次和高级层次)<br>$$ \large cf_{c}^{low}&#x3D;\sum_{i&#x3D;0}^{3}F_{UP}^{2^{i-1} }(cf_{c}^{i}),cf_{c}^{high}&#x3D;\sum_{i&#x3D;4}^{6}cf_{c}^{i}$$</p><h3 id="聚合模块"><a href="#聚合模块" class="headerlink" title="聚合模块"></a>聚合模块</h3><p>由于在预融合阶段，信道数量和层次已经减少，在全融合阶段，我们直接将高层和低层层次串联起来，然后将串联馈送到预测头，以获得最终的全分辨率预测图，表示为：$$ \large S_c&#x3D;F_{p}^{c}([cf_{e}^{low},F_{UP}^{8}(cf_{c}^{high})])$$其中$S_c$表示最终的显著性图，$F_{p}^{c}(\cdot)$表示一个预测头，由两个$3\times3$深度方向可分离卷积（然后是$BatchNorm$层和$ReLU$激活）、一个$3\times3Sigmoid$激活卷积以及一个$2\times$双线性上采样组成，以恢复原始输入大小。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>总损失$\pounds$最终由深度分支损失$\pounds_{c}$和深度监管损失$\pounds_{d}$构成，<br>$$\large \pounds &#x3D; \pounds_{c}(S_{c},G)+ \pounds_{d}(S_{d},G)$$<br>我们使用的是标准的交叉熵损失</p>]]></content>
      
      
      <categories>
          
          <category> SOD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SOD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zoom In and Out:A Mixed-scale Triplet Network for Camouflaged Object Detection</title>
      <link href="/2023/04/11/%E6%B7%B7%E5%90%88%E6%AF%94%E4%BE%8B%E4%B8%89%E5%B1%82%E9%87%8D%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E4%BC%AA%E8%A3%85%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
      <url>/2023/04/11/%E6%B7%B7%E5%90%88%E6%AF%94%E4%BE%8B%E4%B8%89%E5%B1%82%E9%87%8D%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E4%BC%AA%E8%A3%85%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="Zoom-In-and-Out-A-Mixed-scale-Triplet-Network-for-Camouflaged-Object-Detection"><a href="#Zoom-In-and-Out-A-Mixed-scale-Triplet-Network-for-Camouflaged-Object-Detection" class="headerlink" title="Zoom In and Out:A Mixed-scale Triplet Network for Camouflaged Object Detection"></a>Zoom In and Out:A Mixed-scale Triplet Network for Camouflaged Object Detection</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>最近提出的伪装目标检测（COD）试图分割视觉上融入周围环境的目标，这在现实场景中是极其复杂和困难的。除了伪装对象与其背景之间具有很高的内在相似性外，这些对象通常在尺度上具有多样性，外观模糊，甚至被严重遮挡。为了解决这些问题，我们提出了一种混合比例的三重网络ZoomNet，它模仿人类在观察模糊图像时的行为，即放大和缩小。具体而言，我们的ZoomNet采用缩放策略，通过设计的尺度积分单元和分层混合尺度单元学习区分性混合尺度语义，充分挖掘候选对象和背景环境之间的细微线索。此外，考虑到来自不可区分纹理的不确定性和模糊性，我们构造了一个简单而有效的正则化约束，即不确定性感知损失，以促进模型在候选区域准确地生成具有更高置信度的预测。我们提出的高度任务友好的模型在四个公共数据集上始终优于现有的23种最先进的方法。此外，与最新的前沿模型相比，该模型在SOD任务上的优异性能也验证了该模型的有效性和通用性。</p><h2 id="COD任务难点"><a href="#COD任务难点" class="headerlink" title="COD任务难点"></a>COD任务难点</h2><blockquote><ul><li>如何在外观不明显和各种尺度的情况下准确定位伪装对象</li><li>如何抑制来自背景的明显干扰，更可靠地推断伪装对象</li></ul></blockquote><p>为了准确地找到场景中模糊或伪装的对象，人类可以尝试通过放大和缩小图像来参考和比较不同尺度下形状和外观的变化，这种行为模式为本文提供思路，可以通过模拟人类放大和缩小策略来识别伪装的物体。本文中提出一种混合规模的三重网络$ZoomNet$。为了精准定位目标，我们使用尺度空间理论来模拟放大和缩小策略，为此设计了两个关键模块</p><blockquote><ul><li>规模集成单元(SIU):筛选和聚合特定尺度的特征</li><li>分层混合规模单元(HMU):重组和增强混合尺度特征</li></ul></blockquote><p>此结构能够在混合尺度下挖掘出物体和背景之间准确而微妙的语义线索，并产生准确的预测，为了实现效率和有效性的平衡，模型采用共享权重策略，为增强模型在复杂场景下的泛化能力，设计了一个不确定性感知损失(UAL)来指导模型训练，模型结构图：</p><div align="center"><img src="/2023/04/11/%E6%B7%B7%E5%90%88%E6%AF%94%E4%BE%8B%E4%B8%89%E5%B1%82%E9%87%8D%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E4%BC%AA%E8%A3%85%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/image\ZoomNet可视化.PNG"></div><h2 id="网络结构图"><a href="#网络结构图" class="headerlink" title="网络结构图"></a>网络结构图</h2><div align="center"><img src="/2023/04/11/%E6%B7%B7%E5%90%88%E6%AF%94%E4%BE%8B%E4%B8%89%E5%B1%82%E9%87%8D%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E4%BC%AA%E8%A3%85%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/image\ZoomNet.png"></div>SIU：使用一个尺度作为主尺度，另外两个尺度作为辅助，利用共享的三元组特征编码器来提取不同尺度的特征并将它们馈送到尺度合并层。对于高尺度，使用最大池化加平均池化的混合结构进行下采样，这有助于在高分辨率特征中保持伪装对象的有效和多样性响应。对于低尺度使用双线性插值直接向上采样，然后将这些特征输入注意力生成器，并通过一系列卷积层计算出三通道特征图。然后再softmax激活层之后，可以获得对应于每个尺度的注意力映射计算权重为：<div align="center"><img src="/2023/04/11/%E6%B7%B7%E5%90%88%E6%AF%94%E4%BE%8B%E4%B8%89%E5%B1%82%E9%87%8D%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E4%BC%AA%E8%A3%85%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/image\SIU.png"></div>$$\Large A_i=softmax(\Psi[U(f^{0.5}_i,f^{1.0}_i,D(f^{1.5}_i)],\phi)$$$$\Large f_i=A^{0.5}_i\cdot U(f^{0.5}_i)+A^{1.0}_i\cdot f^{1.0}_i+A^{1.5}_i\cdot D(f^{1.5}_i)$$]]></content>
      
      
      <categories>
          
          <category> COD </category>
          
      </categories>
      
      
        <tags>
            
            <tag> COD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation</title>
      <link href="/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/"/>
      <url>/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="Pyramid-Grafting-Network-for-One-Stage-High-Resolution-Saliency-Detection"><a href="#Pyramid-Grafting-Network-for-One-Stage-High-Resolution-Saliency-Detection" class="headerlink" title="Pyramid Grafting Network for One-Stage High Resolution Saliency Detection"></a><center>Pyramid Grafting Network for One-Stage High Resolution Saliency Detection</center></h1><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>由于采样深度和感受野大小之间的矛盾，大多数根据低分辨输入设计的SOD模型在高分辨率图像中表现不佳，本文提出一种金字塔嫁接网络。使用transformer和CNN主干分别从不同分辨率的图像中提取特征，然后将特征从transformer分支嫁接到CNN分支，与此同时提出一种基于注意的分叉模型嫁接模块，使CNN分支能够在解码过程中，在不同信源特征的引导下，更全面地组合破碎的细节信息，此外还设计了一个注意引导丢失来明确监督交叉嫁接模块生成注意矩阵，以帮助网络更好地与来自不同模型的注意进行交互。</p><h3 id="困境"><a href="#困境" class="headerlink" title="困境"></a>困境</h3><p>当前主流的SOD模型遇到高分辨率的图像，为了减少内存的开销往往会将图像先下采样然后对输出结果上采样已恢复原始分辨率，由于现在的SOD模型都是使用编码器-解码器的方式设计的，随着分辨率的大幅度提高，提取的特征大小会增加，但是网络的感受野是固定的，使得相对感受野变小，最终导致无法捕获对任务至关重要的全局语义。</p><div align="center"><img src="/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/高分辨率下的传统模型困境.PNG"></div> <p>目前对于高分辨率SOD方法有两种主流：HRSOD和DHQSOD，其中HRSOD将整个过程分为全局阶段，局部阶段和重组阶段，全局阶段为局部阶段和作物过程提供指导。DHSOD将SOD任务分解为分类任务和回归任务，这两个任务通过他们提出的trimap和不确定性损失连接起来，它们生成具有清晰边界的相对较好的显著性贴图。但是这两者都是使用多阶段架构，将SOD分为语义(低分辨率)和细节(高分辨率)两个阶段。由此引出两个问题：</p><blockquote><ul><li>阶段间语境语义迁移不一致，在前一个阶段获得的中间映射被输入到最后一个阶段，同时错误也被传递，由此后续细化阶段可能将继续放大错误</li><li>耗时，与单阶段相比，多阶段方法不仅难以并行且参数过多，模型运行运行速度较慢</li></ul></blockquote><h3 id="高分辨率SDO发展"><a href="#高分辨率SDO发展" class="headerlink" title="高分辨率SDO发展"></a>高分辨率SDO发展</h3><p>Zeng等人<a href="https://ieeexplore.ieee.org/document/9008818">Towards High-Resolution Salient Object Detection</a>提出了一种高分辨率显著目标检测范式，使用GSN提取语义信息，使用APS引导的LRN优化局部细节，最后使用GLFN进行预测融合。他们还提供了第一个高分辨率显著目标检测数据集（HRSOD）。Tang等人<a href="https://ieeexplore.ieee.org/document/9709916">Disentangled high quality salient object detection</a>提出，显著目标检测应分为两项任务。他们首先设计LRSCN以在低分辨率下捕获足够的语义并生成trimap。通过引入不确定性损失，所设计的HRRN可以对第一阶段使用低分辨率数据集生成的trimap进行细化。然而，它们都使用多级体系结构，这导致推理速度较慢，难以满足某些实际应用场景。更严重的问题是网络之间的语义不一致。</p><p>使用常用的SOD数据集通常分辨率较低，用他们来训练高分辨率网络和评估高质量分割存在以下几点缺点:</p><blockquote><ul><li>图像分辨率低导致细节不足</li><li>注释边缘的质量较差</li><li>注释的更加精细级别不够令人满意</li></ul></blockquote><p>当前可用的高分辨率数据集是HRSOD，但是HRSOD数据集图像数量有限,严重影响模型的泛化能力。</p><h3 id="Staggered-Grafting-Framework"><a href="#Staggered-Grafting-Framework" class="headerlink" title="Staggered Grafting Framework"></a>Staggered Grafting Framework</h3><p>网络框架如图所示：</p><div align="center"><img src="/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/Pyramid_Grafting_network.PNG"></div> <p>由两个编码器和一个解码器构成，使用Swim transformer和ResNet18作为编码器，transformer编码器能够在低分辨率情况下获得准确的全局语义信息，卷积编码器能够在高分辨率输入下获得丰富的细节信息，不同模型之间的提取的特征可能是互补的，可以获得更多的有效特征。</p><p>在编码的过程中，向两个编码器馈送不同分辨率的图像，并行获取全局语义信息和详细信息。解码分为三个过程，一个是Swim解码，一个是嫁接编码，最后是交错结构的ResNet解码，在第二个子阶段的解码特征是从跨模态移植模块产生的，其中全局语义信息从Swin分支移植到ResNet分支，跨模态移植模块还会处理一个名为CAM的矩阵进行监督</p><h3 id="交叉模型迁移模块-CMGM"><a href="#交叉模型迁移模块-CMGM" class="headerlink" title="交叉模型迁移模块(CMGM)"></a>交叉模型迁移模块(CMGM)</h3><p>作用：移植由两个编码器提取的特征，对于transformer所提取的特征$f_{S_2}$能够远距离捕获信息，具有全局语义信息。使用ResNet所得到的$f_{R_5}$有更好的局部信息，也就是更丰富的细节信息。但是由于特征大小和感受野之间的差异，在$f_{R_5}$中有更多的噪声。</p><p>使用常见的融合方法：如逐元素相加和相乘的适用情况限制在显著预测和不同特征生成的预测至少有一部分是对的情况下，否则就是一种错误的适用方式，且这种操作都只关注于有限的局部信息，导致没法实现自我纠错。</p><p>作者提出使用CMGM重新计算ResNet特征和Transformer特征之间的逐点关系，将全局语义信息通过transformer分支转移到ResNet分支，从而弥补常见的错误，通过计算$E&#x3D;|G-P| \in [0,1]$得到误差图</p><h3 id="CMGM纠错效果图"><a href="#CMGM纠错效果图" class="headerlink" title=" CMGM纠错效果图"></a><center> CMGM纠错效果图</center></h3><div align="center"><img src="/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/CMGM纠错.PNG"></div> <h3 id="CMGM网络结构"><a href="#CMGM网络结构" class="headerlink" title=" CMGM网络结构"></a><center> CMGM网络结构</center></h3><div align="center"><img src="/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/CMGM网络结构.PNG"></div><h3 id="实验结果"><a href="#实验结果" class="headerlink" title=" 实验结果"></a><center> 实验结果</center></h3><div align="center"><img src="/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/PGNet定量实验结果.PNG"></div><h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a><center>可视化</center></h3><div align="center"><img src="/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/PGNet可视化结果.PNG"></div><p>CUDA_VISIBLE_DEVICES&#x3D;2,3 python3 -m torch.distributed.launch –nproc_per_node&#x3D;2 train_distributed.py –batchsize 4 –master_port 29501 –savepath “..&#x2F;model&#x2F;PGNet_DUTS_Test&#x2F;“ –datapath “&#x2F;storage&#x2F;GWB&#x2F;Data&#x2F;DUTS-TR”\</p>]]></content>
      
      
      <categories>
          
          <category> 默认分类 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
            <tag> Segmentation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Liunx常用命令</title>
      <link href="/2022/10/07/Liunx%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2022/10/07/Liunx%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p>pwd：显示当前工作目录的绝对路径</p><pre><code>ls:（1）-a：显示当前目录的所有文件和目录包括隐藏的   （2）-l：一列表的方式显示信息</code></pre><p>mkdir：用于创建目录，默认是单级目录，创建多级目录加一个 -p即可</p><p>rmdir:删除空目录，如果目录下面有内容时不可以删除，如果要强行删除有内容的文件夹使用 rm -rf删除</p><p>touch指令：创建空文件</p><p>cp：拷贝指令，拷贝文件到指定目录。基本语法：cp [选项] source dest 其中-r代表可以使用递归复制的方式拷贝整个文件夹</p><p>mv:移动文件与目录或者重命名</p><pre><code>基本语法：（1）mv oldNameFile newNameFile （重命名）         （2）mv /temp/movefile /targetFolder (移动文件)</code></pre><p>cat：查看文件指令， cat [选项] 要查看的文件 -n ：显示行号</p><p>less:用来分屏查看文件内容</p><p>echo：输出内容到控制台 用法：echo [选项] [输出内容]</p><p>head：用于显示文件的开头部分内容，默认情况下是显示文件的前10行内容</p><p>tail：用于显示文件中尾部的内容，默认情况下tail指令显示文件的前10行内容。语法</p><pre><code>（1）tail文件：查看文件尾部10行内容（2）tail -n 5 文件 查看文件尾5行内容，数字可以随便（3）tail -f 文件 实时追踪该文档的所有更新</code></pre><p> $&gt;$ 指令和$&gt;&gt;$指令：输出重定向和追加</p><pre><code>基本语法：（1）ls -l &gt; 文件 （将列表的内容写入文件中覆盖的方式）        （2）ls -al &gt;&gt;文件  （将列表的内容追加写入到文件中）        （3）cat 文件1 &gt; 文件2 （将文件1的内容覆盖到文件2）</code></pre><p>ln：软连接也称为符号链接，类似于windows中的快捷方式，主要存放了链接其他文件的路径</p><pre><code>基本语法： ln -s [原文件或目录] [软链接名] （给原文件创建一个软链接）</code></pre><p>history：查看已经执行过的历史指令，也可以执行历史指令</p>]]></content>
      
      
      <categories>
          
          <category> Liunx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Liunx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习面试(一)</title>
      <link href="/2022/04/07/day_one/"/>
      <url>/2022/04/07/day_one/</url>
      
        <content type="html"><![CDATA[<p>U-Net的业务场景：<br>    U-Net是一种深度学习神经网络结构，主要用于图像分割任务，特别是医学图像分割。相比于普通的CNN，U-Net具有以下特点：<br>    1. U-Net是一种全卷积网络结构，可以对任意大小的图像进行分割，而不需要调整输入图像大小。<br>    2. U-Net采用类似编码器-解码器的结构，通过对输入图像进行多次下采样和上采样，能够提取图像的多层次特征信息。<br>    3. U-Net的解码器部分采用反卷积操作，能够对特征图进行上采样恢复，从而实现尺寸输出等同于原图的效果。<br>    4. U-Net在标注数据有限的情况下，能够获取到更高质量的分割结果，有一定的鲁棒性。<br>U-Net的使用场景，主要是医学图像分割任务，例如血管分割、肺部分割、细胞分割等领域。同时，U-Net也可以用于其他类型的图像分割任务。</p><p>U-Net相较于CNN的特点<br>UNET是一种基于卷积神经网络(CNN)的语义分割模型，具有以下特点：<br>    1. 全卷积结构：UNET采用全卷积结构，使得模型可以接受任意大小的输入图像，而输出相同大小的分割结果。<br>    2. 对称结构：UNET具有对称的编码器-解码器结构，编码器对输入图像进行多层次的特征提取，解码器则将特征图恢复到与输入相同分辨率的输出分割图。<br>    3. 上采样和跳跃连接：UNET使用上采样操作将编码器中的低分辨率特征图恢复到高分辨率，同时使用跳跃连接将编码器中的特征图与解码器中的特征图进行连接，增加了分割结果的精度。<br>    4. 数据增强：UNET采用数据增强技术，通过旋转、缩放、翻转等操作扩充训练数据集，提高模型泛化性能。<br>相比之下，CNN通常用于图像分类任务，它的特点包括：<br>    1. 单一的输出：CNN输出一个标量或向量，表示图像的类别或相关属性。<br>    2. 全连接结构：CNN包含全连接层来将图像特征映射到标签空间。<br>    3. 低级特征提取：CNN通常使用较少的卷积层提取低级特征，因此对于复杂任务需要多个CNN串联才能实现。<br>    4. 精度和速度折衷：CNN是为实时预测设计的，因此在精度和速度之间做出了折衷。</p><p>coding：写一个shuffle函数打乱一维数组：遍历一遍并每个元素与随机元素互换</p><pre><code class="py">import randomdef shuffle(arr):    for i in range(len(arr)):        rand_idx = random.randint(i,len(arr)-1)        arr[i], arr[rand_idx] = arr[rand_idx], arr[i]return arr对h×w的二维灰度图进行均值滤波，模板矩阵k×k：双重循环遍历二维数组，其中嵌套双重循环加和k×k个元素求均值```pyimport numpy as np&#39;&#39;&#39;函数的输入参数为原图像img和模板大小k，返回值为均值滤波后的图像。首先定义了模板中心距离边界的偏移量h_k和w_k。然后定义函数返回值result，并初始化为一个和原图像大小相同的全0矩阵。接下来，通过双重循环遍历原图像的每个像素点(i, j)，并将模板覆盖在当前像素点(i, j)上。对于模板中的每一个元素(m, n)，需要考虑其是否越界。这里用了max和min函数来确保不超出原图像的边界。对于在原图像范围内的模板元素，将其像素值累加到sum变量中，并将计数器count加1。最后，用sum除以count来求这k×k个元素的均值，并将结果赋值给result矩阵中对应的像素值。循环结束后，函数返回result作为均值滤波后的图像。&#39;&#39;&#39;def mean_filter(img, k):    h, w = img.shape    h_k, w_k = k//2, k//2    result = np.zero((h, w), dtype=np.uint8)    for i in range(h):        for j in range(w):            sum = 0            count = 0            for m in range(max(i-h_k, 0), min(i+h_k+1, h)):                for n in range(max(j-w_k, 0), min(j+w_k+1, w)):                    sum += img[m, n]                    count += 1            result[i, j] = sum // count    return result</code></pre><h3 id="DenseUNet和ResNet"><a href="#DenseUNet和ResNet" class="headerlink" title="DenseUNet和ResNet"></a>DenseUNet和ResNet</h3><p>DenseUNet和ResUNet是两种用于语义分割的卷积神经网络模型。<br>DenseUNet模型基于DenseNet的思想，将迭代连接（skip connections）应用到了UNet模型中，提高了模型的学习能力和特征表达能力。该模型还针对边缘区域的分割效果差的问题，采用了VGG-16 的结构对边缘区域进行优化。</p><p>DenseUNet的设计思想主要是将经典的UNet网络与稠密连接（Dense Connection）的概念相结合，以提高图像分割的性能。稠密连接是指将前一层输出与当前层输入连接在一起，使得当前层可以接收到前一层的所有信息，从而增强了特征的复用性，加快了特征传递速度，提高了模型的训练效率。<br>具体来说，DenseUNet将UNet的编码器和解码器部分中的每个卷积块都改成稠密连接块。在编码器部分，每个稠密连接块由一个3×3 卷积层和一个下采样层组成，并且每个输入都连接到当前层上。在解码器部分，每个稠密连接块由一个上采样层、一个3×3 卷积层、一个跳跃连接连接和一个此前的编码器部分的相应层输出连接组成。<br>除此之外，DenseUNet还采用了多尺度的输入和输出模块来处理不同尺度的图像，以及引入了残差连接来消除梯度消失、加快收敛速度。这些设计思想使得DenseUNet在与其他图像分割方法进行比较时，具有更好的分割精度和更快的计算速度。</p><p>ResUNet模型基于ResNet和UNet的思想，使用残差连接和迭代连接实现了端到端地语义分割。该模型在高分辨率图像处理任务中表现优秀，同时还加入了空洞卷积（dilated convolution）和批归一化（batch normalization）等技术，进一步提高了模型的性能<br>总的来说，DenseUNet和ResUNet都是比较优秀的语义分割模型，但具体应该选择哪一个模型还需要根据任务的具体需求进行选择。</p><h3 id="boundary-loss"><a href="#boundary-loss" class="headerlink" title="boundary_loss"></a>boundary_loss</h3><pre><code class="py">import torchdef boundary_loss(pred, mask):    &#39;&#39;&#39;    pred: 模型预测结果, (batch_size, channels, height, width)    mask: 分割图, (batch_size, channels, height, width)    return:    boundary_loss: 边界损失    &#39;&#39;&#39;    # 计算梯度，得到边缘位置    pred_grad_x = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:])    pred_grad_y = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :])    mask_grad_x = torch.abs(mask[:, :, :, :-1] - mask[:, :, :, 1:])    mask_grad_y = torch.abs(mask[:, :, :-1, :] - mask[:, :, 1:, :])    # 计算boundary loss    loss_x = pred_grad_x * mask_grad_x    loss_y = pred_grad_y * mask_grad_y    # 对loss进行求和和平均    boundary_loss = (torch.sum(loss_x) / torch.sum(mask_grad_x) +                     torch.sum(loss_y) / torch.sum(mask_grad_y)) / 2    return boundary_loss</code></pre><p>Boundary Loss是一种针对目标检测任务的损失函数，用于优化物体边缘的预测。我们可以使用PyTorch实现Boundary Loss。</p><p>首先，我们需要导入需要的PyTorch库。</p><pre><code class="python">import torchimport torch.nn as nn</code></pre><p>接下来，我们可以定义Boundary Loss的实现。</p><pre><code class="python">class BoundaryLoss(nn.Module):    def __init__(self, alpha=1.0, beta=1.0, reduction=&#39;mean&#39;):        super(BoundaryLoss, self).__init__()        self.alpha = alpha        self.beta = beta        self.reduction = reduction    def forward(self, pred, mask):        &quot;&quot;&quot;        :param pred: (B, C, H, W) - 模型的预测边缘图        :param mask: (B, C, H, W) - 真实边缘图        :return: boundary_loss - 边缘损失        &quot;&quot;&quot;        # 计算边缘区域        dilated_mask = torch.clamp(            nn.functional.max_pool2d(mask, (3, 3), stride=1, padding=1) - mask, 0, 1)        boundary_mask = mask - dilated_mask        # 将边缘区域应用于预测边缘图        boundary_pred = pred * boundary_mask        # 计算损失        pos_loss = boundary_mask * torch.log(pred + 1e-8)        neg_loss = (1 - boundary_mask) * torch.log(1 - boundary_pred + 1e-8)        boundary_loss = -self.alpha * pos_loss - self.beta * neg_loss        # 返回损失        if self.reduction == &#39;mean&#39;:            return torch.mean(boundary_loss)        elif self.reduction == &#39;sum&#39;:            return torch.sum(boundary_loss)        else:            return boundary_loss</code></pre><p>在实现中，首先我们计算真实边缘图的边缘区域，然后将边缘区域应用于模型的预测边缘图。接着，我们计算正样本和负样本的损失，最终求和得到边缘损失。最后，我们根据设定的reduction参数，选择使用平均值或总和作为最终的损失。（注意，在计算log时，加上一个很小的值1e-8，避免出现log(0)的情况）</p><p>接下来，我们将Boundary Loss应用于目标检测任务中。</p><pre><code class="python"># 定义模型class MyDetectionModel(nn.Module):    def __init__(self):        super(MyDetectionModel, self).__init__()        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)        self.bn1 = nn.BatchNorm2d(16)        self.relu1 = nn.ReLU()        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)        self.bn2 = nn.BatchNorm2d(32)        self.relu2 = nn.ReLU()        self.conv3 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1)        self.sigmoid = nn.Sigmoid()    def forward(self, x):        x = self.conv1(x)        x = self.bn1(x)        x = self.relu1(x)        x = self.conv2(x)        x = self.bn2(x)        x = self.relu2(x)        x = self.conv3(x)        x = self.sigmoid(x)        return x# 定义超参lr = 0.001epochs = 10alpha, beta = 1.0, 1.0reduction = &#39;mean&#39;# 定义数据加载器train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)# 定义模型和优化器model = MyDetectionModel().to(device)optimizer = torch.optim.Adam(model.parameters(), lr=lr)# 定义损失函数criterion = BoundaryLoss(alpha=alpha, beta=beta, reduction=reduction)# 训练模型for epoch in range(epochs):    for i, (images, targets) in enumerate(train_loader):        images = images.to(device)        targets = targets.to(device)        optimizer.zero_grad()                outputs = model(images)        loss = criterion(outputs, targets)        loss.backward()        optimizer.step()</code></pre><p>在训练环节中，我们加载数据，定义模型和优化器，并使用Boundary Loss作为损失函数进行优化。由于Boundary Loss针对物体边缘的优化，因此特别适合目标检测任务。</p><p>Boundary Loss是一种用于图像分割任务的损失函数，其核心思想是度量预测的边缘和真实边缘之间的距离，从而帮助网络更好地学习边缘信息。以下是在PyTorch中实现Boundary Loss的代码：</p><pre><code class="python">import torchdef boundary_loss(pred, target):    &quot;&quot;&quot;    Implementation of boundary loss in PyTorch.    :param pred: predicted segmentation mask, dimension: (N, C, H, W)    :param target: ground-truth segmentation mask, dimension: (N, C, H, W)    :return: boundary loss value    &quot;&quot;&quot;    bce_loss = torch.nn.BCELoss(reduction=&quot;mean&quot;)        # Compute the gradient of the target mask along both spatial dimensions    target_x_grad = torch.abs(target[:, :, :, :-1] - target[:, :, :, 1:])    target_y_grad = torch.abs(target[:, :, :-1, :] - target[:, :, 1:, :])    target_edge = target_x_grad + target_y_grad        # Compute the gradient of the predicted mask along both spatial dimensions    pred_x_grad = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:])    pred_y_grad = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :])    pred_edge = pred_x_grad + pred_y_grad        # Compute the boundary loss, which is the mean of the element-wise product of     # the binary target edge (1 inside the boundary, 0 outside) and the distance     # between the predicted edge and the target edge    loss = bce_loss(target_edge, torch.clamp(pred_edge, 0, 1)) * target_edge.mean()        return loss</code></pre><p>在上述代码中，我们首先定义了一个标准的BCELoss作为Boundary Loss的基础。然后，我们以类似于Sobel算子的方式计算了目标和预测掩码的梯度，并将它们相加得到两个边缘掩码。接下来，我们计算了Boundary Loss，这是目标边缘掩码中每个像素距离它最近的预测边缘掩码像素的欧氏距离的平均值。我们在这里使用了torch.clamp(0,1)来进行预测边缘掩码的截断，以避免边缘像素梯度过大导致训练不稳定。</p><p>最后要注意的一点是，由于在计算Boundary Loss时我们使用了二进制掩码来筛选边界区域，因此我们需要将目标和预测掩码的数值范围压缩到[0,1]之间。如果您的数据集的标签具有多个类别，则需要对每个类别分别计算Boundary Loss，并对这些损失值进行加权平均。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习面试(二)</title>
      <link href="/2022/04/07/day_two/"/>
      <url>/2022/04/07/day_two/</url>
      
        <content type="html"><![CDATA[<p>说说XGBoost和GBDT的不同:</p><ol><li>XGBoost和GBDT都是基于树的集成学习算法，但在实现细节和性能上存在一些不同：</li><li>对于目标函数的优化：XGBoost采用了类似于牛顿法的二阶泰勒展开方式进行目标函数的极值优化，加快了收敛速度；而GBDT采用的是一阶泰勒展开。</li><li>对于特征的选择：XGBoost采用增益和覆盖度的综合指标选择特征；GBDT采用的是信息熵或基尼指数。</li><li>对于剪枝的处理：XGBoost对于树的结构进行分裂之后，采用正则化的方式进行剪枝；GBDT采用的是贪心算法来选择最优分裂点。</li><li>对于模型的并行计算：XGBoost使用了多线程并行运算，在内存使用上更加高效；GBDT则只能串行计算。</li><li>对于缺失值的处理：XGBoost可以自动地学习处理缺失值，GBDT则需要另外进行预处理。<br>综上所述，XGBoost在目标函数优化、特征选择、树结构剪枝以及模型的并行计算方面都具有更大的优势，因此在许多竞赛中取得了很好的成绩。但在数据处理方面相对要求更高。而GBDT则更为直观易懂，数据要求也相对较低。</li></ol><p>XGBoost和GBDT都是决策树集成学习算法，它们的区别主要涵盖以下几个方面：<br>1.算法原理：GBDT是一种基于残差学习的决策树集成算法，每一次学习目标是拟合当前模型残差，使得后续模型能够更好地拟合样本。而XGBoost是一种基于梯度提升的决策树算法，每一次学习目标是拟合当前模型梯度，使得后续模型能够更好地逼近损失函数。<br>2.损失函数：在实际应用中，XGBoost支持更广泛的损失函数选择，除了GBDT中常用的平方误差和绝对误差，还支持logistic、softmax等分类问题的损失函数。而且XGBoost能够集成不同的损失函数。<br>3.正则化：XGBoost加入了二阶导数信息来进行正则化，防止过拟合效果更好。同时，XGBoost还可以通过结构化的正则化方式减少过拟合现象。<br>4.并行化处理：相比于GBDT，XGBoost引入了缓存访问和特征采样等并行处理方式，可以通过并行化处理更快地训练模型。<br>5.可扩展性：XGBoost拓展性更强，支持分布式计算，可以在大数据环境下使用，而GBDT则只能在单机上运行。<br>总的来说，XGBoost是一个更加高效、灵活、容易扩展的算法，能够更好地解决现实生活中的复杂问题，在机器学习和数据挖掘领域中得到了广泛应用。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅谈SQL注入，XSS攻击</title>
      <link href="/2019/03/27/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/"/>
      <url>/2019/03/27/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/</url>
      
        <content type="html"><![CDATA[<p>作为计算机小白，一直都认为黑客很牛逼所以简单的了解一下这反面的知识——信息安全<br>黑客是个英译词，译作Hacker。黑客攻击或者黑计算机的方式多种多样，主要分为两种：<br><code>（1）非破坏性的攻击：一般是为了扰乱系统的运行，并不盗窃系统资料，仅仅只是使服务器暂时失去对外提供服务的能力，通常采用拒绝服务攻击或信息炸弹</code><br> <code>（2）破坏性攻击：是以侵入他人电脑系统、盗窃系统保密信息、破坏目标系统的数据为目的</code><br> 常见的攻击有DDOS，CSRF，Dos等，通常通过的途径有病毒式，洪水式，系统漏洞等。<br> 下面简单的介绍几种</p><h3 id="SQL注入"><a href="#SQL注入" class="headerlink" title="SQL注入"></a>SQL注入</h3><p> <code> 常见的注入式攻击，通过把SQL命令插入到Web表单提交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令。具体来说，它是利用现有应用程序，将（恶意的）SQL命令注入到后台数据库引擎执行的能力，它可以通过在Web表单中输入（恶意）SQL语句得到一个存在安全漏洞的网站上的数据库，而不是按照设计者意图去执行SQL语句</code><a href="https://baike.baidu.com/item/sql%E6%B3%A8%E5%85%A5/150289?fr=aladdin">添加链接描述</a><br> 造成可以进行SQL注入的本质原因就是未将代码与数据进行严格的隔离，导致用户在读取数据的时候，错误的把数据作为代码的一部分执行。<br> 下面举个简单的例子：</p><pre><code class="js">var testCondition;testCondition = Request.from(&quot;testCondition&quot;)var sql =&quot;select * from TableA where id=&#39;&quot;+ testCondition +&quot;&#39;&quot;;</code></pre><p>在上面的例子当中，如果用户输入的ID只是一个数字当然没有任何问题，但是如果用“;‘隔开后，在testCondition里面插入其他SQL语句，则会出现意想不到的结果。例如输入drop，delete等。例如你不小心输入”#--!#@”这样的字符然后保存使得数据库跟新就会使where后面的信息被注释掉了，执行语句就变成了</p><pre><code class="js">updata table set memo=&quot;&quot;# --! #(@&quot; where use_id=xxxxxxx;</code></pre><p>使得全数据库的memo字段的数据都被跟新了，而不是你一个人的数据。<br>下面有几个兄弟写的很详细，大家可以去看看<br>（1）<a href="https://blog.csdn.net/ylw_bk/article/details/78327748">最详细的SQL注入教程–易利伟</a><br>（2）<a href="https://www.cnblogs.com/pursuitofacm/p/6706961.html">web完全篇之SQL</a><br>（3）<a href="https://www.cnblogs.com/wang-sai-sai/p/10234568.html">SQL注入攻击</a><br>（4）<a href="https://cloud.tencent.com/developer/news/61383">用sql注入攻破网站</a><br>大家可以找个一个肉鸡网站去试试或者自己写一个肉鸡网站也是个不错的选择<br>SQL注入的危害极大，在进行程序设计时我们可以从下面几个方面进行预防</p><pre><code>（1）过滤用户输入参数中的特殊字符，从而降低被SQL注入的风险</code></pre><pre><code>（2）禁止使用字符串拼接的SQL语句，严格使用参数绑定传入的SQL参数</code></pre><pre><code>（3）合理使用数据库访问框架提供的防注入机制</code></pre><h2 id="xss攻击"><a href="#xss攻击" class="headerlink" title="xss攻击"></a>xss攻击</h2><pre><code class="html">    XSS攻击全称跨站脚本攻击，是为不和层叠样式表(Cascading Style Sheets,CSS) 的缩写混淆，故将跨站脚本攻击缩写为XSS，XSS是一种在web应用中的计算机安全漏洞， 它允许恶意web用户将代码植入到提供给其它用户使用的页面中。即黑客通过技术手段向 正常用户请求的HTML页面中插入恶意脚本，从而可以执行任意脚本</code></pre><h6 id="xss的分类"><a href="#xss的分类" class="headerlink" title="xss的分类"></a>xss的分类</h6><p>（1）反射型XSS</p><pre><code>   恶意代码并没有保存在目标网站，通过引诱用户点击一个链接到目标网站的恶意链接来实施攻击的。</code></pre><p>（2）存储型XSS</p><pre><code>     恶意代码被保存到目标网站的服务器中，这种攻击具有较强的稳定性和持久性，比较常见场景是在博客，论坛等社交网站上，但OA系统，和CRM系统上也能看到它身影，比如某CRM系统的客户投诉功能上存在XSS存储型漏洞，黑客提交了恶意攻击代码，当系统管理员查看投诉信息时恶意代码执行，窃取了客户的资料，然而管理员毫不知情，这就是典型的XSS存储型攻击。</code></pre><p>  (3)  DOM型XSS</p><pre><code>其实是一种特殊类型的反射型XSS，它是基于DOM文档对象模型的一种漏洞。</code></pre><p>  比如在2011年微博左右XSS蠕虫攻击事件，攻击者就利用微博发布功能中未对action-data漏洞做有效的过滤，在发布微博信息的时候戴上了包含攻击脚本的URL，用户访问该微博是便疯狂加载恶意脚本，该脚本会让用户以自己的账号自动转发同一条微博，通过这样的病毒式扩散，大量用户受到攻击。<br>  下面举个简单的实例可能会导致反射型XSS的文件：</p><pre><code class="html">&lt;div&gt;&lt;h3&gt;反射型XSS实例&lt;/h3&gt;&lt;br&gt;用户:&lt;%=request.getParamer(&quot;useName&quot;)%&gt;&lt;br&gt;系统错误信息：&lt;%=request.getParamer(&quot;errorMessage&quot;)%&gt;&lt;div&gt;</code></pre><p>上面的代码从HTTP请求中取得了userName和errorMessage两个参数，并直接输出到HTML中用于展示，当构造这样一种URL时就出现了反射型XSS，用户便会执行脚本文件</p><pre><code class="js">http://xss.demo/self-xss.jsp?userName=666&lt;script&gt;alert(&quot;666&quot;)&lt;/script&gt;&amp;errorMessage=XSS实例&lt;script scr=http://hacker.demo/xss-script.js&gt;</code></pre><h5 id="XSS攻击的预防主要是通过对用户的输入数据进行过滤和转义，如使用jsonp框架对用户输入的字符串作XSS过滤，使用Sping框架中的HtmlUtils对用户输入的字符串做html转义等"><a href="#XSS攻击的预防主要是通过对用户的输入数据进行过滤和转义，如使用jsonp框架对用户输入的字符串作XSS过滤，使用Sping框架中的HtmlUtils对用户输入的字符串做html转义等" class="headerlink" title="XSS攻击的预防主要是通过对用户的输入数据进行过滤和转义，如使用jsonp框架对用户输入的字符串作XSS过滤，使用Sping框架中的HtmlUtils对用户输入的字符串做html转义等"></a>XSS攻击的预防主要是通过对用户的输入数据进行过滤和转义，如使用jsonp框架对用户输入的字符串作XSS过滤，使用Sping框架中的HtmlUtils对用户输入的字符串做html转义等</h5><p>下面是几篇写的较为详细的XSS攻击博客<br>（1）<a href="https://www.cnblogs.com/stefanieszx11/p/8602138.html">web安全之XSS攻击</a><br>（2）<a href="https://www.cnblogs.com/phpstudy2015-6/p/6767032.html#_label12">XSS跨站脚本攻击</a><br>（3）<a href="https://www.cnblogs.com/digdeep/p/4695348.html">XSS防御方法</a><br>（4）<a href="https://www.cnblogs.com/shawWey/p/8480452.html">浅谈XSS攻击原理</a><br>时间匆匆而逝，下次我再来分享一点点关于第三种黑客攻击：CSRF的知识</p>]]></content>
      
      
      <categories>
          
          <category> 信息安全 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 信息安全 </tag>
            
            <tag> SQL </tag>
            
            <tag> XSS </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
