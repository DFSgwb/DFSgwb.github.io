{"meta":{"title":"DFSgwb","subtitle":"","description":"我的技术分享","author":"GWB","url":"https://github.com/DFSgwb/DFSgwb.github.io.git","root":"/DFSgwb/DFSgwb.github.io.git/"},"pages":[],"posts":[{"title":"Disentangled High Quality Salient Object Detection","slug":"Disentangled High Quality Salient Object Detection","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-11T12:39:23.861Z","comments":true,"path":"2023/04/11/Disentangled High Quality Salient Object Detection/","link":"","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/2023/04/11/Disentangled%20High%20Quality%20Salient%20Object%20Detection/","excerpt":"","text":"Disentangled High Quality Salient Object Detection摘要将高分辨率SOD任务分解为低分辨率显著性分类网络(LRSCN)和高分辨细化网络(HRRN),作为一项逐像素分类任务，LRSCN旨在以低分辨率来捕获足够的语义，以识别明确的显著，背景和不确定区域。HRRN是一项回归任务，旨在准确提炼不明确区域中的像素的显著性值。 Introduction一种好的高分辨率显著目标检测方法不仅要准确地检测出整个显著目标，还要预测显著目标的精确边界。基于低分辨率设计的SOD模型无法直接套用到高分辨率图像中，最主要的原因就是，低分辨的方法往往是将识别和定位两个过程使用一个过程实现，而对于高分辨率图像最为重要的是对于边界的精确分割。对于显著区域的定位我们可以通过扩大感受野来获取足够的语义，但是由于高分辨的特性，这将使得内存的使用大大的增加，此时往往采用下采样操作，但是下采样操作不可避免地会使得结构信息丢失。这种解决问题的思路也就是导致低分辨的SOD模型直接迁移至高分辨率图像中会出现边界模糊的原因。如下图所示 模型结果对比 从上图可以发现，显著像素点可以分为以下三类： (1)大多数显著对象内部的像素具有最高的显著值，我们称为确定的显著像素 (2) 背景区域中的大多数像素具有最低的显著值，我们称为确定的背景显著像素 (3) 模糊物体边界像素的显著值在0到1之间波动，称之为不明确像素 理想的 SOD 方法应有效识别图像中明确的显着区域和背景区域，并准确计算不确定区域中像素的显着性值，以保持清晰的目标边界。也就是一个回归任务加一个分类任务。将高分辨率显着对象检测分离为低分辨率显着性分类网络(LRSCN)和高分辨率细化网络(HRRN).LRSCN旨在以低分辨率捕获足够的语义并将像素分类为三个不同的集合以供以后处理.HRRN旨在准确提炼不确定区域中像素的显着性值,以在GPU内存有限的情况下以高分辨率保持清晰的对象边界.如上所述,HRRN 需要高分辨率图像中的结构细节。然而，广泛使用的低分辨率显着性数据集通常在注释质量方面存在一些问题,几乎不可能从这些有缺陷的数据集中直接获得足够的对象边界细节来训练高分辨率网络。在最近的工作中，Zeng 等人。提出通过使用具有准确注释的高分辨率图像来训练他们的 SOD 网络。然而，如此高质量的图像标注需要大量的人工成本。在我们的论文中,我们认为没有必要在网络训练中使用这种精确注释的高分辨率图像.通过在训练过程中引入不确定性，我们的 HRRN 可以仅使用标注不佳的低分辨率训练数据集很好地解决高分辨率细化任务。 模型方法HRRN High Resolution Network Framework LRSCN的目的是在低分辨率下获取足够的语义并将像素分为三个不同的集合，同时节省内存的使用，HRRN计算回归像素的显著性值，并在高分辨率下保持清晰的对象边界 LRSCN使用一个简单的U-Net编码器解码器架构，VGG-16作为主干网络，因此将从Conv1-2，Conv2-2，Conv3-3，Conv4-3，Conv5-3，Conv6-3获得六个特征，但是由于前两个特征的感受野太小，则不使用。在编码器和解码器之间增加一个多尺度特征提取和跨级特征融合模块(MECF)，以提高特征表示的可辨别性。解码器自上而下的方式融合MECF的输出特征和上一阶段的上采样特征，每个解码器的输出定义为$D_{i},i&#x3D;1,2,3…n$,最后SGA模块建立在D3之上用来生成准确的显著预测图T，为了回归清晰的目标边界值，HRRN的输入是在LRSCN提供的trimap引导下的高分辨率图像。HRRN具有基本的编码器-解码器架构，在不确定性损失的帮助下，网络可以对噪声数据更加鲁棒，并预测具有清晰边界的高分辨率显着图。 LRSCN###LRSCN架构图 开发了一种基于全局卷积网络GCN的多尺度特征提取模块ME，以扩大特征感受野并获得多尺度信息。为了实现第二个目标，我们利用跨级别特征融合模块CF来利用不同级别的特征优势。此外，在设计网络架构时，受的启发，我们使用拆分变换策略进一步放大 特征感受野，从而产生更具辨别力的特征表示。具体来说，我们将输入F按通道维度均匀地分成两部分${ F_{1}, F_2}$，然后将$F_1$送入多尺度特征提取路径，将$F_2$送入跨级特征融合路径。这两个路径的输出连接在一起作为最终输出。我们称这个桥接模块为MECF模块，如上图所示。 SGA模块：每个解码器融合来自MECF模块和前一解码器级的特征，然后使用$3×3$卷积层进行最终预测。为了保持trimap和显著图的一致性，确保trimap的不确定区域能够准确覆盖显著图的边界，我们在D3上设计了一个显著引导注意模块（SGA）。具体来说，我们首先使用$3×3$卷积和sigmoid函数来计算显著性映射。然后，将显著性图视为空间权重图，有助于细化特征并生成精确的trimap。最后，输出trimap T是3通道分类logits。整个SGA模块保证trimap和saliecny地图的对齐。 HRRN模块HRRN遵循解纠缠原则，在LRSCN提供的trimap的指导下，精确细化不确定区域中像素的显著性值，以保持高分辨率下清晰的目标边界。HRRN的架构如图2所示。HRRN有一个简单的类似U-NET的体系结构。为了在高分辨率下进行更好的预测，我们进行了一些非平凡的修改。首先，底层特征包含丰富的空间和细节信息，这些信息在恢复清晰的对象边界方面起着至关重要的作用，因此解码器在每个上采样块之前而不是在每个上采样块之后组合编码器特征。此外，我们使用一个两层的快捷块来对齐编码器特征通道，以进行特征融合。其次，为了让网络更加关注细节信息，我们通过一个快捷块将原始输入直接反馈到最后一个卷积层，以产生更好的结果。最后，从图像生成任务中学习，我们对每个卷积层使用谱归一化，以对网络的$Lipschitz$常数添加约束并稳定训练。 为了监督LRSCN，我们应该生成trimap的GT表示为$T^{gt}$，它可以表示确定的显着、确定的背景和不确定的区域。如上所述，不确定区域主要存在于对象的边界处。因此，我们使用随机像素数（5、7、9、11、13）在对象边界处擦除和扩展二进制真实图，以生成GT不确定区域。 剩余的前景和背景区域代表明确的显着和背景区域。$T^{gt}$ 定义为：$$\\Large T_{gt}(x,y) &#x3D;\\begin{cases}2 &amp; T_{gt}(x,y)\\in definite salient \\0 &amp; T_{gt}(x,y)\\in definite background \\1 &amp; T_{gt}(x,y)\\in uncertain region \\\\end{cases}$$其中$(x,y)$表示图像上的每个像素位置。如下图所示 结果可视化 对于trimap的监督我们使用交叉熵损失$$\\Large L_{trimap}&#x3D;\\dfrac{1}{N}\\sum_{i}-log(\\dfrac{e^{T_{i} } }{\\sum_{j} e^{T_{j} } })$$为保障trimap的准确率，我们在trimap监督的基础上增加了额外显著性监督$L_{saliency}$，总损失是$$\\Large L_{LRSCN}&#x3D;L_{saliency}+L_{trimap}$$不使用不确定性损失，因为LRSCN的主要目标是获取足够的语义，而不是精确的边界。 对于输入的高分辨率图像$I$，让$G^H$表示其背景真值，预测显著性图为$S^H$。我们利用$L_1$损失来比较预测显著性图和背景真值在明确的显著性和背景区域上的绝对差异：$$\\Large L_1 &#x3D; \\dfrac{1}{E}\\sum_{i\\in E}|S_{i}^H-G_{i}^H|$$其中$E$表示在$trimap$中被标记为明确显着或背景的像素数，$S_{H}^i$和$G_{H}^i$表示位置$i$处的预测值和$groundtruth$值。由于数据集本身在注释质量方面存在一些问题，因此引入一个不确定损失来解决数据集本身带来的缺陷。使用高斯似然的方式建模不确定性$$\\Large p(y|f(x))&#x3D;N(f(x),\\delta^2)$$其中$\\delta$表示测量的不确定性，$y$是输出，在最大似然推断中，我们将模型的对数似然最大化，表示为:$$\\Large logp(y|f(x))\\propto-\\dfrac{||y-f(x)||}{2\\delta^2}-\\dfrac{1}{2}log{\\delta^2}$$则不确定损失定义为：$$\\Large L_{uncertainty}&#x3D;\\dfrac{||y-f(x)||^2}{2\\delta^2}+\\dfrac{1}{2}log\\delta^2$$将其转化为像素的表达形式：$$\\Large L_{uncertainty}&#x3D;\\dfrac{1}{U}\\dfrac{||S_{i}^H-G_{i}^H||}{2\\delta_{i}^2}+\\dfrac{1}{2}log\\delta_{i}^2$$ HRRN损失可视化","categories":[{"name":"SOD","slug":"SOD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/categories/SOD/"}],"tags":[{"name":"SOD","slug":"SOD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/SOD/"}]},{"title":"Looking for the Detail and Context Devils:High-Resolution Salient Object Detection","slug":"Looking for the Detail and Context Devils","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-11T12:42:20.641Z","comments":true,"path":"2023/04/11/Looking for the Detail and Context Devils/","link":"","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/2023/04/11/Looking%20for%20the%20Detail%20and%20Context%20Devils/","excerpt":"","text":"Looking for the Detail and Context Devils:High-Resolution Salient Object Detection 缺乏显著对象的边界细节和语义上下文是低分辨率SOD数据集的一大弊端，本文设计了一个端到端的学习框架，称为DRFNet,使用一个共享特征提取器和两个有效的细化头构成。通过解耦细节和上下文信息，一个细化头采用全局感知和特征金字塔，在不增加太多计算负担的情况下，提升空间细节信息，从而缩小高级语义和低级细节之间的差距，另一个细化头采用混合扩张卷积块和分组上采样，这在提取上下文信息方面非常有效，基于双重细化，使得实现扩大感受野并从高分辨率图像中获取更多的判别特征。 高分辨率图像具有一个突出的特点包含更多可以覆盖范围和形状的结构对象和更多的细节信息。一方面高级上下文特征更适合检测大而混乱的对象，而小对象则受益于低级精细特征。不同层次特征的结合将为语义定位和细节增强提供更丰富的信息。 现有的基于FCN结构的方法一个缺点就是特征通常是以粗到精细的方式集成，它缺乏获取足够的局部和全局上下文信息或远程依赖关系的能力。导致不显眼的对象和混淆区域的准确性较差。大量的使用卷积操作使得对于算力和内存的要求变得极高，但如果将输入图像限制为相对较低的分辨率，又阻碍了细节感知和高分辨的实际需求。 现有的高分辨率图像像素级标记方法大致分为三大类， 1首先将高分辨率图像裁剪为低分辨图像，然后预测低分辨率结果并将其结果插值为原始图像大小。这种操作虽然简单但是图像空间细节的丢失是不可避免，导致出现对物体边界的错误预测 2设计轻型编码器-解码器网络,通过特征融合层次特征，之间提高空间分辨率并恢复一些缺少的细节，但是这种由于连续的下采样操作会带来空间信息的丢失且缺乏足够的对象的感受野 3 引入具有多个分支的不对称网络,每个分支以不同分辨率运行，即低分辨率图像中提取全局信息，高分辨率图像中提取精细细节，但是如何在不同分支上整合全局和局部信息还没一个很好的方法，由于高级语义和低级细节之间的差距，不好的融合方式可能使得它们在预测中出现奇怪的预测区域 常见的HRSOD网络架构 本文网络结构 共享特征提取器采用修改后的VGG-16和ResNet-18作为共享特征提取器 Detail Refinement HeadDRH包括三个关键块： 1卷积特征缩减块(CFRB):该块旨在缩小多尺度深度特征的维数，本质上就是一个$1\\times1$的卷积块，后面是批归一化和Relu激活函数，为减少高分辨率图像的计算和内存需求，卷积滤波器的数量设为为32 2深度特征上采样块(DFUB):采用C组的反卷积进行上采样，通过适当的上采样率，可以放大较深层的输出特征以匹配较浅层产生的特征，且进一步减少计算量 3全局感知特征交互块(GFIB)：由于接受域有限，无法获取足够的全局信息，为表达增强表现能力，首先对CFRB和DFUB的特征进行级联全局平均池化。然后将其转发到全连接层以生成全局权重向量,整个过程可以表示为$$\\Large \\alpha_G &#x3D; \\sigma(W_1GAP([F_C,F_D])+b)$$$$\\Large F_R&#x3D;g(W_2[F_C,F_D]+b)$$$$\\Large F_G &#x3D; \\alpha_G\\odot F_R$$ Context Refinement Head在直接堆叠或使用金字塔结构扩大感受野的策略中具有两个非常明显的缺点。1：计算量大，占用内存，不适合高分辨率图像。2：缺乏捕获足够多尺度局部上下文信息的能力，导致对于不显眼的对象的准确性较差本文提出的CRH使用混合膨胀卷积和分组上采样组成。具体的来说就是使用一个混合扩张卷积块和一个分组上采样组成","categories":[{"name":"SOD","slug":"SOD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/categories/SOD/"}],"tags":[{"name":"SOD","slug":"SOD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/SOD/"}]},{"title":"像素级光场显著性检测","slug":"像素级光场显著性检测","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-11T12:38:51.239Z","comments":true,"path":"2023/04/11/像素级光场显著性检测/","link":"","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/2023/04/11/%E5%83%8F%E7%B4%A0%E7%BA%A7%E5%85%89%E5%9C%BA%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B/","excerpt":"","text":"摘要在一个统一的框架中识别干净的标签且有效的整合光场线索之间的关系。将学习描述为光场内特征融合流和场景间相关流的联合优化，以生成预测，首先引入一个像素遗忘引导融合模块，以相互增强光场特征，并利用迭代过程中的像素一致性来识别噪声像素，再引入跨场景噪声惩罚损失，以更好地反映训练数据的潜在结构，并使学习对噪声保持不变。 光场图像： 光场：是一个四维的参数化表示，是空间中同时包含位置和方向信息的四维光辐射场，简单地说，涵盖了光线在传播中的所有信息，在空间内任意角度，任意的位置都可以获得整个空间环境的真实信息，用光场获得的图像信息更加全面，品质更好。 光场成像的原理：传统相机成像是光线穿过镜头，而后传播到成像平面，光场成像则是在传感器平面添加了一个微透镜矩阵，在于将穿过主镜头的光线再次穿过每个微透镜，从而收获到光场的方向与位置信息，使成像结果在后期更加可调节，达到先拍照后聚焦的效果。 直接在像素级别噪声标签上训练显著性检测网络可能会引导网络过度适应损坏的标签。且当前现有的方式都缺乏全局视角来探索整个数据集之间的关系模式 光场显著性：","categories":[{"name":"SOD","slug":"SOD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/categories/SOD/"}],"tags":[{"name":"SOD","slug":"SOD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/SOD/"}]},{"title":"Salient Object Detection via Dynamic Scale Routing","slug":"Salient Object Detection via Dynamic Scale Routing","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-11T12:43:15.328Z","comments":true,"path":"2023/04/11/Salient Object Detection via Dynamic Scale Routing/","link":"","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/2023/04/11/Salient%20Object%20Detection%20via%20Dynamic%20Scale%20Routing/","excerpt":"","text":"Salient Object Detection via Dynamic Scale Routing 摘要现有的SOD模型的编码器可以通过提取多尺度特征，并通过各种微妙的解码器组合特征，但是这个特征通常是固定的，实际上，在不同场景中配合使用不同的内核大小是更可取的，因此本文提出了一种动态的金字塔卷积模型，动态选择最适合的内核大小，其次提出了一种自适应双向解码器以最好适应基于DPConv的编码器。最重要的的亮点是它能够在特征尺度及其动态集合之间进行路由，使推理过程具有尺度感知能力","categories":[{"name":"SOD","slug":"SOD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/categories/SOD/"}],"tags":[{"name":"SOD","slug":"SOD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/SOD/"}]},{"title":"Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation","slug":"多尺度高分辨率Transformer","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-11T12:52:13.459Z","comments":true,"path":"2023/04/11/多尺度高分辨率Transformer/","link":"","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/2023/04/11/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87Transformer/","excerpt":"","text":"Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation摘要VITs主要是为生成低分辨率表示的图像分类任务而设计的，这使得VITs的语义分割等密集预测任务具有挑战性，本文提出的HRVIT，通过高分辨率多分枝架构与ViT集成来增强ViT以学习语义丰富和空间精确的多尺度表示，通过各种分支块协同优化技术平衡HRVIT的模型行恩那个和效率 IntroductionVIT的单尺度和低分辨率表示对于需要高敏感性和细粒度图像细节的语义分割不友好。已有的多尺度VIT网络大多遵循类似于分类的网络拓扑，具有顺序或串联架构，基于复杂性考虑，都是逐渐对特征图进行下采样，以提取更高级别的低分辨表示，并将每个阶段的输出直接馈送到下游分割头，这样的顺序结构缺乏足够的跨尺度交互，因此没法产生高质量的高分辨率表示 HRVIT并行提取多分辨率特征并反复融合它们以生成具有丰富语义信息的高质量HR表示。简单的将HRNET中所有的卷积残差块替换为Transformer将遇到严重的可扩展性问题，如果没有良好的架构块协同优化，从多尺度继承的高表示能力可能会被硬件上令人望而却步的延迟和能源成本所击倒。因此本文使用以下方式进行优化 1）HRViT的多分支HR架构在跨分辨率融合的同时提取多尺度特征 2）使用增强局部注意力消除率冗余键和值以提高效率，并通过额外的并行卷积路径，额外的非线性单元和用于特征多样性增强的辅助快捷方式来增强模型的表达能力。 3）HRViT采用混合尺度卷积前馈网络加强多尺度特征提取 4）HRVIT的HR卷积结构和高效的补丁嵌入层在降低硬件成本的情况下保持率更多的低级细粒度特征 HRViT网络结构图 由图可知，HRViT第一部分由卷积干组成，同时提取低层特征，在卷积stem后，HRViT部署了四个渐进式Transformer阶段，其中第n阶段包含n个并行的多尺度Transformer分支，每个阶段可以有一个或多个模块。每个模块从一个轻量级密集融合层开始，以实现跨分辨率交互和一个用于局部特征提取的有效补丁嵌入块，然后是重复的增强局部自注意力块和混合尺度卷积前馈网络，与逐步降低空间维度以生成金字塔特征的顺序ViT主干不同，我们在整个网络中维护HR特征，以通过跨分辨率融合增强HR表示的质量。 多分支HRNet和self-attention运算所带来的高度复杂性会迅速导致内存占用，参数大小急剧上升，计算成本爆炸性增长，简单地在每个模块上分配相同局部注意力窗口大小的块将导致巨大的计算成本，根据对于复杂性分析， 最后决定使用狭窄的关注窗口代销，并在两条HR路径上使用最小数量的块。在中等分辨率的第三个分支，使用具有大窗口的深度分支，以提供大的感野和提取良好的高级特征。低分辨率的分支包含大多数参数们对于提供具有全局感受野的高级特征以及生成粗分割图非常有用，但是较低的空间尺度会导致图像细节丢失过多，因此旨在低分辨率的分支上部署几个大窗口块，已在参数预算下提高高级特征质量。十字形self-attention算子 cross-shaped self-attention 细粒度注意力 近似全局视图：通过两个平行正交的局部注意力，能够收集全局信息 可伸缩复杂性：窗口是一个维度固定的，避免了图像大小的二次复杂性 遵循CSWin中的十字形窗口划分方法，将输入$x\\in R^{H\\times W\\times C}$分成两部分${x_H,x_V \\in R^{H\\times W\\times C&#x2F;2}}$,$x_H$被分割成不相交的水平窗口，而另外一半$x_V$被分割成垂直窗口。将窗口设置为$s\\times W$或者$H\\times s$，在每个窗口中，将补丁分块为$K$个$d_k$维头部，然后应用局部self-attention。将零填充应用于输入$x_H$或$x_V$，以允许完整的第k个窗口，然后将注意力图中的填充区域屏蔽为0，以避免不连贯的语义关联原始的QKV线性层在计算和参数方面非常昂贵，因此共享键和值张量的线性投影，以节省计算和参数，此外，引入一个辅助路径，该路径具有并行深度方向卷积，以注入归纳偏置以促进训练，与CSWin中的局部位置编码不同，我们的并行路径是非线性的，并且在没有窗口划分的情况下应用于整个4—D特征映射$W^Vx$而没有窗口分区，这条路径可以被视为一个反向残差模块，它与self-attention中的线性投影层共享逐点卷积。这种共享路径可以有效注入归纳偏差，并以边际硬件开销的情况下增强局部特征聚合，作为对上述键值共享的性能补偿，引入一个额外的Hardswish函数来改善非线性，附加一个初始化为恒等投影的BatchNorm层以稳定分布以获得更好的可训练性，此外还添加了一个通道式投影作为多样性增强快捷方式，与传统增强的快捷方式不同，此快捷方式具有更高的非线性，不依赖于对硬件不友好的傅里叶变换。 混合尺度卷积前馈网络 受到MIT的MIxFFN和HR-NAS中多分支倒置残差块的启发，我们通过在两个线性层之间插入多个尺度深度卷积路径来设计混合尺度卷积，在MiXCFN中，在LayerForm之后，我们将信道按r的比例展开，然后将其分成两个分支，$3\\times 3$和$5 \\times 5$深度方向卷积用于增加HRViT的多尺度局部信息提取，出于效率的考虑，我们利用信道冗余，将MiXCFN扩展比r从4降到3或者2， 下采样部分self-attention的复杂度与图像大小成二次方，为解决大图像是的可伸缩性问题，在输入端对图像进行4倍的下采样，不在stem中使用注意操作，因为早期卷积比self-attention更能有效的提取低级特征，作为早期的卷积，遵循HRNet中的设计，并使用两个步长为2的CONV-BNReLU块作为更强的下采样stem，以提取C通道特征，并保留更多信息，这与之前使用步长为4的卷积ViTs不同. 在每个模块中的Transformer块之前，我们在分支上添加一个补丁嵌入块，用于匹配通道并通过增强的补丁之间通信提取补丁信息，但是n阶段的每个模块将会有n个嵌入块所带来的巨大算力代价，我们将补丁嵌入简化为逐点CONV，然后是深度CONV。 交叉分辨率融合，在每个模块的开头插入重复的交叉分辨率融合层。为了帮助LR特征保持更多的图像细节和精准的位置信息，我们将它们与下采样的HR特征合并，不使用基于渐进卷积的下采样路径来匹配张量形状，而是采用直接下采样路径来最小化算力开销，在第i个输入和第j个输出之间的下采样路径中，使用步长为$2j-i$的深度可分离卷积来缩小空间维度并匹配输出通道。 多尺度ViT分层架构来逐步下采样的金字塔特征。PVT将金字塔结构集成到ViT中以进行多尺度特征提取，Twins交织局部和全局注意力以学习多尺度表示，SegFormer提出了一种有效的分层编码器来提取粗略和精细的特征，CSWin通过多尺度十字形局部注意力进一步提高性能。 用于语义分割的多尺度表示学习：原有的分割框架是逐步对特征图进行下采样以计算LR表示，并通过上采样恢复HR特征，例如SegNet，UNet，Hourglass，HRNet通过跨分辨率融合在整个网络中维护HR表示，Lite-HRNet提出了条件通道加权块来跨分辨率交换信息，HR-NAS搜索反转残差块和辅助Transformer分支的通道","categories":[{"name":"默认分类","slug":"默认分类","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/categories/%E9%BB%98%E8%AE%A4%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"High-Resolution Vision","slug":"High-Resolution-Vision","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/High-Resolution-Vision/"},{"name":"Transformer","slug":"Transformer","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/Transformer/"},{"name":"Segmentation","slug":"Segmentation","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/Segmentation/"}]},{"title":"Zoom In and Out:A Mixed-scale Triplet Network for Camouflaged Object Detection","slug":"混合比例三层重网络实现伪装目标检测","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-11T12:31:17.127Z","comments":true,"path":"2023/04/11/混合比例三层重网络实现伪装目标检测/","link":"","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/2023/04/11/%E6%B7%B7%E5%90%88%E6%AF%94%E4%BE%8B%E4%B8%89%E5%B1%82%E9%87%8D%E7%BD%91%E7%BB%9C%E5%AE%9E%E7%8E%B0%E4%BC%AA%E8%A3%85%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/","excerpt":"","text":"Zoom In and Out:A Mixed-scale Triplet Network for Camouflaged Object Detection摘要最近提出的伪装目标检测（COD）试图分割视觉上融入周围环境的目标，这在现实场景中是极其复杂和困难的。除了伪装对象与其背景之间具有很高的内在相似性外，这些对象通常在尺度上具有多样性，外观模糊，甚至被严重遮挡。为了解决这些问题，我们提出了一种混合比例的三重网络ZoomNet，它模仿人类在观察模糊图像时的行为，即放大和缩小。具体而言，我们的ZoomNet采用缩放策略，通过设计的尺度积分单元和分层混合尺度单元学习区分性混合尺度语义，充分挖掘候选对象和背景环境之间的细微线索。此外，考虑到来自不可区分纹理的不确定性和模糊性，我们构造了一个简单而有效的正则化约束，即不确定性感知损失，以促进模型在候选区域准确地生成具有更高置信度的预测。我们提出的高度任务友好的模型在四个公共数据集上始终优于现有的23种最先进的方法。此外，与最新的前沿模型相比，该模型在SOD任务上的优异性能也验证了该模型的有效性和通用性。 COD任务难点 如何在外观不明显和各种尺度的情况下准确定位伪装对象 如何抑制来自背景的明显干扰，更可靠地推断伪装对象 为了准确地找到场景中模糊或伪装的对象，人类可以尝试通过放大和缩小图像来参考和比较不同尺度下形状和外观的变化，这种行为模式为本文提供思路，可以通过模拟人类放大和缩小策略来识别伪装的物体。本文中提出一种混合规模的三重网络$ZoomNet$。为了精准定位目标，我们使用尺度空间理论来模拟放大和缩小策略，为此设计了两个关键模块 规模集成单元(SIU):筛选和聚合特定尺度的特征 分层混合规模单元(HMU):重组和增强混合尺度特征 此结构能够在混合尺度下挖掘出物体和背景之间准确而微妙的语义线索，并产生准确的预测，为了实现效率和有效性的平衡，模型采用共享权重策略，为增强模型在复杂场景下的泛化能力，设计了一个不确定性感知损失(UAL)来指导模型训练，模型结构图： 网络结构图 SIU：使用一个尺度作为主尺度，另外两个尺度作为辅助，利用共享的三元组特征编码器来提取不同尺度的特征并将它们馈送到尺度合并层。对于高尺度，使用最大池化加平均池化的混合结构进行下采样，这有助于在高分辨率特征中保持伪装对象的有效和多样性响应。对于低尺度使用双线性插值直接向上采样，然后将这些特征输入注意力生成器，并通过一系列卷积层计算出三通道特征图。然后再softmax激活层之后，可以获得对应于每个尺度的注意力映射计算权重为： $$\\Large A_i=softmax(\\Psi[U(f^{0.5}_i,f^{1.0}_i,D(f^{1.5}_i)],\\phi)$$ $$\\Large f_i=A^{0.5}_i\\cdot U(f^{0.5}_i)+A^{1.0}_i\\cdot f^{1.0}_i+A^{1.5}_i\\cdot D(f^{1.5}_i)$$","categories":[{"name":"COD","slug":"COD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/categories/COD/"}],"tags":[{"name":"COD","slug":"COD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/COD/"}]},{"title":"基于深度质量的特征操作实现高效的RGBD显著目标检测","slug":"基于深度质量的特征操作实现高效的RGBD显著目标检测","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-11T12:32:25.631Z","comments":true,"path":"2023/04/11/基于深度质量的特征操作实现高效的RGBD显著目标检测/","link":"","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/2023/04/11/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E8%B4%A8%E9%87%8F%E7%9A%84%E7%89%B9%E5%BE%81%E6%93%8D%E4%BD%9C%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%95%88%E7%9A%84RGBD%E6%98%BE%E8%91%97%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/","excerpt":"","text":"摘要基于RGBD显著目标检测模型在减少模型参数时，模型精度通常会下降，且受到深度质量的影响。本文设计了一种基于深度质量的特征操作(DQFM)：利用低级RGB和深度特征的对齐，以及深度流的整体关注来明确控制和增强跨模态融合。这是一个轻量化模型。 高质量的深度图通常具有一些与相应RGB图像对齐的边界 Efficient RGBD SOD Method将知识蒸馏就是用于深度蒸馏器，将从深度流获取到的深度知识转移到RGB流，从而实现无深度推理框架，后Chen设计了一个定制的深度主干，以提取互补特征 网络结构图 由编码器和解码器构成，其中RGB分支同时负责RGB特征提取和RGB与深度特征之间的交叉模式融合，另一方面解码器部分负责进行简单的两阶段融合以生成最终的显著性图，具体的说就是：编码器包括一个基于MobileNet-v2的RGB相关分支，一个深度相关分支，以及DQFM。在某个层次提取的深度特征通过DQFM后，再经过简单的元素加法融合到RGB分支中，然后发送到下一个层次。为了捕获多尺度语义信息，在RGB分支的末尾添加了PPM(金字塔池模块),在实际操作中，DQFM包含两个连续操作，深度质量启发加权和深度整体注意。 DQW结构 首先将低层特征$f_{r}^{1}$ 和 $f_{d}^{1}$ 转化为特征$ f_{rt^{‘} }$和$ f_{dt^{‘} }$,$$ \\large f_{rt^{‘} }&#x3D;BConv_{1\\times1}(f_{r}^{1}),f_{dt^{‘} }&#x3D;BConv_{1\\times1}(f_{d}^{1})$$ 其中$BConv$表示$1\\times1$卷积和$ReLU$激活函数，为了评估低级特征对齐，对这两个特征进行对齐编码$$ \\large V_{BA}&#x3D;\\dfrac{GAP(f_{rt^{‘} }\\otimes f_{dt^{‘} })}{GAP(f_{rt^{‘} } + f_{dt^{‘} })}$$ 其中$GAP(\\cdot)$表示全局平均池化操作，$\\otimes$表示按元素乘法。增强向量的计算方式：$$ \\large V_{BA}^{ms}&#x3D;[V_{BA},V_{BA}^{1},V_{BA}^{2}]$$其中[$\\cdot$]表示通道串联。然后使用两个完全连接的层使得$\\alpha\\in\\mathbb{R}^{5}$转化到$V_{BA}^{ms}$计算方式为：$$\\large\\alpha&#x3D;MLP(V_{BA}^{ms})$$$MLP(\\cdot)$表示末端为$Sigmoid$函数的感知器。 DHA结构 首先利用最高级特征$f_{d}^{5}$从深度流定位粗糙的突出区域，使用压缩和上采样方式使得$f_{d}^{5}$转化为$f_{dht}$计算方式为$$\\large f_{dht}&#x3D;F_{UP}^{8}(BConv_{1\\times1}(F_{DN}^2(f_{d}^5))$$ 其中$F_{UP}^{8}$表示8层双线性上采样，然后结合低层RGB和深度特征进行重新校准。为了更好地模拟低水平和高水平特征之间的长期依赖性，同时保持DHA的效率，我们采用最大池运算和扩大卷积来快速增加感受野。重新校准过程定义为：$$ \\large F_{rec}(f_{dht})&#x3D;F_{UP}^{2}(DConv_{3\\times3}(F_{DN}^{2}(f_{dht}+f_{ec})))$$ $F_{rec}(\\cdot)$表示重新校准过程。$DConv_{3\\times3}(\\cdot)$表示$3\\times3$扩张卷积，步长为1，扩张率为2.然后是$BatchNorm$和$ReLU$激活函数，$F_{UP}^{2}(\\cdot)&#x2F;F_{DN}^{2}(\\cdot)$表示双线性上采样\\下采样操作。为提高性能，再进行两次重新校准。$$\\large f_{dht}^{‘}&#x3D;F_{rec}(f_{dht}),f^{‘’}{dht}&#x3D;F{rec}(f^{‘}_{dht})$$ 最终实现整体注意力地图：$$\\large \\beta&#x3D;BConv_{3 \\times 3}(f_{ec}+f_{dht}^{‘’})$$ 最后获得五张深度整体注意图$\\large{ {\\beta_{1},\\beta_{2},\\beta_{3},\\beta_{4},\\beta_{5} }}$如下图所示： 通常情况下，深度学习不如RGB图像，为实现效率和准确性的平衡，本文选择定制深度主干(TDB)，具体来说就是基于$MobliceNetV2$中的反向剩余瓶颈块(IRB)并构建一个新的更小的主干，减少信道数量和堆叠块。结构如下： 解码器简化版的两级解码器，包括预融合和完全融合，预融合是通过信道压缩和层次分组来减少特征信道和层次，完全融合则是进一步聚合低层和高层特征，生成最终的显著图。 预融合阶段首先使用具有$BatchNorm$和$ReLU$激活的$3\\times3$深度可分离卷积，表示为$DSConv_{3\\times3}$,将压缩编码器特征$f_{c}^{i},(i&#x3D;1,2,…6)$到统一信道16，然后使用通道注意算子$F_{CA}$通过加权不同信道来增强特征。这个过程可以表示为：$$\\large cf_{c}^{i}&#x3D;F_{CA}(DSConv_{3\\times3}(f_{c}^{i}))$$其中$cf_{c}^{i}$表示压缩和增强功能。为了减少特征层次，作者将6个层次分为两个层次(低级层次和高级层次)$$ \\large cf_{c}^{low}&#x3D;\\sum_{i&#x3D;0}^{3}F_{UP}^{2^{i-1} }(cf_{c}^{i}),cf_{c}^{high}&#x3D;\\sum_{i&#x3D;4}^{6}cf_{c}^{i}$$ 聚合模块由于在预融合阶段，信道数量和层次已经减少，在全融合阶段，我们直接将高层和低层层次串联起来，然后将串联馈送到预测头，以获得最终的全分辨率预测图，表示为：$$ \\large S_c&#x3D;F_{p}^{c}([cf_{e}^{low},F_{UP}^{8}(cf_{c}^{high})])$$其中$S_c$表示最终的显著性图，$F_{p}^{c}(\\cdot)$表示一个预测头，由两个$3\\times3$深度方向可分离卷积（然后是$BatchNorm$层和$ReLU$激活）、一个$3\\times3Sigmoid$激活卷积以及一个$2\\times$双线性上采样组成，以恢复原始输入大小。 损失函数总损失$\\pounds$最终由深度分支损失$\\pounds_{c}$和深度监管损失$\\pounds_{d}$构成，$$\\large \\pounds &#x3D; \\pounds_{c}(S_{c},G)+ \\pounds_{d}(S_{d},G)$$我们使用的是标准的交叉熵损失","categories":[{"name":"SOD","slug":"SOD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/categories/SOD/"}],"tags":[{"name":"SOD","slug":"SOD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/SOD/"}]},{"title":"Multi-Scale High-Resolution Vision Transformer for Semantic Segmentation","slug":"高分辨率显著性检测的金字塔嫁接模型","date":"2023-04-11T07:30:16.000Z","updated":"2023-04-11T12:31:25.904Z","comments":true,"path":"2023/04/11/高分辨率显著性检测的金字塔嫁接模型/","link":"","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/2023/04/11/%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%9A%84%E9%87%91%E5%AD%97%E5%A1%94%E5%AB%81%E6%8E%A5%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"Pyramid Grafting Network for One-Stage High Resolution Saliency Detection摘要由于采样深度和感受野大小之间的矛盾，大多数根据低分辨输入设计的SOD模型在高分辨率图像中表现不佳，本文提出一种金字塔嫁接网络。使用transformer和CNN主干分别从不同分辨率的图像中提取特征，然后将特征从transformer分支嫁接到CNN分支，与此同时提出一种基于注意的分叉模型嫁接模块，使CNN分支能够在解码过程中，在不同信源特征的引导下，更全面地组合破碎的细节信息，此外还设计了一个注意引导丢失来明确监督交叉嫁接模块生成注意矩阵，以帮助网络更好地与来自不同模型的注意进行交互。 困境当前主流的SOD模型遇到高分辨率的图像，为了减少内存的开销往往会将图像先下采样然后对输出结果上采样已恢复原始分辨率，由于现在的SOD模型都是使用编码器-解码器的方式设计的，随着分辨率的大幅度提高，提取的特征大小会增加，但是网络的感受野是固定的，使得相对感受野变小，最终导致无法捕获对任务至关重要的全局语义。 目前对于高分辨率SOD方法有两种主流：HRSOD和DHQSOD，其中HRSOD将整个过程分为全局阶段，局部阶段和重组阶段，全局阶段为局部阶段和作物过程提供指导。DHSOD将SOD任务分解为分类任务和回归任务，这两个任务通过他们提出的trimap和不确定性损失连接起来，它们生成具有清晰边界的相对较好的显著性贴图。但是这两者都是使用多阶段架构，将SOD分为语义(低分辨率)和细节(高分辨率)两个阶段。由此引出两个问题： 阶段间语境语义迁移不一致，在前一个阶段获得的中间映射被输入到最后一个阶段，同时错误也被传递，由此后续细化阶段可能将继续放大错误 耗时，与单阶段相比，多阶段方法不仅难以并行且参数过多，模型运行运行速度较慢 高分辨率SDO发展Zeng等人Towards High-Resolution Salient Object Detection提出了一种高分辨率显著目标检测范式，使用GSN提取语义信息，使用APS引导的LRN优化局部细节，最后使用GLFN进行预测融合。他们还提供了第一个高分辨率显著目标检测数据集（HRSOD）。Tang等人Disentangled high quality salient object detection提出，显著目标检测应分为两项任务。他们首先设计LRSCN以在低分辨率下捕获足够的语义并生成trimap。通过引入不确定性损失，所设计的HRRN可以对第一阶段使用低分辨率数据集生成的trimap进行细化。然而，它们都使用多级体系结构，这导致推理速度较慢，难以满足某些实际应用场景。更严重的问题是网络之间的语义不一致。 使用常用的SOD数据集通常分辨率较低，用他们来训练高分辨率网络和评估高质量分割存在以下几点缺点: 图像分辨率低导致细节不足 注释边缘的质量较差 注释的更加精细级别不够令人满意 当前可用的高分辨率数据集是HRSOD，但是HRSOD数据集图像数量有限,严重影响模型的泛化能力。 Staggered Grafting Framework网络框架如图所示： 由两个编码器和一个解码器构成，使用Swim transformer和ResNet18作为编码器，transformer编码器能够在低分辨率情况下获得准确的全局语义信息，卷积编码器能够在高分辨率输入下获得丰富的细节信息，不同模型之间的提取的特征可能是互补的，可以获得更多的有效特征。 在编码的过程中，向两个编码器馈送不同分辨率的图像，并行获取全局语义信息和详细信息。解码分为三个过程，一个是Swim解码，一个是嫁接编码，最后是交错结构的ResNet解码，在第二个子阶段的解码特征是从跨模态移植模块产生的，其中全局语义信息从Swin分支移植到ResNet分支，跨模态移植模块还会处理一个名为CAM的矩阵进行监督 交叉模型迁移模块(CMGM)作用：移植由两个编码器提取的特征，对于transformer所提取的特征$f_{S_2}$能够远距离捕获信息，具有全局语义信息。使用ResNet所得到的$f_{R_5}$有更好的局部信息，也就是更丰富的细节信息。但是由于特征大小和感受野之间的差异，在$f_{R_5}$中有更多的噪声。 使用常见的融合方法：如逐元素相加和相乘的适用情况限制在显著预测和不同特征生成的预测至少有一部分是对的情况下，否则就是一种错误的适用方式，且这种操作都只关注于有限的局部信息，导致没法实现自我纠错。 作者提出使用CMGM重新计算ResNet特征和Transformer特征之间的逐点关系，将全局语义信息通过transformer分支转移到ResNet分支，从而弥补常见的错误，通过计算$E&#x3D;|G-P| \\in [0,1]$得到误差图 CMGM纠错效果图 CMGM网络结构 实验结果 可视化 CUDA_VISIBLE_DEVICES&#x3D;2,3 python3 -m torch.distributed.launch –nproc_per_node&#x3D;2 train_distributed.py –batchsize 4 –master_port 29501 –savepath “..&#x2F;model&#x2F;PGNet_DUTS_Test&#x2F;“ –datapath “&#x2F;storage&#x2F;GWB&#x2F;Data&#x2F;DUTS-TR”\\","categories":[{"name":"默认分类","slug":"默认分类","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/categories/%E9%BB%98%E8%AE%A4%E5%88%86%E7%B1%BB/"}],"tags":[{"name":"Transformer","slug":"Transformer","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/Transformer/"},{"name":"Segmentation","slug":"Segmentation","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/Segmentation/"}]}],"categories":[{"name":"SOD","slug":"SOD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/categories/SOD/"},{"name":"默认分类","slug":"默认分类","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/categories/%E9%BB%98%E8%AE%A4%E5%88%86%E7%B1%BB/"},{"name":"COD","slug":"COD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/categories/COD/"}],"tags":[{"name":"SOD","slug":"SOD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/SOD/"},{"name":"High-Resolution Vision","slug":"High-Resolution-Vision","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/High-Resolution-Vision/"},{"name":"Transformer","slug":"Transformer","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/Transformer/"},{"name":"Segmentation","slug":"Segmentation","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/Segmentation/"},{"name":"COD","slug":"COD","permalink":"https://github.com/DFSgwb/DFSgwb.github.io.git/tags/COD/"}]}